{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafc21a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bafc21a2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3053ef6c4c1420bc5667e6a4a747f193",
     "grade": false,
     "grade_id": "cell-509ca0ded5c64f20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1: Word2Vec Representations (10 Marks)\n",
    "\n",
    "## Due: June 20, 2023\n",
    "\n",
    "Welcome to the Assignment 1 of the course. This week we will learn about vector representations for words and how can we utilize them to solve the topic classification task that we discussed in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c70cb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33882,
     "status": "ok",
     "timestamp": 1686765571496,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "d8c70cb4",
    "outputId": "703109a1-8fd5-4136-d32b-de1626a9d8bb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Assignment1/data/ag_news_csv\"\n",
    "except:\n",
    "    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/TLPNLP2023/source/Assignment1/data/ag_news_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40186d50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 37015,
     "status": "ok",
     "timestamp": 1686765637173,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "40186d50",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4025dd1a18667a132e8af5e7b47e2c7",
     "grade": false,
     "grade_id": "cell-5fd0f300224e0638",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3c189143-f300-47db-e5b1-fa8b5de334a9"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21676ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 9412,
     "status": "ok",
     "timestamp": 1686765646579,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "f21676ea",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e71cc913be4617a2caac6f6f81c7b16f",
     "grade": false,
     "grade_id": "cell-29786c43c3f990f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1b0278a9-5993-4a8e-998a-b5ad88f8717e"
   },
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5845fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4f5845fb",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d2f23096c8d1eb11053b68f4702371d",
     "grade": false,
     "grade_id": "cell-5de65c2212dbb4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Similar to last time we will again be working on the AG News Dataset. Below we load the dataset into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6828c92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 2861,
     "status": "ok",
     "timestamp": 1686765649433,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "a6828c92",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738534f5ccbc90db3bbd011ab79b9f5d",
     "grade": false,
     "grade_id": "cell-3b092d562863ae9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "574659b1-89a2-4ea1-9316-c16c5351d8cc"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = 4\n",
    "LABELS_MAP = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "def load_dataset(split):\n",
    "    ## Load the datasets and specify the column names\n",
    "    df = pd.read_csv(f\"{data_dir}/{split}.csv\", names=[\"label\", \"title\", \"description\"])\n",
    "\n",
    "    ## Merge the title and description columns\n",
    "    df[\"news\"] = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n",
    "    ## Remove the title and description columns\n",
    "    df = df.drop([\"title\", \"description\"], axis=1)\n",
    "\n",
    "    ## Have the labels start from 0\n",
    "    df[\"label\"] = df[\"label\"] - 1\n",
    "\n",
    "    ## Map the label to the corresponding class\n",
    "    df[\"label_readable\"] = df[\"label\"].apply(lambda x: LABELS_MAP[int(x)])\n",
    "\n",
    "    df = df[[\"news\", \"label\", \"label_readable\"]]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "## Load the datasets and specify the column names\n",
    "train_df = load_dataset(\"train\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Number of Training Examples: {len(train_df)}\")\n",
    "print(f\"Number of Test Examples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e30c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1686765649434,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "998e30c1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "874e5314e53c05c8cdf13df2d3926197",
     "grade": false,
     "grade_id": "cell-e4595f3246a0e463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3b90467c-f3af-427e-f953-0c5cd437d5e3"
   },
   "outputs": [],
   "source": [
    "# View a sample of the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328480bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "328480bc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7011afdc87bafd67208763ccbc6b7ba9",
     "grade": false,
     "grade_id": "cell-6ed2b7d10d7204ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 0: Warm Up Excercise (2 Marks)\n",
    "\n",
    "To start we ask you to re-implement some functions from the Lab 1. Mainly you will implement the preprocessing pipeline and vocabulary building functions again as well as some new but related functions. Details about the functions will be given in their Doc Strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bcb88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b03bcb88",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2f67ea2187b711f1563b6993b36dd3a",
     "grade": false,
     "grade_id": "cell-979e1e262f589562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 0.1: Preprocessing Pipeline (1 Mark)\n",
    "\n",
    "Implement the preprocessing pipeline like we did in Lab1, however, this time we will only implement converting the text to lower case and removing punctuations.\n",
    "\n",
    "We are not doing any stemming this time as we will be using pre-trained word representations in this assignment, and like it was discussed in the lectures stemming often results in the words that may not exist in common dictionaries.\n",
    "\n",
    "We are also skipping stop words removal this time around, the reason being that removing stop words can often hurt the structural integrity of a sentence and the choice of stop words to use can be very subjective and depend upon the task at hand. For example: In the stop words list that we used last time contained the word `not`, removing which can change the sentiment of the sentence, eg. I did not like this movie -> I did like this movie. In this assignment we will explore more sophisticated ways to handle the stop words than just directly removing them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342b357",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1686765660671,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "8342b357",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a756ea5c0d1c6b78d851fb87a93aa31",
     "grade": false,
     "grade_id": "cell-353290c941bcd294",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(text):\n",
    "    \"\"\"\n",
    "    Given a piece of text applies preprocessing techniques\n",
    "    like converting to lower case, removing stop words and punctuations.\n",
    "\n",
    "    Apply the functions in the following order:\n",
    "    1. to_lower_case\n",
    "    2. remove_punctuations\n",
    "\n",
    "    Inputs:\n",
    "    - text (str) : A python string containing text to be pre-processed\n",
    "\n",
    "    Returns:\n",
    "    - text_preprocessed (str) : Resulting string after applying preprocessing\n",
    "\n",
    "    Note: You may implement the functions for the two steps seperately in this cell\n",
    "            or just write all the code in this function only we leave that up to you.\n",
    "    \"\"\"\n",
    "\n",
    "    text_preprocessed = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c53fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1686765672141,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "2c8c53fd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02e2686d2c7d9f10c409dfcfbc81285c",
     "grade": true,
     "grade_id": "cell-f9c3aa14c0041549",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "197c09e0-e75e-460c-aa1a-ecd24b571185"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_string_test_cases(test_case_input,\n",
    "                        test_case_func_output,\n",
    "                        test_case_exp_output):\n",
    "\n",
    "    print(f\"Input: {test_case_input}\")\n",
    "    print(f\"Function Output: {test_case_func_output}\")\n",
    "    print(f\"Expected Output: {test_case_exp_output}\")\n",
    "\n",
    "    if test_case_func_output == test_case_exp_output:\n",
    "        print(\"Test Case Passed :)\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Test Case Failed :(\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return False\n",
    "\n",
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\"\n",
    "test_case_answer = \"mr and mrs dursley of number four privet drive were proud to say that they were perfectly normal\"\n",
    "test_case_student_answer = preprocess_pipeline(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"\\\"Little tyke,\\\" chortled Mr. Dursley as He left the house.\"\n",
    "test_case_answer = \"little tyke chortled mr dursley as he left the house\"\n",
    "test_case_student_answer = preprocess_pipeline(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ca64f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4975,
     "status": "ok",
     "timestamp": 1686765684068,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "593ca64f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04da13416eac329831acd934aea220ea",
     "grade": false,
     "grade_id": "cell-9a5f9fae0f5eff1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Preprocess the dataset\n",
    "\n",
    "train_df[\"news\"] = train_df[\"news\"].apply(lambda x : preprocess_pipeline(x))\n",
    "test_df[\"news\"] = test_df[\"news\"].apply(lambda x : preprocess_pipeline(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6afbfd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f6afbfd5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3f58234551ff1bca6716082c0780f61",
     "grade": false,
     "grade_id": "cell-1d9b7cdf2d10f0e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 0.2: Create Vocabulary (0.25 Marks)\n",
    "\n",
    "Implement the `create_vocab` function below like you did during the lab. Do not forget using `nltk.tokenize.word_tokenize` to tokenize the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f01f82",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1686765822352,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "43f01f82",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cec35c59da69529160e5ce2e6001cbd5",
     "grade": false,
     "grade_id": "cell-8893f5c7966540bb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_vocab(documents):\n",
    "    \"\"\"\n",
    "    Given a list of documents each represented as a string,\n",
    "    create a word vocabulary containing all the words that occur\n",
    "    in these documents.\n",
    "    (0.25 Marks)\n",
    "\n",
    "    Inputs:\n",
    "        - documents (list) : A list with each element as a string representing a\n",
    "                            document.\n",
    "\n",
    "    Returns:\n",
    "        - vocab (list) : A **sorted** list containing all unique words in the\n",
    "                        documents\n",
    "\n",
    "    Example Input: ['john likes to watch movies mary likes movies too',\n",
    "                  'mary also likes to watch football games']\n",
    "\n",
    "    Expected Output: ['also',\n",
    "                    'football',\n",
    "                    'games',\n",
    "                    'john',\n",
    "                    'likes',\n",
    "                    'mary',\n",
    "                    'movies',\n",
    "                    'to',\n",
    "                    'too',\n",
    "                    'watch']\n",
    "\n",
    "\n",
    "    Hint: `nltk.tokenize.word_tokenize` function may come in handy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return sorted(vocab) # Don't change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea49b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1686765826580,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "6dea49b0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a6ab09623b78b7c909e7f6e58d2e2c",
     "grade": true,
     "grade_id": "cell-e36c6b1e7f49a6af",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "35b9c91c-0fba-4b3f-be03-f5916927383c"
   },
   "outputs": [],
   "source": [
    "def evaluate_list_test_cases(test_case_input,\n",
    "                        test_case_func_output,\n",
    "                        test_case_exp_output):\n",
    "\n",
    "    print(f\"Input: {test_case_input}\")\n",
    "    print(f\"Function Output: {test_case_func_output}\")\n",
    "    print(f\"Expected Output: {test_case_exp_output}\")\n",
    "\n",
    "    if test_case_func_output == test_case_exp_output:\n",
    "        print(\"Test Case Passed :)\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Test Case Failed :(\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "\n",
    "test_case = [\"john likes to watch movies mary likes movies too\",\n",
    "              \"mary also likes to watch football games\"]\n",
    "test_case_answer = ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
    "test_case_student_answer = create_vocab(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "\n",
    "test_case = [\"We all live in a yellow submarine.\",\n",
    "             \"Yellow submarine, yellow submarine!!\"\n",
    "             ]\n",
    "test_case_answer = ['!', ',', '.', 'We', 'Yellow', 'a', 'all', 'in', 'live', 'submarine', 'yellow']\n",
    "test_case_student_answer = create_vocab(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0067f2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 29358,
     "status": "ok",
     "timestamp": 1686765859676,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "c0067f2f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37e225d4eae202c09513d10c78a10e1c",
     "grade": false,
     "grade_id": "cell-c5e83435becaae76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create vocabulary from training data\n",
    "train_documents = train_df[\"news\"].values.tolist()\n",
    "train_vocab = create_vocab(train_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d15dce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a6d15dce",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97fe05be24e2cfc399861fa8417f351f",
     "grade": false,
     "grade_id": "cell-448eebb0efc2ac9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 0.3: Get Word Frequencies (0.75 Marks)\n",
    "\n",
    "We define the normalized frequency of a word `w` in a corpus as:\n",
    "\n",
    "p(w) = Number of occurences of `w` in all documents / Total Number of occurences of all words in all documents\n",
    "\n",
    "Word frequencies can be helpful as it can help us recognize the most common words which in most cases will be stop words as well as rare words that occur in the documents. Later we will be making use of word frequencies to create sentence representations, but for now just implement the `get_word_frequencies` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0db0d",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1686765866597,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "a1a0db0d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d184d720232bd829645a606b36cf2ac3",
     "grade": false,
     "grade_id": "cell-3f09f88f24427b07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_word_frequencies(documents):\n",
    "    \"\"\"\n",
    "    Gets the normalized frequency of each word w i.e.\n",
    "    p(w) =  #num_of_occurences_of_w / #total_occurences_of_all_words\n",
    "    present in documents\n",
    "\n",
    "    Inputs:\n",
    "        - documents(list): A list of documents\n",
    "\n",
    "    Returns:\n",
    "        - word2freq(dict): A dictionary containing words as keys\n",
    "                           and values as their corresponding frequencies\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word2freq = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return word2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da445e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 28755,
     "status": "ok",
     "timestamp": 1686765898845,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "da445e8e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bbaaf0e6da86fb0c5c0bae7bdd2c2d",
     "grade": true,
     "grade_id": "cell-bf6d404f736519d9",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a796bbd8-bc2e-4e24-bf08-928511d9f19f"
   },
   "outputs": [],
   "source": [
    "def check_dicts_same(dict1, dict2):\n",
    "    if not isinstance(dict1, dict):\n",
    "        print(\"Your function output is not a dictionary!\")\n",
    "        return False\n",
    "    if len(dict1) != len(dict2):\n",
    "        return False\n",
    "\n",
    "    for key in dict1:\n",
    "        val1 = dict1[key]\n",
    "        val2 = dict2[key]\n",
    "        if isinstance(val1, float) and isinstance(val1, float):\n",
    "            if not np.allclose(val1, val2, 1e-4):\n",
    "                return False\n",
    "        if val1 != val2:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "print(\"Running Sample Test Case 1\")\n",
    "sample_documents = [\n",
    "    'john likes to watch movies mary likes movies too',\n",
    "    'mary also likes to watch football games'\n",
    "]\n",
    "actual_word2freq = {'john': 0.0625,\n",
    "                     'likes': 0.1875,\n",
    "                     'to': 0.125,\n",
    "                     'watch': 0.125,\n",
    "                     'movies': 0.125,\n",
    "                     'mary': 0.125,\n",
    "                     'too': 0.0625,\n",
    "                     'also': 0.0625,\n",
    "                     'football': 0.0625,\n",
    "                     'games': 0.0625}\n",
    "\n",
    "output_word2freq = get_word_frequencies(sample_documents)\n",
    "print(f\"Input Documents: {sample_documents}\")\n",
    "print(f\"Output Word Frequencies: {output_word2freq}\")\n",
    "print(f\"Expected Word Frequencies: {actual_word2freq}\")\n",
    "\n",
    "assert check_dicts_same(output_word2freq, actual_word2freq)\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 2\")\n",
    "sample_documents = [\n",
    "    'We all live in a yellow submarine.',\n",
    "    'Yellow submarine, yellow submarine!!'\n",
    "]\n",
    "actual_word2freq = {'We': 0.06666666666666667,\n",
    "                    'all': 0.06666666666666667,\n",
    "                    'live': 0.06666666666666667,\n",
    "                    'in': 0.06666666666666667,\n",
    "                    'a': 0.06666666666666667,\n",
    "                    'yellow': 0.13333333333333333,\n",
    "                    'submarine': 0.2,\n",
    "                    '.': 0.06666666666666667,\n",
    "                    'Yellow': 0.06666666666666667,\n",
    "                    ',': 0.06666666666666667,\n",
    "                    '!': 0.13333333333333333}\n",
    "\n",
    "output_word2freq = get_word_frequencies(sample_documents)\n",
    "print(f\"Input Documents: {sample_documents}\")\n",
    "print(f\"Output Word Frequencies: {output_word2freq}\")\n",
    "print(f\"Expected Word Frequencies: {actual_word2freq}\")\n",
    "\n",
    "assert check_dicts_same(output_word2freq, actual_word2freq)\n",
    "print(\"****************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb793f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8dbb793f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "035499b52a6e12a24adb1d6144527547",
     "grade": false,
     "grade_id": "cell-cb63290c9a9faade",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Word2Vec Representations\n",
    "\n",
    "In this task you will learn how to use word2vec for obtaining vector representations for words and then how to use them further to create sentence/document level vector representations. We will be using the popular [gensim](https://radimrehurek.com/gensim/) package that has great support for vector space models and supports various popular word embedding methods like word2vec, fasttext, LSA etc. For the purposes of this assignment we will be working with the pretrained word2vec vectors on the google news corpus containing about 100 billion tokens. Below we provide a tutorial on how to use gensim for obtaining these word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3942d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4ee3942d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8c1b4e500691f49ee8070824677e914",
     "grade": false,
     "grade_id": "cell-9cbececf20649cac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We start by downloading pretrained word2vec vectors and create a `gensim.models.keyedvectors` obect. The download has a size of about 2GB, so might take a few minutes to download and load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ed16f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 496288,
     "status": "ok",
     "timestamp": 1686766398984,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "e65ed16f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a6ce1a75eb60a924ffd10d06a33d79d",
     "grade": false,
     "grade_id": "cell-73dd0118ce378a1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "b4ee98f4-a85a-4a3c-b6a6-8575d9e2e0b4"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298caa61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "298caa61",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ba8ed299739fe897b0967aec789dfab",
     "grade": false,
     "grade_id": "cell-2ec190e6285691e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The `wv` object has a bunch of methods that we can use to obtain vector representations of words, finding similar words etc. We start with how to obtain vectors for words using it, which can be done using the `get_vector` method as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb7ee1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1686766529235,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "19cb7ee1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf33dde1d542b2b4fdcb72d32919d14",
     "grade": false,
     "grade_id": "cell-f6f0467707b76ef3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d7cf2e36-4c80-4d71-8df6-1d9041d70ab0"
   },
   "outputs": [],
   "source": [
    "word = \"bad\"\n",
    "vector = wv.get_vector(word)\n",
    "print(f\"Word : {word}\")\n",
    "print(f\"Length of the vector: {len(vector)}\")\n",
    "print(f\"Vector:\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f78a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1f3f78a9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ebdab812a9479411729817e75a1946c",
     "grade": false,
     "grade_id": "cell-e8ea71f63cd29438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can also obtain the brackets by using angular brackets notation i.e. `wv[\"bad\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17f380",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1686766533300,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "3f17f380",
    "outputId": "d42da703-3eef-4eaf-8aa9-3babd13052a6"
   },
   "outputs": [],
   "source": [
    "word = \"bad\"\n",
    "vector = wv[word]\n",
    "print(f\"Word : {word}\")\n",
    "print(f\"Length of the vector: {len(vector)}\")\n",
    "print(f\"Vector:\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea9cdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b2ea9cdd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879b6981a00a275555d915d274403d40",
     "grade": false,
     "grade_id": "cell-8e044d48bc3d945c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Also note that the word2vec model might not have vectors for all words, you can check for Out of Vocabulary (OOV) words using the `in` operator as shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f75bce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1686766538044,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "e1f75bce",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "960f2b8166d838e07b5f0ba258db7c1c",
     "grade": false,
     "grade_id": "cell-66a1c4a28ac5655d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "033d3316-f044-456d-80b9-0843d3056694"
   },
   "outputs": [],
   "source": [
    "print(\"book\" in wv)\n",
    "print(\"blastoise\" in wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bd4d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "278bd4d0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2454333a3d0950ecc2ad906062ce294",
     "grade": false,
     "grade_id": "cell-af317103566b235b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Just looking at the vectors we cannot really gain any insights about them, but it is the relation between the vectors of different words that is much more easier to interpet. `wv` object has a `most_similar` method that for a given word obtains the words that are most similar to it by computing cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0669e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 15152,
     "status": "ok",
     "timestamp": 1686766555922,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "42f0669e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3354f1ff87379df70f29be49745cfc5d",
     "grade": false,
     "grade_id": "cell-ac4f122af54887f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d84ea12f-8f62-4498-bd71-4e8865218bfa"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(\"bad\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad7571",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1686766564842,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "7dad7571",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edb14356c0c0783c5f39c0f2898156d1",
     "grade": false,
     "grade_id": "cell-6aac9038cfba8b93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "dde1bc77-8a0d-4821-a70b-e88f3a51c8a8"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(\"king\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319424",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f9319424",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04e84bcad2385b88001486831b87bbec",
     "grade": false,
     "grade_id": "cell-c4c59eb5b19eb50f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see that the we obtain very reasonable similar words in both examples. We can also use `most_similar` to do the analogy comparison that was discussed in the class. For eg: man : king :: woman : ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499db84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1686766568496,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "0499db84",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "553f0b0d30babf6f8e31370f26a92416",
     "grade": false,
     "grade_id": "cell-7a44322336c7d397",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3a021b4e-68d0-43cd-da1e-4db975468ddb"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f1241",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1686766571093,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "624f1241",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c82ad7139468ace874097a6b68fc20ca",
     "grade": false,
     "grade_id": "cell-c1c43a088c7b5158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "4ef39bd7-a685-4039-9ffc-691be5cfa2fc"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['woman', 'father'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93c9b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "df93c9b8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19e0138a1c498236f9061ddc4b068230",
     "grade": false,
     "grade_id": "cell-a875955d15d39023",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.1 Sentence representations using Word2Vec : Bag of Words Methods (2 Marks)\n",
    "\n",
    "Now that we know how to obtain the vectors of each word, how can we obtain a vector representation for a sentence or a document? One of the simplest way is to add the vectors of all the words in the sentence to obtain sentence vector. This is also called the Bag of Words approach. Can you think of why? Last time when we discussed bag of words features for a sentence, it contained counts of each word occuring in the sentence. This can be just thought of as just adding one hot vectors for all the words in a sentence. Hence, adding word2vec vectors for each word in the sentence can also be viewed as a bag of words representation.\n",
    "\n",
    "Implement the `get_bow_sent_vec` function below that takes in a sentence and adds the word2vec vectors for each word occuring in the sentence to obtain the sentence vector. Also, in practice it is helpful to divide the sum of word vectors by the number of words to normalize the representation obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d348d35",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1686766578543,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "7d348d35",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e164dc4c3d3cd834c5a3a3806dc2d6e",
     "grade": false,
     "grade_id": "cell-c0d2059784aa66e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_bow_sent_vec(sentence, wv):\n",
    "    \"\"\"\n",
    "    Obtains the vector representation of a sentence by adding the word vectors\n",
    "    for each word occuring in the sentence (and dividing by the number of words) i.e\n",
    "\n",
    "    v(s) = sum_{w \\in s}(v(w)) / N(s)\n",
    "    where N(s) is the number of words in the sentence,\n",
    "    v(w) is the word2vec representation for word w\n",
    "    and v(s) is the obtained vector representation of sentence s\n",
    "\n",
    "    Inputs:\n",
    "        - sentence (str): A string containing the sentence to be encoded\n",
    "        - wv (gensim.models.keyedvectors.KeyedVectors) : A gensim word vector model object.\n",
    "\n",
    "    Returns:\n",
    "        - sentence_vec (np.ndarray): A numpy array containing the vector representation\n",
    "        of the sentence\n",
    "\n",
    "    Note : Not all the words might be present in `wv` so you will need to check for that,\n",
    "          and only add vectors for the words that are present. Also while normalization\n",
    "          divide by the number of words for which a word vector was actually present in `wv`\n",
    "\n",
    "    Important Note: In case no word in the sentence is present in `wv`, return an all zero vector!\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vec = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd545f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1686766581863,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "f1fd545f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd5dd4c68962a770f64a822683c804f",
     "grade": true,
     "grade_id": "cell-89f0581a4dd4e139",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a50b9a2f-0c97-4c6f-9925-519593c6bb8f"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Case 1\")\n",
    "sample_sentence ='john likes watching movies mary likes movies too'\n",
    "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
    "expected_sent_vec = np.array([ 0.03330994,  0.11713409,  0.00738525,  0.24951172, -0.0202179 ])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 2\")\n",
    "sample_sentence ='We all live in a yellow submarine.'\n",
    "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
    "expected_sent_vec = np.array([-0.08424886,  0.14601644,  0.0727946 ,  0.09978231, -0.02655029])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 3\")\n",
    "sample_sentence ='blastoise pikachu charizard'\n",
    "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
    "expected_sent_vec = np.array([0.,  0.,  0. ,  0., 0.])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10d6cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2e10d6cf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79773da050366aa665259b340e7ada7d",
     "grade": false,
     "grade_id": "cell-0fc251fe019587b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.2 Sentence representations using Word2Vec : Inverse Frequency Weighted Sum Method (2 Marks)\n",
    "\n",
    "Instead of directly adding the vectors for all the words in the sentence, we can do something slightly better which tends to work very well in practice. [Arora et al. 2017](https://openreview.net/pdf?id=SyK00v5xx) proposes the following method for computing sentence embedding from word vectors\n",
    "\n",
    "<img src=\"https://i.ibb.co/vwzHXHy/sent-embed.jpg\" alt=\"sent-embed\" border=\"0\">\n",
    "\n",
    "Here v_w is the vector representation of the word w, p(w) is the frequency of the word w, |s| is the number of words in the sentence, and `a` is just a constant with a typical value between 1e-3 to 1e-4.\n",
    "\n",
    "Intuitively, we take a weighted sum of all the word vectors where the weights are inversely propotional to the frequency of the word (p(w)). This ensures that very frequent words which are often stop words like \"the\", \"I\" etc. are given lower weightage when constructing the sentence vector. `a` is used as smoothing constant, such that when p(w) = 0 we still have finite weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63553403",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1686766588822,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "63553403",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccda0fe171659dd03a2ab490eb88a151",
     "grade": false,
     "grade_id": "cell-19256c98f438046c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_weighted_bow_sent_vec(sentence, wv, word2freq, a = 1e-4):\n",
    "    \"\"\"\n",
    "    Obtains the vector representation of a sentence by adding the word vectors\n",
    "    for each word occuring in the sentence (and dividing by the number of words) i.e\n",
    "\n",
    "    v(s) = (sum_{w \\in s} a / (a + p(w)) * (v(w))) / N(s)\n",
    "\n",
    "    Inputs:\n",
    "        - sentence (str): A string containing the sentence to be encoded\n",
    "        - wv (gensim.models.keyedvectors.KeyedVectors) : A gensim word vector model object.\n",
    "        - word2freq (dict): A dictionary with words as keys and their frequency in the\n",
    "                            entire training dataset as values\n",
    "        - a (float): Smoothing constant\n",
    "\n",
    "    Returns:\n",
    "        - sentence_vec (np.ndarray): A numpy array containing the vector representation\n",
    "        of the sentence\n",
    "\n",
    "    Important Note: In case no word in the sentence is present in `wv`, return an all zero vector!\n",
    "\n",
    "    Hint: If a word is not present in the `word2freq` dictionary, you can consider frequency\n",
    "          of that word to be zero\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vec = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457e35c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1686766593134,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "6457e35c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73e06089782de8d3e2895ff6b0841211",
     "grade": true,
     "grade_id": "cell-eca82e8303f85b83",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "eb426e21-203d-4eb2-9185-517cbec7b080"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Case 1\")\n",
    "sample_sentence ='john likes watching movies mary likes movies too'\n",
    "sample_word2freq = {\n",
    "    \"john\" : 0.001,\n",
    "    \"likes\": 0.01,\n",
    "    \"watching\" : 0.01,\n",
    "    \"movies\": 0.05,\n",
    "    \"mary\" : 0.001,\n",
    "    \"too\": 0.1\n",
    "}\n",
    "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, sample_word2freq)\n",
    "expected_sent_vec = np.array([-0.00384654,  0.00208942,  0.00010824,  0.00648482, -0.00236967])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 2\")\n",
    "sample_sentence ='We all live in a yellow submarine.'\n",
    "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, word2freq = {}, a = 1e-3)\n",
    "expected_sent_vec = np.array([-0.08424886,  0.14601644,  0.0727946 ,  0.09978231, -0.02655029])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 3\")\n",
    "sample_sentence ='blastoise pikachu charizard'\n",
    "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, word2freq = {}, a = 1e-3)\n",
    "expected_sent_vec = np.array([0.,  0.,  0. ,  0., 0.])\n",
    "print(f\"Input Sentence: {sample_sentence}\")\n",
    "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
    "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
    "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"*******************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e02b99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "55e02b99",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbc1d5777439da6a7b4e7e6d82925532",
     "grade": false,
     "grade_id": "cell-291c9ad1bba3a8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have implemented the sentence vector functions, let's obtain sentence vectors for all the sentences in our training and test sets. This will take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f493b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 176122,
     "status": "ok",
     "timestamp": 1686766784119,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "6f7f493b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed3e69873fbc2970a7166cea9773df64",
     "grade": false,
     "grade_id": "cell-59a4971906e0da17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_documents = train_df[\"news\"].values.tolist()\n",
    "test_documents = test_df[\"news\"].values.tolist()\n",
    "train_vocab = create_vocab(train_documents)\n",
    "train_word2freq = get_word_frequencies(train_documents)\n",
    "\n",
    "train_bow_vectors = np.array([\n",
    "    get_bow_sent_vec(document, wv)\n",
    "    for document in train_documents\n",
    "])\n",
    "test_bow_vectors = np.array([\n",
    "    get_bow_sent_vec(document, wv)\n",
    "    for document in test_documents\n",
    "])\n",
    "\n",
    "train_w_bow_vectors = np.array([\n",
    "    get_weighted_bow_sent_vec(document, wv, train_word2freq, a = 1e-3)\n",
    "    for document in train_documents\n",
    "])\n",
    "test_w_bow_vectors = np.array([\n",
    "    get_weighted_bow_sent_vec(document, wv, train_word2freq, a = 1e-3)\n",
    "    for document in test_documents\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d964a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "15d964a1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81c51b9feb74f79521e6a5242b1be34b",
     "grade": false,
     "grade_id": "cell-87c866ac94ce9071",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Train a Topic Classifier using Sentence Vectors\n",
    "\n",
    "This part will be just like Lab 1, but instead of the Bag of Word features we defined last time to train the classifier, we will use the sentence vectors obtained from word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dab076",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "41dab076",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e796dcedbd4c53fd32fbff12a6baa59",
     "grade": false,
     "grade_id": "cell-ef5aff9d05dd7f03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Define a Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5530c3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1686766823036,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "c5530c3d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0adc72a5a1782de16d2a7a4b6ef4154",
     "grade": false,
     "grade_id": "cell-7d07e1f5c96cba40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340d830",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1340d830",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05bb7a9919deb35f5ef7e23612b55196",
     "grade": false,
     "grade_id": "cell-31d60a7015686bb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1: Define the Multinomial Logistic Regression Model (1 Mark)\n",
    "\n",
    "Like last time define a Multinomial Logistic Regression model that takes as input the sentence vector and predicts the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5bcc2",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1686766826460,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "cff5bcc2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ca849a2559b6ed3a327c3143da29c09",
     "grade": false,
     "grade_id": "cell-42cde4e860bc4f41",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultinomialLogisticRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_input, num_labels):\n",
    "        \"\"\"\n",
    "        Define the architecture of a Multinomial Logistic Regression classifier.\n",
    "        You will need to define two components, one will be the linear layer using\n",
    "        nn.Linear, and a log-softmax activation function for the output\n",
    "        (log-softmax is numerically more stable and as we will see later we just need to log of the probabilities to calculate the loss).\n",
    "\n",
    "        Inputs:\n",
    "          - d_input (int): The dimensionality or number of features in each input.\n",
    "                            This will be required to define the linear layer\n",
    "          - num_labels (int): The number of classes in the dataset.\n",
    "\n",
    "        Hint: Recall that in multinomial logistic regression we obtain a `num_labels` probablilities (or log-probabilities in this case)\n",
    "        value for each input that denotes how likely is the input belonging\n",
    "        to each class.\n",
    "        \"\"\"\n",
    "        #Need to call the constructor of the parent class\n",
    "        super(MultinomialLogisticRegressionModel, self).__init__()\n",
    "\n",
    "        self.linear_layer = None\n",
    "        self.log_softmax_layer = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input `x` through the layers in the network and returns the output\n",
    "\n",
    "        Inputs:\n",
    "          - x (torch.tensor): A torch tensor of shape [batch_size, d_input] representing the batch of inputs\n",
    "\n",
    "        Returns:\n",
    "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
    "\n",
    "        \"\"\"\n",
    "        output = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d5c8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1686766830618,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "e33d5c8d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d8279a10e8aa06b0dbbe465f3499c4",
     "grade": true,
     "grade_id": "cell-3abd5485d61989dd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ba014a27-3617-4b26-eccb-473e369b2ded"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "torch.manual_seed(42)\n",
    "d_input = 5\n",
    "num_labels = 4\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = d_input,num_labels = num_labels)\n",
    "print(f\"Sample Test Case 1: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
    "in_features = sample_lr_model.linear_layer.in_features\n",
    "out_features = sample_lr_model.linear_layer.out_features\n",
    "\n",
    "print(f\"Number of Input Features: {in_features}\")\n",
    "print(f\"Number of Output Features: {out_features}\")\n",
    "print(f\"Expected Number of Input Features: {d_input}\")\n",
    "print(f\"Expected Number of Output Features: {4}\")\n",
    "assert in_features == d_input and out_features == 4\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "d_input = 24\n",
    "num_labels=6\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = d_input,num_labels = num_labels)\n",
    "print(f\"Sample Test Case 2: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
    "in_features = sample_lr_model.linear_layer.in_features\n",
    "out_features = sample_lr_model.linear_layer.out_features\n",
    "\n",
    "print(f\"Number of Input Features: {in_features}\")\n",
    "print(f\"Number of Output Features: {out_features}\")\n",
    "print(f\"Expected Number of Input Features: {d_input}\")\n",
    "print(f\"Expected Number of Output Features: {6}\")\n",
    "assert in_features == d_input and out_features == 6\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 3: Checking if the model gives correct output\")\n",
    "test_input = torch.rand(d_input)\n",
    "model_output = sample_lr_model(test_input)\n",
    "model_output_np = model_output.detach().numpy()\n",
    "expected_output = np.array([-1.2607676, -1.8947134, -2.0088696, -2.7715783, -2.0052252, -1.4487281])\n",
    "print(f\"Model Output: {model_output_np}\")\n",
    "print(f\"Expected Output: {expected_output}\")\n",
    "\n",
    "assert np.allclose(model_output_np, expected_output, 1e-5)\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 4: Checking if the model gives correct output\")\n",
    "test_input = torch.rand(4, d_input)\n",
    "model_output = sample_lr_model(test_input)\n",
    "model_output_np = model_output.detach().numpy()\n",
    "expected_output = np.array([-1.4812257, -1.9529424, -1.8019284, -2.575539,  -2.2114434, -1.272432 ])\n",
    "print(f\"Model Output: {model_output_np}\")\n",
    "print(f\"Expected Output: {expected_output}\")\n",
    "\n",
    "assert model_output_np[0].shape == expected_output.shape and np.allclose(model_output_np[0], expected_output, 1e-5)\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587fc76",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d587fc76",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d104c0249ebb6a18b8bba2e1c0d0d7e",
     "grade": false,
     "grade_id": "cell-4ab15e4378615a15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.2: Training and Evaluating the Model (3 Marks)\n",
    "\n",
    "Write the training and evaluation script like the last time to train and evaluate topic classification model. You will need to write the entire functions on your own this time. You can refer to the code in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f106f5",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1686766897145,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "03f106f5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37224f0d2f043734ddd6abd5de4a570",
     "grade": true,
     "grade_id": "cell-b8cd46cbe3da08ec",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train(model, train_dataloader,\n",
    "          lr = 1e-3, num_epochs = 20,\n",
    "          device = \"cpu\",):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs the training loop\n",
    "\n",
    "    Inputs:\n",
    "    - model (MultinomialLogisticRegressionModel): Multinomial Logistic Regression model to be trained\n",
    "    - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
    "    - lr (float): The learning rate for the optimizer\n",
    "    - num_epochs (int): Number of epochs to train the model for.\n",
    "    - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    - model (MultinomialLogisticRegressionModel): Model after completing the training\n",
    "    - epoch_loss (float) : Loss value corresponding to the final epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae507c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1686766901437,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "dae507c4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93fc54b624c90cb889a731e6fbe33081",
     "grade": true,
     "grade_id": "cell-69ff1eb67c532051",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "349fb59d-1930-4c77-f251-818201153c9d"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "print(\"Training on 100 data points for sanity check\")\n",
    "sample_documents = train_df[\"news\"].values.tolist()[:100]\n",
    "sample_labels = train_df[\"label\"].values.tolist()[:100]\n",
    "sample_features = np.array([get_bow_sent_vec(document, wv) for document in sample_documents])\n",
    "sample_dataset = AGNewsDataset(sample_features, sample_labels)\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size=64)\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = len(sample_features[0]),num_labels=4)\n",
    "\n",
    "sample_lr_model, loss = train(sample_lr_model, sample_dataloader,\n",
    "      lr = 1e-2, num_epochs = 10,\n",
    "      device = \"cpu\")\n",
    "\n",
    "expected_loss = 0.9724720418453217\n",
    "print(f\"Final Loss Value: {loss}\")\n",
    "print(f\"Expected Loss Value: {expected_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eade345",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0eade345",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d442bd0dd0ae8b68705825068dae5aed",
     "grade": false,
     "grade_id": "cell-e384402d5adfb651",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Don't worry if the loss values do not match exactly but you should see a decreasing trend and the final value should be of the same order of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb67eb",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1686766909257,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "85eb67eb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd65e240b0a2f71ad8ae7700f9f6ad55",
     "grade": true,
     "grade_id": "cell-c6446835e1756c2d",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluates `model` on test dataset\n",
    "\n",
    "    Inputs:\n",
    "    - model (MultinomialLogisticRegressionModel): Logistic Regression model to be evaluated\n",
    "    - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Average accuracy over the test dataset\n",
    "    - preds (np.ndarray): Predictions of the model on test dataset\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model = model.eval() # Set model to evaluation model\n",
    "    accuracy = 0\n",
    "    preds = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b1c36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1686766913868,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "410b1c36",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4aae70c1bd88841ac76441d6e8f3d5a",
     "grade": true,
     "grade_id": "cell-2639c9ccaacbd8df",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "eeb02b13-5419-41dd-f6d0-207b7943f9c7"
   },
   "outputs": [],
   "source": [
    "print(f\"Testing the sample model on 100 examples for sanity check\")\n",
    "torch.manual_seed(42)\n",
    "sample_documents = test_df[\"news\"].values.tolist()[:100]\n",
    "sample_labels = test_df[\"label\"].values.tolist()[:100]\n",
    "sample_features = np.array([get_bow_sent_vec(document, wv) for document in sample_documents])\n",
    "\n",
    "sample_dataset = AGNewsDataset(sample_features,\n",
    "                            sample_labels)\n",
    "\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size = 64)\n",
    "accuracy = evaluate(sample_lr_model, sample_dataloader, device =\"cpu\")\n",
    "expected_accuracy = 0.7161458333333333\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Expected Accuracy: {expected_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dc1f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5f8dc1f3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8225076c648f4626a49080360a3a534c",
     "grade": false,
     "grade_id": "cell-460ebcb1a5f355f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have implemented the training and evaluation functions, we will train (and evaluate) 2 different models and compare their performance. The 2 models are:\n",
    "\n",
    "    - Multinomial Logistic Regression with Bag of Word2vec features\n",
    "    - Multinomial Logistic Regression with Weighted Bag of Word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b6910",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 23549,
     "status": "ok",
     "timestamp": 1686766942240,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "1a9b6910",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c93d55f4ca0e000eb7a95a42d4ada66",
     "grade": false,
     "grade_id": "cell-77b3ce9c9547870f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e4cba7fd-ad20-4dd8-aeb8-8f2b2daaf783"
   },
   "outputs": [],
   "source": [
    "print(f\"Training and Evaluating Multinomial Logistic Regression with Bag of Word2vec features\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_labels = train_df[\"label\"].values.tolist()\n",
    "test_labels = test_df[\"label\"].values.tolist()\n",
    "\n",
    "train_dataset = AGNewsDataset(train_bow_vectors, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64)\n",
    "\n",
    "test_dataset = AGNewsDataset(test_bow_vectors, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64)\n",
    "\n",
    "lr_bow_model = MultinomialLogisticRegressionModel(\n",
    "    d_input = wv.vector_size,num_labels= 4\n",
    ")\n",
    "\n",
    "lr_bow_model, loss = train(lr_bow_model, train_loader,\n",
    "      lr = 1e-2, num_epochs = 10,\n",
    "      device = device)\n",
    "\n",
    "test_accuracy = evaluate(\n",
    "    lr_bow_model, test_loader,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243ec3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22953,
     "status": "ok",
     "timestamp": 1686766990039,
     "user": {
      "displayName": "Kabir Ahuja",
      "userId": "06971643340203436870"
     },
     "user_tz": -330
    },
    "id": "5243ec3d",
    "outputId": "72ccc0c6-53b7-4b14-f1d7-0f2a6110e32d"
   },
   "outputs": [],
   "source": [
    "print(f\"Training and Evaluating Multinomial Logistic Regression with Weighted Bag of Word2vec features\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_labels = train_df[\"label\"].values.tolist()\n",
    "test_labels = test_df[\"label\"].values.tolist()\n",
    "\n",
    "train_dataset = AGNewsDataset(train_w_bow_vectors, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64)\n",
    "\n",
    "test_dataset = AGNewsDataset(test_w_bow_vectors, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64)\n",
    "\n",
    "lr_bow_model = MultinomialLogisticRegressionModel(\n",
    "    d_input = wv.vector_size, num_labels=4\n",
    ")\n",
    "\n",
    "lr_bow_model, loss = train(lr_bow_model, train_loader,\n",
    "      lr = 1e-2, num_epochs = 10,\n",
    "      device = device)\n",
    "\n",
    "test_accuracy = evaluate(\n",
    "    lr_bow_model, test_loader,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a85bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "059a85bb",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cea44f6c794702ded76826ef8548f29",
     "grade": false,
     "grade_id": "cell-3feb895a07064c4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First thing that you can notice is that these models train substantially faster than the models in Lab 1, as now we have much more lower sized sentence representations i.e. 300, compared to last time when it was equal to the size of vocabulary i.e. around 10k!\n",
    "\n",
    "Both models get around ~88% test accuracy, which is close to what we got with Bag of Words features in Lab 1 only. The reason we do not see much improvement in performance is because both models still take a (weighted) sum of the individual word vectors to obtain sentence vectors, and fails to encode any structural information as well as semantics properly. For eg. for sentiment analysis task, both of the following sentences:\n",
    "\n",
    "- it was a good movie adapted from a bad book\n",
    "- it was a bad movie adapted from a good book\n",
    "\n",
    "both of these sentences will get exact similar vector representations according to both the methods and hence the model will never be able to distinguish between the sentiment of these two sentences giving same prediction for both.\n",
    "\n",
    "In the next labs and assignments we shall see how we can learn more contextual representation of the sentences that can help us solve the task much more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ab094",
   "metadata": {
    "id": "fc0ab094"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
