{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c247ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59b55e241e3a3e68a112a35bf15fa780",
     "grade": false,
     "grade_id": "cell-23a7f8d3de35de0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 2: Fine-tuning BERT To Perform Causal Common Sense Reasoning\n",
    "\n",
    "## Due: June 28, 2023\n",
    "\n",
    "Welcome to the Assignment 2 of our course on Natural Language Processing. Similar to Lab 2, we will again be working on a common sense reasoning task and fine-tuning BERT to solve the same. Specifically, we will be looking at [Choice Of Plausible Alternatives (COPA) dataset](https://people.ict.usc.edu/~gordon/copa.html) which was created to access common-sense causal reasoning of NLP models. This assignment should flow naturally from Lab 2, and we shall see with minimal changes we will be able to adapt what we learned for SocialIQA task on COPA.\n",
    "\n",
    "Suggested Reading: \n",
    "- [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463â€“4473, Hong Kong, China. Association for Computational Linguistics.] (https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c077e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    copa_data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Assignment2/data/copa/\"\n",
    "except:\n",
    "    copa_data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/TLPNLP2023/source/Assignment2/data/copa/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242992d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b746d7065e47851c69e7bd3f33db670",
     "grade": false,
     "grade_id": "cell-00e4313e5ec6522c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b0e64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74e6a48338634eb68823113f0a3ded17",
     "grade": false,
     "grade_id": "cell-eb2f90d7f62cad18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import os\n",
    "from functools import partial\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(40) # to avoid warnings from transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04bc11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7635760d8230cbcbee34d3b84415728b",
     "grade": false,
     "grade_id": "cell-43d6067e24a242c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## COPA Dataset\n",
    "\n",
    "We start by discussing the dataset that we will making use of in today's Assignment. As described above, the COPA evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Some examples from the dataset include:\n",
    "\n",
    "```\n",
    "Premise: The man broke his toe. What was the CAUSE of this?\n",
    "Alternative 1: He got a hole in his sock.\n",
    "Alternative 2: He dropped a hammer on his foot.\n",
    "\n",
    "Premise: I tipped the bottle. What happened as a RESULT?\n",
    "Alternative 1: The liquid in the bottle froze.\n",
    "Alternative 2: The liquid in the bottle poured out.\n",
    "\n",
    "Premise: I knocked on my neighbor's door. What happened as a RESULT?\n",
    "Alternative 1: My neighbor invited me in.\n",
    "Alternative 2: My neighbor left his house.\n",
    "```\n",
    "\n",
    "Below we load the dataset in memory. Since there is no seperate training set, we use dev set for training the model and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f82e9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32a72469d10b05c018317e43d1b44cf7",
     "grade": false,
     "grade_id": "cell-61ae8095088b1abc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_copa_dataset(split=\"test\"):\n",
    "    tree = ET.parse(f\"{copa_data_dir}/copa-{split}.xml\")\n",
    "    root = tree.getroot()\n",
    "    items = root.findall(\"item\")\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for item in items:\n",
    "        data.append(\n",
    "            {\n",
    "                \"question\": item.get(\"asks-for\"),\n",
    "                \"premise\": item.find(\"p\").text,\n",
    "                \"choice1\": item.find(\"a1\").text,\n",
    "                \"choice2\": item.find(\"a2\").text,\n",
    "            }\n",
    "        )\n",
    "        labels.append(int(item.get(\"most-plausible-alternative\")) - 1)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_labels = parse_copa_dataset(\"dev\")\n",
    "test_data, test_labels = parse_copa_dataset(\"test\")\n",
    "\n",
    "print(f\"Number of Training Examples: {len(train_data)}\")\n",
    "print(f\"Number of Test Examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc04307",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a583c16326137429d7cf8380e7725dc2",
     "grade": false,
     "grade_id": "cell-a56ddf5acc609f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf24a61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70e3ecb51abfd2ca15b0b8f5f24de0e7",
     "grade": false,
     "grade_id": "cell-602fe165abcf4503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# View a sample of the dataset\n",
    "print(\"Example from dataset\")\n",
    "pprint(train_data[100], sort_dicts=False, indent=4)\n",
    "print(f\"Label: {train_labels[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167fd9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d9df755ea6f651e6459eb56e62acb7f",
     "grade": false,
     "grade_id": "cell-ced0973b2702f8c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the dataset is pretty much very similar as SocialIQA, with the main difference being that we have two answer choices instead of three. Hence, we just need to concatenate choice1 and choice2, seperately with premise and question this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c710244",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d84de84586859b327b1a02898651245f",
     "grade": false,
     "grade_id": "cell-3525f43dec94191b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the BertTokenizer from the library\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a pre-trained BERT Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "example = train_data[100]\n",
    "premise = example[\"premise\"]\n",
    "question = example[\"question\"]\n",
    "choice1 = example[\"choice1\"]\n",
    "choice2 = example[\"choice2\"]\n",
    "\n",
    "pqc1 = premise + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + choice1\n",
    "pqc2 = premise + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + choice2\n",
    "\n",
    "print(pqc1)\n",
    "print(pqc2)\n",
    "\n",
    "tokenized_pqc1 = bert_tokenizer(pqc1)\n",
    "tokenized_pqc2 = bert_tokenizer(pqc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6b904",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "706312962725684e9697682e99578266",
     "grade": false,
     "grade_id": "cell-23562a8e437add81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Setting up Custom Datasets and Dataloaders (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627768e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3e2152aa60e7a07ddbc83e96159c250",
     "grade": false,
     "grade_id": "cell-a3cb841c571f2b47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.1: Custom Dataset Class (2 Marks)\n",
    "\n",
    "Similar to Lab 2, you will start by implementing a custom Dataset class for COPA dataset. The only difference will be that `__getitem__` should return tokenized outputs corresponding to the two choices choice1 and choice2, instead of three like in the case of SocialIQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffd2ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "103e2504ffda268e015c164fa3c1666e",
     "grade": false,
     "grade_id": "cell-84536c665947d263",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class COPABertDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels, bert_variant = \"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Constructor for the `COPABertDataset` class. Stores the `data` and `labels` which can then be used by\n",
    "        other methods. Also initializes the tokenizer\n",
    "        \n",
    "        Inputs:\n",
    "            - data (list) : A list COPA dataset examples\n",
    "            - labels (list): A list of answer labels corresponding to each example\n",
    "            - bert_variant (str): A string indicating the variant of BERT to be used.\n",
    "        \"\"\"\n",
    "        self.data = None\n",
    "        self.labels = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset\n",
    "        \"\"\"\n",
    "        length = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the training example corresponding to COPA example present at the `idx` position in the dataset\n",
    "        \n",
    "        Inputs:\n",
    "            - idx (int): Index corresponding to the dataset example to be returned\n",
    "            \n",
    "        Returns:\n",
    "            - tokenized_input_dict (dict(str, dict)): A dictionary corresponding to tokenizer outputs for the two resulting sequences due to each answer choices as described above\n",
    "            - label (int): Answer label for the corresponding sentence. 0 for first answer choice and 1 for second answer choice.\n",
    "        \n",
    "        Example Output:\n",
    "            - tokenized_input_dict: {\n",
    "                \"choice1\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 4855, 1037, 6411, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
    "                \"choice2\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 17733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
    "            }\n",
    "            - label: 0\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_input_dict = {\"choice1\": None, \"choice2\": None}\n",
    "        label = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return tokenized_input_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c8dea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b16eaab10b679a51aba4d5a1b3bfbabf",
     "grade": true,
     "grade_id": "cell-0ff4b72642c7bacb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "sample_dataset = COPABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n",
    "\n",
    "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
    "dataset_len= len(sample_dataset)\n",
    "expected_len = 2\n",
    "print(f\"Dataset Length: {dataset_len}\")\n",
    "print(f\"Expected Length: {expected_len}\")\n",
    "assert len(sample_dataset) == expected_len\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
    "sample_idx = 0\n",
    "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_tokenized_input_dict = {'choice1': {'input_ids': [101, 2026, 2303, 3459, 1037, 5192, 2058, 1996, 5568, 1012, 102, 3426, 102, 1996, 3103, 2001, 4803, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
    " 'choice2': {'input_ids': [101, 2026, 2303, 3459, 1037, 5192, 2058, 1996, 5568, 1012, 102, 3426, 102, 1996, 5568, 2001, 3013, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
    "expected_label = 0\n",
    "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
    "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
    "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "\n",
    "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
    "sample_idx = 1\n",
    "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_tokenized_input_dict = {'choice1': {'input_ids': [101, 1996, 2450, 25775, 2014, 2767, 1005, 1055, 3697, 5248, 1012, 102, 3426, 102, 1996, 2450, 2354, 2014, 2767, 2001, 2183, 2083, 1037, 2524, 2051, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
    " 'choice2': {'input_ids': [101, 1996, 2450, 25775, 2014, 2767, 1005, 1055, 3697, 5248, 1012, 102, 3426, 102, 1996, 2450, 2371, 2008, 2014, 2767, 2165, 5056, 1997, 2014, 16056, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
    "\n",
    "expected_label = 0\n",
    "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
    "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
    "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\")\n",
    "sample_dataset = COPABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-cased\")\n",
    "sample_idx = 0\n",
    "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_tokenized_input_dict = {'choice1': {'input_ids': [101, 2026, 2303, 3459, 1037, 5192, 2058, 1996, 5568, 1012, 102, 3426, 102, 1996, 3103, 2001, 4803, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
    " 'choice2': {'input_ids': [101, 2026, 2303, 3459, 1037, 5192, 2058, 1996, 5568, 1012, 102, 3426, 102, 1996, 5568, 2001, 3013, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
    "expected_label = 0\n",
    "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
    "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
    "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2e35e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc5d11b79dc8589bdd3573c800432f66",
     "grade": false,
     "grade_id": "cell-cd1f8ca8abf3200c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now create Dataset instances for both training and dev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3994ab0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1623dbcdbe53c5fe99f0e0be7d6fb18",
     "grade": false,
     "grade_id": "cell-8429b84248f83374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = COPABertDataset(train_data, train_labels, bert_variant=\"bert-base-uncased\")\n",
    "test_dataset = COPABertDataset(test_data, test_labels, bert_variant=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54d73b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f583ee1938e3c3778142dce9a1badbf5",
     "grade": false,
     "grade_id": "cell-f884c5a2bf862a62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.2: Custom `collate_fn` Class (2 Marks)\n",
    "\n",
    "Similar to Lab 2, you will now implement a custom `colate_fn` for the `COPABertDataset` class. Remember, a `colate_fn` informs a dataloader on how to construct batches from a list of sequences in a batch. Implement `collate_fn` that takes a batch which is a list of tuples of the form `(tokenized_input_dict, label)` and constructs the batches of following:\n",
    "- `input_ids_dict`: A dictionary containing batch of `input_ids` tensors corresponding to both choice1 and choice2. You will need to do padding here,\n",
    "- `attn_mask_dict`: A dictionary containing batch of `attention_mask` tensors corresponding to both choice1 and choice2. You will need to do padding here,\n",
    "- `labels`: A tensor of shape [batch_size, ] containing the labels for each example in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6fb0b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c213845b8ff719000f72eee0c139f318",
     "grade": false,
     "grade_id": "cell-7d9d1bf7151de3f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(tokenizer, batch):\n",
    "    \"\"\"\n",
    "    Collate function to be used when creating a data loader for the COPA dataset.\n",
    "    :param tokenizer: The tokenizer to be used to tokenize the inputs.\n",
    "    :param batch: A list of tuples of the form (tokenized_input_dict, label)\n",
    "    :return: \n",
    "        - A tuple of the form (input_ids_dict, attn_mask_dict, labels) as described above\n",
    "    \"\"\"\n",
    "\n",
    "    colated_batch = (None, None, None)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return colated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181b5ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef7d170b9624549c959e4dbd070c7ee3",
     "grade": true,
     "grade_id": "cell-9081bf7fa9e64049",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "sample_dataset = COPABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n",
    "batch = [sample_dataset.__getitem__(0), sample_dataset.__getitem__(1)]\n",
    "\n",
    "\n",
    "print(f\"Sample Test Case 1: Checking if the return output of `collate_fn` is of the correct type\")\n",
    "colated_batch = collate_fn(bert_tokenizer, batch)\n",
    "print(f\"Output type: {type(colated_batch)}\")\n",
    "assert (type(colated_batch) == tuple)\n",
    "print(f\"Tuple Length: {len(colated_batch)}\")\n",
    "assert (len(colated_batch) == 3)\n",
    "print(f\"Tuple 0th element type: {type(colated_batch[0])}\")\n",
    "assert (type(colated_batch[0]) == dict)\n",
    "print(f\"Tuple 1st element type: {type(colated_batch[1])}\")\n",
    "assert (type(colated_batch[1]) == dict)\n",
    "print(f\"Tuple 2nd element type: {type(colated_batch[2])}\")\n",
    "assert (type(colated_batch[2]) == torch.Tensor)\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 2: Checking if the return output of `collate_fn` is of the correct shape\")\n",
    "print(f\"Tuple 0th element shape for choice1: {colated_batch[0]['choice1'].shape}\")\n",
    "assert (colated_batch[0]['choice1'].shape == torch.Size([2, 27]))\n",
    "print(f\"Tuple 0th element shape for choice2: {colated_batch[0]['choice2'].shape}\")\n",
    "assert (colated_batch[0]['choice2'].shape == torch.Size([2, 27]))\n",
    "\n",
    "print(f\"Tuple 1st element shape for choice1: {colated_batch[1]['choice1'].shape}\")\n",
    "assert (colated_batch[1]['choice1'].shape == torch.Size([2, 27]))\n",
    "print(f\"Tuple 1st element shape for choice2: {colated_batch[1]['choice2'].shape}\")\n",
    "assert (colated_batch[1]['choice2'].shape == torch.Size([2, 27]))\n",
    "\n",
    "print(f\"Tuple 2nd element shape: {colated_batch[2].shape}\")\n",
    "assert (colated_batch[2].shape == torch.Size([2]))\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 3: Checking if the return output of `collate_fn` is of the correct values\")\n",
    "tup0_choice1_expected = torch.tensor([[  101,  2026,  2303,  3459,  1037,  5192,  2058,  1996,  5568,  1012,\n",
    "           102,  3426,   102,  1996,  3103,  2001,  4803,  1012,   102,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0],\n",
    "        [  101,  1996,  2450, 25775,  2014,  2767,  1005,  1055,  3697,  5248,\n",
    "          1012,   102,  3426,   102,  1996,  2450,  2354,  2014,  2767,  2001,\n",
    "          2183,  2083,  1037,  2524,  2051,  1012,   102]])\n",
    "print(f\"Tuple 0th element predicted values for choice1: {colated_batch[0]['choice1']}\")\n",
    "print(f\"Tuple 0th element expected values for choice1: {tup0_choice1_expected}\")\n",
    "assert (torch.allclose(colated_batch[0]['choice1'], tup0_choice1_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548c845",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfae6a1d238b0ffba62a6ed394323100",
     "grade": false,
     "grade_id": "cell-734504dd3028188a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have defined the collate_fn, lets create the dataloaders. It is common to use smaller batch size while fine-tuning these big models, as they occupy quite a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, train_dataset.tokenizer))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, test_dataset.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b42884",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n",
    "print(f\"batch_input_ids:\\n {batch_input_ids}\")\n",
    "print(f\"batch_attn_mask:\\n {batch_attn_mask}\")\n",
    "print(f\"batch_labels:\\n {batch_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849edfc0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b50925746f31c6b46f0e477351963287",
     "grade": false,
     "grade_id": "cell-21c75958035402dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Implementing and Training BERT-based Multiple Choice Classifier (6 Marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BertModel from the library\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create an instance of pretrained BERT\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f03f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea617ac9cc550912593440ba288eb4bb",
     "grade": false,
     "grade_id": "cell-c224cab96a9915ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1: Implementing BERT-based Classifier for Multiple Choice Classification (2 Marks)\n",
    "\n",
    "We will be using trhe same exact architecture as SocialIQA dataset here as well i.e. we have the BERT model as the backbone, using which we obtain the contextualized representation of the [premise, question, answer] sequence. We then use the \\[CLS\\] token's embedding as the sequence representation and feed it to a 2 layer MLP (Linear(768, 768) -> ReLU -> Linear(768, 1)) that scores the answer.\n",
    "\n",
    "![architecture](https://i.ibb.co/hVmS9Qx/siqa-bert-arch-excalli.png)\n",
    "\n",
    "The only change this time will be that to predict the correct answer, we need to score each of the two choices each instead of the three answers. Afterwards, like last time we ormalize the scores for the choices by applying softmax, that gives us the probability of each option being the correct answer.\n",
    "\n",
    "\n",
    "<!-- ![forward pass](https://i.ibb.co/r3SrLHY/siqa-bert-forward-excalli.png) -->\n",
    "\n",
    "Implement the architecture and forward pass in `BertMultiChoiceClassifierModel` class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f626042",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a12e0a739daafee347c07821a6fb6f",
     "grade": false,
     "grade_id": "cell-7930c03ec3b56775",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BertMultiChoiceClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Define the architecture of Bert-Based mulit-choice classifier.\n",
    "        You will mainly need to define 3 components, first a BERT layer\n",
    "        using `BertModel` from transformers library,\n",
    "        a two layer MLP layer to map the representation from Bert to the output i.e. (Linear(d_hidden, d_hidden) -> ReLU -> Linear(d_hidden, 1)),\n",
    "        and a log sftmax layer to map the scores to a probabilities\n",
    "        \n",
    "        Inputs:\n",
    "            - d_hidden (int): Size of the hidden representations of bert\n",
    "            - bert_variant (str): BERT variant to use\n",
    "        \"\"\"\n",
    "        super(BertMultiChoiceClassifierModel, self).__init__()\n",
    "        self.bert_layer = None\n",
    "        self.mlp_layer = None\n",
    "        self.log_softmax_layer = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, input_ids_dict, attn_mask_dict):\n",
    "        \"\"\"\n",
    "        Forward Passes the inputs through the network and obtains the prediction\n",
    "        \n",
    "        Inputs:\n",
    "            - input_ids_dict (dict(str,torch.tensor)): A dictionary containing input_ids corresponding to each answer choice. Keys are choice1 and choice2, and value is a torch tensor of shape [batch_size, seq_len]\n",
    "                                        representing the sequence of token ids\n",
    "            - attn_mask_dict (dict(str,torch.tensor)): A dictionary containing attention mask corresponding to each answer choice. Keys are choice1 and choice2, and value is a torch tensor of shape [batch_size, seq_len]\n",
    "                                        \n",
    "        Returns:\n",
    "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
    "                                        \n",
    "        \"\"\"\n",
    "        output = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65d7c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c69fc8204eae96054f5033e69364d39",
     "grade": true,
     "grade_id": "cell-cf9b5db5de53eeac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running Sample Test Cases!\")\n",
    "torch.manual_seed(42)\n",
    "model = BertMultiChoiceClassifierModel()\n",
    "\n",
    "print(\"Sample Test Case 1\")\n",
    "batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n",
    "bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n",
    "expected_bert_out = np.array([[-0.6788204 , -0.7076822 ],\n",
    "       [-0.6965156 , -0.68979   ],\n",
    "       [-0.69193876, -0.69435704],\n",
    "       [-0.69665235, -0.6896543 ],\n",
    "       [-0.69182485, -0.6944712 ],\n",
    "       [-0.69328713, -0.69300735],\n",
    "       [-0.698088  , -0.68823063],\n",
    "       [-0.6868452 , -0.6994891 ],\n",
    "       [-0.6811901 , -0.70524895],\n",
    "       [-0.69229984, -0.69399524],\n",
    "       [-0.6957624 , -0.6905388 ],\n",
    "       [-0.70050216, -0.68584585],\n",
    "       [-0.71216923, -0.67448026],\n",
    "       [-0.68982404, -0.69648135],\n",
    "       [-0.6903453 , -0.6959569 ],\n",
    "       [-0.68135405, -0.705081  ]])\n",
    "print(f\"Model Output: {bert_out}\")\n",
    "print(f\"Expected Output: {expected_bert_out}\")\n",
    "\n",
    "assert bert_out.shape == expected_bert_out.shape\n",
    "assert np.allclose(bert_out, expected_bert_out, 1e-4)\n",
    "print(\"Test Case Passed! :)\")\n",
    "print(\"******************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 2\")\n",
    "batch_input_ids, batch_attn_mask, batch_labels = next(iter(test_loader))\n",
    "bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n",
    "expected_bert_out = np.array([[-0.7176868 , -0.6691954 ],\n",
    "       [-0.68573594, -0.7006138 ],\n",
    "       [-0.68500787, -0.7013533 ],\n",
    "       [-0.6948556 , -0.6914417 ],\n",
    "       [-0.716335  , -0.6704848 ],\n",
    "       [-0.69969493, -0.68664205],\n",
    "       [-0.69120455, -0.6950935 ],\n",
    "       [-0.7088388 , -0.677698  ],\n",
    "       [-0.6794593 , -0.707025  ],\n",
    "       [-0.69712466, -0.68918544],\n",
    "       [-0.70202625, -0.68434626],\n",
    "       [-0.6890422 , -0.6972691 ],\n",
    "       [-0.6993814 , -0.68695164],\n",
    "       [-0.69202346, -0.69427204],\n",
    "       [-0.69150096, -0.694796  ],\n",
    "       [-0.6884656 , -0.6978507 ]])\n",
    "print(f\"Model Output: {bert_out}\")\n",
    "print(f\"Expected Output: {expected_bert_out}\")\n",
    "\n",
    "assert bert_out.shape == expected_bert_out.shape\n",
    "assert np.allclose(bert_out, expected_bert_out, 1e-4)\n",
    "print(\"Test Case Passed! :)\")\n",
    "print(\"******************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f523b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f479e8f52b781d14ed0a70460ab9f436",
     "grade": false,
     "grade_id": "cell-d26d1e16847283ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.2: Training and Evaluating the Model (3 Marks)\n",
    "\n",
    "Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model as in Lab 2. You will need to implement the `train` and `evaluate` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e43b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d05d3d6a4d2eccabf70a9caa0af6d82",
     "grade": true,
     "grade_id": "cell-37de7065d6cb7392",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluates `model` on test dataset\n",
    "\n",
    "    Inputs:\n",
    "        - model (BertMultiChoiceClassifierModel): A BERT based multiple choice classification model to be evaluated\n",
    "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
    "\n",
    "    Returns:\n",
    "        - accuracy (float): Average accuracy over the test dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    accuracy = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, test_dataloader,\n",
    "          lr = 1e-5, num_epochs = 3,\n",
    "          device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Runs the training loop. Define the loss function as BCELoss like the last tine\n",
    "    and optimizer as Adam and traine for `num_epochs` epochs.\n",
    "\n",
    "    Inputs:\n",
    "        - model (BertMultiChoiceClassifierModel):  A BERT based multiple choice classification model to be trained\n",
    "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
    "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
    "        - lr (float): The learning rate for the optimizer\n",
    "        - num_epochs (int): Number of epochs to train the model for.\n",
    "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        - test_accuracy (float): Test accuracy corresponding to the last epoch\n",
    "        Note that we are not doing model selection here since we do not have access to a validation set for this task.\n",
    "          It is not a good practice to do model selection on the test set. Hence, we just return the performance we get after training the model\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    model = model.to(device)\n",
    "    test_accuracy = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b6cf0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfcb09534574ad8f7bcdd72afe2d9500",
     "grade": false,
     "grade_id": "cell-33834a90b1557f37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = BertMultiChoiceClassifierModel()\n",
    "test_acc = train(model, train_loader, test_loader, num_epochs = 10, lr = 5e-6, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397015f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33f16716fb13edc80f70e482d4e18948",
     "grade": false,
     "grade_id": "cell-eb742cd2053baea4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should expect about ~65% test accuracy. Note that the model quickly overfits to the dataset in this case, i.e. the training loss reduces dramatically, but there isn't much improvement in the test accuracy after the first epoch. This happens because our training data consists of just 500 examples, which is usually not sufficient for training these large models. Next, we try out a simple strategy to improve the performance drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc730b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e61a8c3dcbdb0f567669549d3f56c21",
     "grade": false,
     "grade_id": "cell-2b8b200f1b89eb3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.3: Continued Fine-tuning of BERT trained on SocialIQA Dataset (1 Mark)\n",
    "\n",
    "In Lab2, we fine-tuned BERT on SocialIQA, which is also a common sense reasoning task and has a much larger training set. Deep learning models exhibhit a remarkable property of transfer learning where we can leverage a model trained on task to transfer it's knowledge for learning a new task much more effectively. The idea is that training on SocialIQA dataset would have endowed our model with some common-sense reasoning capabilities, which we can leverage to learn COPA task as well. \n",
    "\n",
    "Below, you are needed to load the model that you trained in Lab2 and the train that model instead. You can read about how to load pre-trained models in pytorch [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html). Once you load the model, to train it, just call the `train` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b29f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b06940af3a66df6a42c14575a38c51d0",
     "grade": true,
     "grade_id": "cell-03df891689d9741c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = BertMultiChoiceClassifierModel()\n",
    "model_path = \"../Lab2/models/siqa_bert-base-uncased/model.pt\" # Change this to the path of your model trained in Lab2.\n",
    "\n",
    "# Step 1: Load the pre-trained model weights\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Step 2: Train the loaded model on COPA dataset\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3032d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8b499f11463eb9a57d5a7032519b661",
     "grade": false,
     "grade_id": "cell-f7815c12e73d1b7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should expect ~77% accuracy with this, which is quite a large increase over the original 65% accuracy that we obtained by training the model from scratch. This illustrates the effectiveness of transfer-learning for NLP tasks, specially when the two tasks are related as they were in this case. Transfer learning had been the dominant paradigm in NLP since 2018. However, recently we have been witnessing a new paradigm emerge called \"Prompting\", which has taken the NLP community and in many ways the whole world by a storm. In the next lab and assignments, we will learn more about this new paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef516c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
