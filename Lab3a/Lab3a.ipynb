{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb86eec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53485c9f7bdce1444b82b50b44bf4156",
     "grade": false,
     "grade_id": "cell-91ebf1488c04a4d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 3: Multilingual BERT and Zero-Shot Transfer\n",
    "\n",
    "## June 27, 2023\n",
    "\n",
    "Welcome to the third lab of the course. In this assignment we will learn how to fine-tune a multilingual BERT or mBERT model on a Natural Language Inference task [XNLI](https://arxiv.org/abs/1809.05053). We will fine-tune the model on English Training data and then evaluate the performance of the fine-tuned models on different languages demonstrating the zero-shot capabilities of mBERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Lab3a/data/xnli\"\n",
    "except:\n",
    "    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/TLPNLP2023/source/Lab3a/data/xnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c0d05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b521099e0bd56fdab244087ccaa1d203",
     "grade": false,
     "grade_id": "cell-46aef099f8e4ea7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382573bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2a1e7bfeecd628b51ea90d2e6f6877d",
     "grade": false,
     "grade_id": "cell-1062334620127fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e4a45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84d7f50f97de9f380bb9e9cfba0c3395",
     "grade": false,
     "grade_id": "cell-041213c39777b5c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## XNLI: Task Description\n",
    "\n",
    "XNLI is a multilingual benchmark for Natural Language Inference, that contains training data available in English which was obtained from the popular [MNLI](https://cims.nyu.edu/~sbowman/multinli/), and test and dev sets available for 15 different languages. In NLI, we are given two sentences, one is a premise and other an hypothesis, and the task is to predict whether the hypothesis is i) entialed in the premise, or ii) contradicts the premise, or iii) neutral to the premise. \n",
    "\n",
    "<img src=\"https://i.ibb.co/bd4P20K/nli-examples.jpg\" alt=\"nli-examples\" border=\"0\">\n",
    "\n",
    "This makes NLI a multi-class classification task where we want to predict the correct label out of the three possible classes. We start by loading the dataset into memory. The training set in XNLI is comparitively huge with around 400k examples, which can lead to higher training times. Hence for the purpose of this assignment we will work with a fraction of the full data i.e. ~40k examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf1fe5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b38f3039796b80be9d8e3316a8ad1ba8",
     "grade": false,
     "grade_id": "cell-0003d05313e8c61e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_xnli_dataset(lang, split = \"train\"):\n",
    "    filename = os.path.join(data_dir, f\"{split}-{lang}.tsv\")\n",
    "    sentence1s = []\n",
    "    sentence2s = []\n",
    "    labels = []\n",
    "    with open(filename) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            row = line.split(\"\\t\")\n",
    "            sentence1 = row[0]\n",
    "            sentence2 = row[1]\n",
    "            label = row[2].split(\"\\n\")[0]\n",
    "            sentence1s.append(sentence1)\n",
    "            sentence2s.append(sentence2)\n",
    "            labels.append((label))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"premise\": sentence1s,\n",
    "        \"hypothesis\" : sentence2s,\n",
    "        \"label\" : labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b9f05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "201fd35a258af684f16835dc96d9ed32",
     "grade": false,
     "grade_id": "cell-bf5a68175cf3ee9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Training data in english\n",
    "train_en_data = load_xnli_dataset(\"en\", \"train\")[:40000]\n",
    "\n",
    "#Like last assignment we will use split the training data to get some validation examples as well\n",
    "train_en_data, val_en_data = train_test_split(train_en_data, test_size=0.05)\n",
    "\n",
    "print(f\"Number of examples in training data: {len(train_en_data)}\")\n",
    "print(f\"Number of examples in validation data: {len(val_en_data)}\")\n",
    "\n",
    "train_en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96957733",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "177a9d3016915e1bd0d92851c6a984cd",
     "grade": false,
     "grade_id": "cell-3134142c0522246b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Test data in other languages\n",
    "test_langs = [\"ar\", \"bg\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hi\", \"ru\", \"sw\", \"th\", \"tr\", \"ur\", \"vi\", \"zh\"]\n",
    "\n",
    "lang2test_df = {lang : load_xnli_dataset(lang, \"dev\") for lang in test_langs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc71b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "526807fb3e424cb3a6e5f508453dff0b",
     "grade": false,
     "grade_id": "cell-136d9fe03dce3b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of Test examples: {len(lang2test_df['en'])}\")\n",
    "lang2test_df[\"en\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ebef0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ade4d187289c6e7f62f6bcd4c50cf7e",
     "grade": false,
     "grade_id": "cell-3da194bc84d86655",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for lang, test_df in lang2test_df.items():\n",
    "    print(f\"{lang} test set:\")\n",
    "    print(test_df.head())\n",
    "    print(\"***************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc90924",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f75d7f46a31d2cf1bf425a9a2be0935",
     "grade": false,
     "grade_id": "cell-013e2371157f4782",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## mBERT using HuggingFace's transformers library\n",
    "\n",
    "mBERT is a multilingual variant of BERT, which is trained on wikipedia articles in around [100 languages](BertTokenizer). Like monolingual BERT the transformers library also provides pre-trained models and tokenizers for multilingual BERT. To create an instance of one, we only need to specify `\"bert-base-multilingual-cased\"` or `\"bert-base-multilingual-uncased\"` in `BertTokenizer.from_pretrained` and `BertModel.from_pretrained` methods and that's it! See examples below for a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b8b04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "278019f64d93fd2db2a4f2cc159f7482",
     "grade": false,
     "grade_id": "cell-3a062d7c88d10ac9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddbd7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1261f9972aa9c677c445c52f91c96d38",
     "grade": false,
     "grade_id": "cell-535cdd20dca1dc48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ad78a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b407c6095c2d9afc29803479ba719da",
     "grade": false,
     "grade_id": "cell-400af0375928d1a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_tokenizer.tokenize(\"thinking machines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeed8c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88b1dfe7d15c6e96bc26f78d3eee8996",
     "grade": false,
     "grade_id": "cell-842ccbf6a53421a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_tokenizer.tokenize(\"maquinas de pensar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7157dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8e1cbf8849f71685502e1b8e3cc3d5d",
     "grade": false,
     "grade_id": "cell-2b6fe9c343e0ec15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_tokenizer.tokenize(\"सोच मशीन\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fe02a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d944dd50fcaf88293da17de2d940522",
     "grade": false,
     "grade_id": "cell-eca329091fb26c50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see mBERT's tokenizer works on different languages. We can similarly load a pretrained mbert model and feed data in different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403c05e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0daa6cf2ab4b0e27d60c1d4848767b3f",
     "grade": false,
     "grade_id": "cell-e915621e74adbeef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63671ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2551fd5abbe85a749f1e7ada9d9a7b2d",
     "grade": false,
     "grade_id": "cell-b71102b681349429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mbert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bce87e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ba0a6bc0db5a35c9598cb104d0384d1",
     "grade": false,
     "grade_id": "cell-8af866d62658bfa4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see the architecture is identical to the original BERT model. The only thing that is different is the shape of word_embeddings which is 105879 X 768, meaning there are 105879 unique tokens supported by mBERT (uncased). In contrast BERT (uncased) supports 30522 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent = \"thinking machines\"\n",
    "tokenizer_output = mbert_tokenizer(en_sent, return_tensors=\"pt\")\n",
    "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "\n",
    "mbert_model(input_ids, attention_mask = attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a3a77",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "575947d93fc67a7a69c0431fa5c361dd",
     "grade": false,
     "grade_id": "cell-864d2651e08b1f53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "es_sent = \"maquinas de pensar\"\n",
    "tokenizer_output = mbert_tokenizer(es_sent, return_tensors=\"pt\")\n",
    "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "\n",
    "mbert_model(input_ids, attention_mask = attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d2bf6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bc62d212402b7026542d09ef180f69a",
     "grade": false,
     "grade_id": "cell-b422899d3425a2c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hi_sent = \"सोच मशीन\"\n",
    "tokenizer_output = mbert_tokenizer(hi_sent, return_tensors=\"pt\")\n",
    "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
    "\n",
    "mbert_model(input_ids, attention_mask = attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877901b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "758afc3128c3c2bcd3fdaccb4fd23bdf",
     "grade": false,
     "grade_id": "cell-9e5d50972307ba7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hence, we can very easily use mBERT for generating predictions on texts written in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f23d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5dd28d2a38af65a242340afb6d2d1eb1",
     "grade": false,
     "grade_id": "cell-cdfd3a0891636fbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Fine-tune mBERT on XNLI\n",
    "\n",
    "We can now start fine-tuning mBERT on this dataset. We will start by defining the custom `Dataset` class for the task and then define the model and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6240",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07f84bb5cd0b23d4484af5b2e7469b2d",
     "grade": false,
     "grade_id": "cell-6fca49793918e102",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.1: Custom Dataset Class (15 minutes)\n",
    "\n",
    "Like in the previous assignments, implement the `XNLImBertDataset` class below that processes and stores the data as well as provides a way to iterate through the dataset. The details about various methods in the class are mentioned in their docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d8f79",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f123ca6ae3c2f7fd4a0d53269b6a0dfe",
     "grade": false,
     "grade_id": "cell-bd53750d1e2086fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class XNLImBertDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, premises,\n",
    "                 hypotheses,\n",
    "                 labels,\n",
    "                 max_length,\n",
    "                mbert_variant = \"bert-base-multilingual-uncased\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Constructor for the `XNLImBertDataset` class. Stores the `premises`, `hypotheses` and `labels`\n",
    "        which can then be used by other methods. Also initializes the tokenizer.\n",
    "        \n",
    "        Inputs:\n",
    "            - premises (list) : A list of sentences constituting the premise in each example\n",
    "            - hypotheses (list) : A list of sentences constituting the hypothesis in each example\n",
    "            - labels (list) : A list of labels denoting for each premise-hypothesis pair.\n",
    "            - max_length (int): Maximum length of the encoded sequence.  \n",
    "                                If number of tokens are lower than `max_length` add padding otherwise truncate\n",
    "        \n",
    "        \n",
    "        Note that labels are in the form of strings \"entailment\", \"contradiction\" and \"neutral\". For training the\n",
    "        models we will want the labels in the numeric form, so you should define a mapping from the text label\n",
    "        to a numeric id. You should order the labels in alphabetical order while defining the mapping i.e. \n",
    "        contadiction -> 0, entailment -> 1, \"neutral\" - > 2 (such that we have consistency across everyone).\n",
    "        \n",
    "        Also note that we have a `max_length` argument today. This is to ensure that all sequences in the batch size.\n",
    "        This way we will not need to define a seperate collate_fn like we did in the previous assignments / labs.\n",
    "        You may want to look up how to pad upto a maximum length (padding, max_length, and truncate arguments while calling the tokenizer might be useful)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.premises = None\n",
    "        self.hypotheses = None\n",
    "        self.labels = None\n",
    "        self.max_length = None\n",
    "        self.tokenizer = None\n",
    "        self.label2id = None # Define it as a dictionary\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset\n",
    "        \"\"\"\n",
    "        length = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        \n",
    "        Returns the features and label corresponding to the the `idx` entry in the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "            - idx (int): Index corresponding to the sentence_pair,label to be returned\n",
    "        \n",
    "        Returns:\n",
    "            - input_ids (torch.tensor): Indices of the tokens in the sentence pair.\n",
    "                                        Shape of the tensor should be (`seq_len`,)\n",
    "            - mask (torch.tensor): Attention mask indicating which tokens are padded.\n",
    "            - label (int): Label for the premise-hypothesis pair\n",
    "            \n",
    "        Hint: We have 2 sentences in a pair which must be concatenated using the [SEP] token before we tokenize and encode them\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        input_ids = None\n",
    "        mask = None\n",
    "        label = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return input_ids.squeeze(0), mask.squeeze(0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ba52b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7abf56bc933af6fa0d5bfdb8eda3db9",
     "grade": true,
     "grade_id": "cell-986e61caa912e8fa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
    "                    \"An older and younger man smiling.\",\n",
    "                   \"A soccer game with multiple males playing.\"\n",
    "                    ]\n",
    "sample_hypotheses = [\"The man is sleeping.\",\n",
    "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
    "                    \"Some men are playing a sport.\"]\n",
    "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "sample_max_len = 32\n",
    "sample_dataset = XNLImBertDataset(\n",
    "    sample_premises,\n",
    "    sample_hypotheses,\n",
    "    sample_labels,\n",
    "    sample_max_len\n",
    ")\n",
    "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
    "dataset_len= len(sample_dataset)\n",
    "expected_len = len(sample_labels)\n",
    "print(f\"Dataset Length: {dataset_len}\")\n",
    "print(f\"Expected Length: {expected_len}\")\n",
    "assert len(sample_dataset) == len(sample_premises)\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
    "sample_idx = 0\n",
    "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_input_ids =  torch.tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
    "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
    "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0])\n",
    "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0])\n",
    "expected_label = 0\n",
    "print(f\"input_ids:\\n {input_ids}\")\n",
    "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
    "assert (expected_input_ids == input_ids).all()\n",
    "\n",
    "print(f\"mask:\\n {mask}\")\n",
    "print(f\"Expected mask:\\n {expected_mask}\")\n",
    "assert (expected_mask == mask).all()\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
    "sample_idx = 1\n",
    "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_input_ids = torch.tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
    "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
    "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
    "            0,     0])\n",
    "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 0, 0, 0, 0, 0])\n",
    "expected_label = 2\n",
    "print(f\"input_ids:\\n {input_ids}\")\n",
    "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
    "assert (expected_input_ids == input_ids).all()\n",
    "\n",
    "print(f\"mask:\\n {mask}\")\n",
    "print(f\"Expected mask:\\n {expected_mask}\")\n",
    "assert (expected_mask == mask).all()\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "\n",
    "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\")\n",
    "sample_idx = 2\n",
    "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_input_ids = torch.tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
    "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0])\n",
    "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0])\n",
    "expected_label = 1\n",
    "print(f\"input_ids:\\n {input_ids}\")\n",
    "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
    "assert (expected_input_ids == input_ids).all()\n",
    "\n",
    "print(f\"mask:\\n {mask}\")\n",
    "print(f\"Expected mask:\\n {expected_mask}\")\n",
    "assert (expected_mask == mask).all()\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "\n",
    "\n",
    "sample_premises = [\"एक आदमी किसी पूर्वी एशियाई देश में एक आकृति की वर्दी का निरीक्षण करता है।\",\n",
    "                    \"एक बूढ़ा और छोटा आदमी मुस्कुरा रहा है।\",\n",
    "                   \"एक फ़ुटबॉल खेल जिसमें कई पुरुष खेल रहे हैं।\"\n",
    "                    ]\n",
    "sample_sentence2s = [\"आदमी सो रहा है।\",\n",
    "                     \"फर्श पर खेल रही बिल्लियों को देखकर दो आदमी मुस्कुरा रहे हैं और हंस रहे हैं।\",\n",
    "                    \"कुछ पुरुष कोई खेल खेल रहे हैं।\"\n",
    "                    ]\n",
    "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "sample_max_len = 36\n",
    "sample_dataset = XNLImBertDataset(\n",
    "    sample_premises,\n",
    "    sample_sentence2s,\n",
    "    sample_labels,\n",
    "    sample_max_len\n",
    ")\n",
    "\n",
    "print(f\"Sample Test Case 5: Checking for hindi\")\n",
    "sample_idx = 1\n",
    "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "expected_input_ids =  torch.tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
    "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
    "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
    "        91329, 19086, 29931,   533, 13764,   102])\n",
    "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "expected_label = 2\n",
    "print(f\"input_ids:\\n {input_ids}\")\n",
    "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
    "assert (expected_input_ids == input_ids).all()\n",
    "\n",
    "print(f\"mask:\\n {mask}\")\n",
    "print(f\"Expected mask:\\n {expected_mask}\")\n",
    "assert (expected_mask == mask).all()\n",
    "\n",
    "print(f\"label:\\n {label}\")\n",
    "print(f\"Expected label:\\n {expected_label}\")\n",
    "assert expected_label == label\n",
    "\n",
    "print(\"Sample Test Case Passed!\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863fe4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd322812bdc6c132e70b0461cbb1dea4",
     "grade": false,
     "grade_id": "cell-e54399bd0c198789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Initialize dataset and dataloaders for english training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913533",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9f65471ed3d4b5013ea1e3dfa7d75a8",
     "grade": false,
     "grade_id": "cell-28f5edeee6719aee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "batch_size = 8\n",
    "\n",
    "train_en_premises, train_en_hypotheses = train_en_data[\"premise\"].values, train_en_data[\"hypothesis\"].values\n",
    "train_en_labels = train_en_data[\"label\"].values\n",
    "\n",
    "val_en_premises, val_en_hypotheses = val_en_data[\"premise\"].values, val_en_data[\"hypothesis\"].values\n",
    "val_en_labels = val_en_data[\"label\"].values\n",
    "\n",
    "train_en_dataset = XNLImBertDataset(train_en_premises, train_en_hypotheses, train_en_labels, max_seq_len)\n",
    "val_en_dataset = XNLImBertDataset(val_en_premises, val_en_hypotheses, val_en_labels, max_seq_len)\n",
    "\n",
    "train_en_dataloader = DataLoader(train_en_dataset, batch_size = batch_size)\n",
    "val_en_dataloader = DataLoader(val_en_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca63c08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04f5a954d94f3a4eba835849196164f7",
     "grade": false,
     "grade_id": "cell-8768db288237ad8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.2: Implement mBERT Based Classifier for NLI (15 minutes)\n",
    "\n",
    "Similar to last assignment implement a classifier with an mBERT module followed by a classification layer. Note that we have a 3-class classification problem this time and unlike last time we have fixed labels for all examples (Phew!). So we just need a linear layer (followed by log-softmax) on top the CLS embeddings to get log probabilities for each of the class. The architecture will look something like this\n",
    "\n",
    "![architecture](images/mbert_xnli.png)\n",
    "\n",
    "Implement the `mBERTNLIClassifierModel` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c8194",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f46be789886b5e8ac26400d06c192574",
     "grade": false,
     "grade_id": "cell-8891889003664406",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class mBERTNLIClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_hidden = 768, mbert_variant = \"bert-base-multilingual-uncased\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Constructor for the `mBERTNLIClassifierModel` class. Use this to define  the network architecture\n",
    "        which should be: Input -> mBERT -> Linear Layer -> Log-Softmax\n",
    "        \n",
    "        Inputs:\n",
    "            - d_hidden (int): Size of the hidden representations of mbert\n",
    "            - mbert_variant (str): mBERT variant to use\n",
    "        \n",
    "        \"\"\"\n",
    "        super(mBERTNLIClassifierModel, self).__init__()\n",
    "        \n",
    "        self.mbert_layer = None\n",
    "        self.output_layer = None\n",
    "        self.log_softmax_layer = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward Passes the inputs through the network and obtains the prediction\n",
    "        \n",
    "        Inputs:\n",
    "            - input_ids (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
    "                                        representing the sequence of token ids\n",
    "            - attn_mask (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
    "                                        representing the attention mask such that padded tokens are 0 and rest 1\n",
    "                                        \n",
    "        Returns:\n",
    "          - output (torch.tensor): A torch tensor of shape [batch_size, 3] containing (log) probabilities\n",
    "          of each class \n",
    "                                                \n",
    "        \"\"\"\n",
    "        \n",
    "        output = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229181",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5937bc959d8bf73cfc74338f0e4b6588",
     "grade": true,
     "grade_id": "cell-ac73904ec54c5600",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Running Sample Test Cases!\")\n",
    "torch.manual_seed(42)\n",
    "model = mBERTNLIClassifierModel()\n",
    "\n",
    "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
    "                    \"An older and younger man smiling.\",\n",
    "                   \"A soccer game with multiple males playing.\"\n",
    "                    ]\n",
    "sample_hypotheses = [\"The man is sleeping.\",\n",
    "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
    "                    \"Some men are playing a sport.\"]\n",
    "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "sample_max_len = 32\n",
    "sample_dataset = XNLImBertDataset(\n",
    "    sample_premises,\n",
    "    sample_hypotheses,\n",
    "    sample_labels,\n",
    "    sample_max_len\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Sample Test Case 1\")\n",
    "sample_idx = 0\n",
    "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
    "expected_mbert_cls_out = np.array([[-0.9885041, -1.479876,  -0.915788 ]])\n",
    "print(f\"Model Output: {mbert_cls_out }\")\n",
    "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
    "\n",
    "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
    "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
    "print(\"Test Case Passed! :)\")\n",
    "print(\"******************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 2\")\n",
    "sample_idx = 1\n",
    "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
    "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
    "expected_mbert_cls_out = np.array([[-0.97441876, -1.4775381,  -0.9304163 ]])\n",
    "print(f\"Model Output: {mbert_cls_out }\")\n",
    "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
    "\n",
    "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
    "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
    "print(\"Test Case Passed! :)\")\n",
    "print(\"******************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8ca8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b820de50b1b730cb9b6469ca14768ea2",
     "grade": false,
     "grade_id": "cell-ae43b3865e1ae3bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.3: Training and Evaluating the Model (30 minutes)\n",
    "\n",
    "Similar to previous assignments implement the `train` and `evaluate` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c97cf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c0f817adc46e30d3fcf9a4ad0d30146",
     "grade": true,
     "grade_id": "cell-9d6c36df00bc73ae",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates `model` on test dataset\n",
    "\n",
    "    Inputs:\n",
    "        - model (mBERTNLIClassifierModel): mBERT based classifier model to be evaluated\n",
    "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
    "\n",
    "    Returns:\n",
    "        - accuracy (float): Average accuracy over the test dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    accuracy = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "def train(model, train_dataloader, val_dataloader,\n",
    "          lr = 1e-5, num_epochs = 3,\n",
    "          device = \"cpu\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs the training loop. Define the loss function as NLLLoss\n",
    "    and optimizer as Adam and train for `num_epochs` epochs.\n",
    "\n",
    "    Inputs:\n",
    "        - model (mBERTNLIClassifierModel): mBERT based classifer model to be trained\n",
    "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
    "        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n",
    "        - lr (float): The learning rate for the optimizer\n",
    "        - num_epochs (int): Number of epochs to train the model for.\n",
    "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        - best_model (mBERTNLIClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n",
    "        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n",
    "    \"\"\"\n",
    "        \n",
    "    best_val_accuracy = float(\"-inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    best_model.zero_grad()\n",
    "    return best_model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77e27c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b035aa8061efaa1b8d769b18625057b4",
     "grade": true,
     "grade_id": "cell-4907afdd10b1d9b0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "print(\"Training on 100 data points for sanity check\")\n",
    "\n",
    "max_seq_len = 128\n",
    "batch_size = 8\n",
    "\n",
    "sample_premises, sample_hypotheses = train_en_data[\"premise\"].values[:100], train_en_data[\"hypothesis\"].values[:100]\n",
    "sample_labels = train_en_data[\"label\"].values[:100]\n",
    "\n",
    "sample_dataset = XNLImBertDataset(sample_premises, sample_hypotheses, sample_labels, max_seq_len)\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size = batch_size)\n",
    "\n",
    "\n",
    "model = mBERTNLIClassifierModel()\n",
    "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, lr = 5e-5, num_epochs = 10, device = \"cuda\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
    "print(f\"Expected Best Validation Accuracy: {0.99}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628b8d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8fe45a9683a786986fb3ec67a03e93b",
     "grade": false,
     "grade_id": "cell-0f4e40f77801822b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since we just trained and evaluated on same 100 examples, you should expect nearly perfect 99% accuracy. Now let's train on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d168c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d2899845f43271da6bc2a81268395ef",
     "grade": false,
     "grade_id": "cell-e88c407755223b49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = mBERTNLIClassifierModel()\n",
    "best_model, best_val_acc = train(model, train_en_dataloader, val_en_dataloader, lr = 1e-5, num_epochs = 2, device = \"cuda\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
    "print(f\"Expected Best Validation Accuracy: {0.7675}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700e982",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "965708cde86ca694508084ec9a95047f",
     "grade": false,
     "grade_id": "cell-0fc8ce883f13f2ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.4: Zero-Shot Transfer (30 minutes)\n",
    "\n",
    "Pre-trained multilingual models like mBERT have shown to exhibit zero-shot transfer capabilities to new languages for which the model was never fine-tuned on. You can read more about zero-shot transfer in mBERT in this [paper](https://arxiv.org/abs/1906.01502). We now test this phenomenon for ourselves, where we will evaluate the performance of the mBERT classifier that we just trained on the English on the test sets in 15 different languages. Implement the `evaluate_on_diff_langs` function below that does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcad1a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c181d9d192a64c0b7bd074271231cf2",
     "grade": true,
     "grade_id": "cell-8ba4a916f2c1bd6b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_on_diff_langs(model, lang2test_df, max_length = 128, batch_size = 8, device = \"cpu\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of the fine-tuned model on test data in different langauges.\n",
    "    \n",
    "    Inputs:\n",
    "        - model (mBERTNLIClassifierModel): mBERT based classifer model fine-tuned on English data\n",
    "        - lang2test_df (dict): A dictionary with langauges as keys and\n",
    "                                their corresponding test sets (in form of pandas dataframe)\n",
    "                                as values\n",
    "                                \n",
    "    Returns:\n",
    "        - lang2acc (dict): A dictionary with language ids as keys and the accuracy on it's test set as values\n",
    "                            eg: {\"en\" : 0.8, \"fr\" : 0.77, \"hi\": 0.72, ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lang2acc = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return lang2acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff6b5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b1db38570a82961a708b794c787c88",
     "grade": true,
     "grade_id": "cell-d87dce517151ef2b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lang2acc = evaluate_on_diff_langs(best_model, lang2test_df, max_length = 128, batch_size = 8, device = \"cuda\")\n",
    "expected_vals = {'ar': 0.5989583333333334,\n",
    " 'bg': 0.6454326923076923,\n",
    " 'de': 0.6698717948717948,\n",
    " 'el': 0.6402243589743589,\n",
    " 'en': 0.7263621794871795,\n",
    " 'es': 0.6923076923076923,\n",
    " 'fr': 0.6802884615384616,\n",
    " 'hi': 0.5893429487179487,\n",
    " 'ru': 0.6478365384615384,\n",
    " 'sw': 0.53125,\n",
    " 'th': 0.35136217948717946,\n",
    " 'tr': 0.610176282051282,\n",
    " 'ur': 0.5637019230769231,\n",
    " 'vi': 0.6193910256410257,\n",
    " 'zh': 0.6073717948717948}\n",
    "print(f\"Langauge to Accuracy:\\n {lang2acc}\")\n",
    "print(f\"Expected Values:\\n {expected_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65558d2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2aedb40539425342565ac0819fce25f",
     "grade": false,
     "grade_id": "cell-9eed3e772b4dbab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Don't worry if the values do not match exactly, but you can expect similar patterns i.e. the fine-tuned model on English data, performs reasonably on other new langauges as well compared to it's performance on English test data. Performance on langauges like German, French and Spanish is much closer to the performance on English. However, it is on the lower side for languages like Swahilli, Urdu and Thai. The values are still surprisingly high, considering a random guess will fetch you an accuracy of 33%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c3169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
