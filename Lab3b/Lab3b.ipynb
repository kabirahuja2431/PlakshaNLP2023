{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d42ca1b98bca990dc1125593d655f0f0",
     "grade": false,
     "grade_id": "cell-6f66ba12e085ceae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 3: Prompting LLMs to solve NLP Problems\n",
    "\n",
    "## June 27, 2023\n",
    "\n",
    "Welcome to Lab 3 of our course on Natural Language Processing. Today, we will be diving deep into the fourth and most recent paradigm in NLP teased in the previous Lab, i.e. Pre-train, Prompt and Predict. The core idea behind the paradigm is that once we train a big enough language model (pre-training + instruction tuning), we do not really need to train these models further to solve any specific taks, but instead can directly prompt the model to solve a task by specifying instructions, task descriptions and in some cases a few examples.\n",
    "\n",
    "Like last time we will be working on the with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset, and demonstrating how to work with LLMs to solve such tasks.\n",
    "\n",
    "Before geting started, we recommend signing up for a free-trial of the [OpenAI API](https://openai.com/blog/openai-api), which should give you free credits worth 5$ for three months. This should be plenty for the tutorial today and for your final projects. After signing up for the API, get the api key and place it in the `key.txt` file located in the same directory. Once that's setup you can proceed with the tutorial.\n",
    "\n",
    "\n",
    "Recommended Reading:\n",
    "- Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. <i>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</i>. https://arxiv.org/abs/2107.13586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    siqa_data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Lab3b/data/socialiqa-train-dev/\"\n",
    "except:\n",
    "    siqa_data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/TLPNLP2023/source/Lab3b/data/socialiqa-train-dev/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca5a87ec2ad4d0edae633ffbc2572d5b",
     "grade": false,
     "grade_id": "cell-11afe8cdb419a221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade openai\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ca380382d807e1e51578aa45a7e5bf4",
     "grade": false,
     "grade_id": "cell-0f04be700b9e8b6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import random\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c356df741401527aca179894b96f8191",
     "grade": false,
     "grade_id": "cell-f1d6fbb21c28467c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify the key\n",
    "with open(\"key.txt\") as f:\n",
    "    openai.api_key = f.read().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9da485e83450c84b8d2a8b9deba8b9ef",
     "grade": false,
     "grade_id": "cell-608536dd6a8a576c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading the SocialIQA dataset\n",
    "\n",
    "def load_siqa_data(split):\n",
    "\n",
    "    # We first load the file containing context, question and answers\n",
    "    with open(f\"data/socialiqa-train-dev/{split}.jsonl\") as f:\n",
    "        data = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "    # We then load the file containing the correct answer for each question\n",
    "    with open(f\"data/socialiqa-train-dev/{split}-labels.lst\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "train_data, train_labels = load_siqa_data(\"train\")\n",
    "dev_data, dev_labels = load_siqa_data(\"dev\")\n",
    "\n",
    "print(f\"Number of Training Examples: {len(train_data)}\")\n",
    "print(f\"Number of Validation Examples: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b50619d83545217e2248dc96472ab00",
     "grade": false,
     "grade_id": "cell-478779c184868fba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d74784ef130af66fb3c3246865e32519",
     "grade": false,
     "grade_id": "cell-3d1472da5328a12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62c58c860c6edd54ab635ba5fe4568c8",
     "grade": false,
     "grade_id": "cell-718885ec92ef0830",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Prompting Basics (30 minutes)\n",
    "\n",
    "In this task, you will be learning how create standard NLP problems into text prompts which can then be fed to an LLM for its prediction. Mainly there are 2 concepts that are important to understand while creating prompts:\n",
    "- Prompt Template or Function: a textual string that has two slots: an input slot [X] for input x and an answer slot\n",
    "[Z] for an intermediate generated answer text z that will later be mapped into y.\n",
    "- Answer verbalizer: A mapping between the task labels to words or phrases that converts the more artificial looking labels to natural language that fits with the prompt. eg. for sentiment analysis we can define Z = {“excellent”, “good”, “OK”, “bad”, “horrible”} to represent each of the classes in Y = {++, +, ~, -, --}.\n",
    "\n",
    "<img src=\"images/prompting_basics.png\" alt=\"prompting\" border=\"0\">\n",
    "\n",
    "We can also include more interesting stuff like instruction of the task in the template and explanation of the answer in the verbalizer to make more powerful prompts, as we will see a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f50ba12da5e674d0cdbcc3a190435ba",
     "grade": false,
     "grade_id": "cell-00b6b72fcbd2e765",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.1 Defining prompt function and verbalizer for SocialIQA.\n",
    "\n",
    "For the purpose of this excercise, we ask you to implement this prompt function:\n",
    "```\n",
    "Context: {{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Which one of these answers best answers the question according to the context?\n",
    "\n",
    "A: {{answerA}}\n",
    "B: {{answerB}}\n",
    "\n",
    "C: {{answerC}}\n",
    "```\n",
    "\n",
    "and verbalizer:\n",
    "\n",
    "```\n",
    "{\"1\": \"The answer is A\", \"2\": \"The answer is B\", \"3\": \"The answer is C\"}\n",
    "```\n",
    "\n",
    "This prompt was obtained from [PromptSource](https://huggingface.co/spaces/bigscience/promptsource), an awesome resource for finding prompts for hundreds of NLP tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1885000d0376ac315a35d1d38b66ec12",
     "grade": false,
     "grade_id": "cell-4fb2017a6eb5d6d0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def social_iqa_prompting_fn(siqa_example: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Takes an example from the SocialIQA dataset, fills in the prompt template, and returns the prompt.\n",
    "    \n",
    "    Inputs:\n",
    "        siqa_example: A dictionary containing the context, question and answerA, answerB, answerC for a SocialIQA example.\n",
    "\n",
    "    Outputs:\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f64152de9fb4a724bb9296c461f66ce9",
     "grade": true,
     "grade_id": "cell-d66e3a61d86329f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "siqa_example = train_data[0]\n",
    "prompt = social_iqa_prompting_fn(siqa_example)\n",
    "expected_prompt = \"\"\"Context: Cameron decided to have a barbecue and gathered her friends together.\n",
    "Question: How would Others feel as a result?\n",
    "Which one of these answers best answers the question according to the context?\n",
    "AnswerA: like attending\n",
    "AnswerB: like staying home\n",
    "AnswerC: a good friend to have\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "assert prompt == expected_prompt\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2\")\n",
    "siqa_example = train_data[100]\n",
    "prompt = social_iqa_prompting_fn(siqa_example)\n",
    "expected_prompt = \"\"\"Context: Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\n",
    "Question: How would Jordan feel afterwards?\n",
    "Which one of these answers best answers the question according to the context?\n",
    "AnswerA: selling a couch\n",
    "AnswerB: Disgusted\n",
    "AnswerC: Relieved\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "assert prompt == expected_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "079415882fcaea37db4873cdb5afbd6d",
     "grade": false,
     "grade_id": "cell-5b6cc267bda95fc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def social_iqa_verbalizer(label: str):\n",
    "    \"\"\"\n",
    "    Takes in the label and coverts it into a natural language phrase as specified above\n",
    "\n",
    "    Inputs:\n",
    "        label: A string containing the correct answer for a SocialIQA example.\n",
    "    \n",
    "    Outputs:\n",
    "        A string containing the natural language phrase corresponding to the label.\n",
    "    \"\"\"\n",
    "\n",
    "    verbalized_label = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return verbalized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ccf5b774f538dde594118c8d11e5b05",
     "grade": true,
     "grade_id": "cell-5edf39482dddcf55",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "siqa_example = train_labels[0]\n",
    "output = social_iqa_verbalizer(siqa_example)\n",
    "expected_output = \"\"\"The answer is A\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2\")\n",
    "siqa_example = train_labels[100]\n",
    "output = social_iqa_verbalizer(siqa_example)\n",
    "expected_output = \"\"\"The answer is B\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea5c2652e71771f86b181952b3d7990e",
     "grade": false,
     "grade_id": "cell-e5aa4e21d153ecaa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Let's now obtain the prompts and verbalized labels for each of the the examples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db90927ed2182078976a6c6f4ead02ca",
     "grade": false,
     "grade_id": "cell-c0605c90405b761a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_prompts = None\n",
    "train_verbalized_labels = None\n",
    "val_prompts = None\n",
    "val_verbalized_labels = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "457d570e43ae3bbe549374744fc9d8ed",
     "grade": true,
     "grade_id": "cell-7b1adfcde96fccb6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "idx = 10\n",
    "siqa_example = train_data[idx]\n",
    "prompt = train_prompts[idx]\n",
    "expected_prompt = \"\"\"Context: Sydney was a school teacher and made sure their students learned well.\n",
    "Question: How would you describe Sydney?\n",
    "Which one of these answers best answers the question according to the context?\n",
    "AnswerA: As someone that asked for a job\n",
    "AnswerB: As someone that takes teaching seriously\n",
    "AnswerC: Like a leader\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2\")\n",
    "idx = 10\n",
    "siqa_label = train_labels[idx]\n",
    "verbalized_label = \"The answer is B\"\n",
    "print(f\"Input Example:\\n{siqa_label}\")\n",
    "print(f\"Verbalized Label:\\n{verbalized_label}\")\n",
    "print(f\"Expected Verbalized Label:\\n{verbalized_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "873644fd818901e929d14a6dcc1f2178",
     "grade": false,
     "grade_id": "cell-576067356a80e135",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is often useful to have a reverse verbalizer as well that converts the verbalized labels back to the structured and consistent labels in the dataset. For example, \"The answer is A\" is mapped back to \"1\" and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "793d900531a62d3a3fed373a0ea82ff3",
     "grade": false,
     "grade_id": "cell-a87dbedb3c503dfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def social_iqa_reverse_verbalizer(verbalized_label: str):\n",
    "    \"\"\"\n",
    "    Reverses the verbalized label into the label\n",
    "    Inputs:\n",
    "        verbalized_label: A string containing the natural language phrase corresponding to the label.\n",
    "    Outputs:\n",
    "        label: A string containing the correct answer for a SocialIQA example.\n",
    "    \n",
    "    Important Note: We will be using this function to map LLM's output to structured label. The output of LLM now can be in some format other than what we expect\n",
    "    For example, it can be \"The answer is A\" or \"The answer is A.\" or or \"<some text> The answer is A\" or \"The answer is A <some text>\"\n",
    "    When you reverse the verbalized label, make sure you handle these cases.\n",
    "    \n",
    "    Important Note 2: If the resulting text doesn't have the answer, then just return an empty string.\n",
    "    \"\"\"\n",
    "\n",
    "    label = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65dfc93a122e5bf0894564290039a087",
     "grade": true,
     "grade_id": "cell-7c9c107abd6be27b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "example_verbalized_label = \"The answer is C\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"3\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2\")\n",
    "example_verbalized_label = \"The answer is B.\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"2\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 3\n",
    "print(\"Running Sample Test Case 3\")\n",
    "example_verbalized_label = \"some explanation before the actual answer, The answer is A\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"1\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 4\n",
    "print(\"Running Sample Test Case 4\")\n",
    "example_verbalized_label = \"some text here the answer is C, some more text\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"3\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 5\n",
    "print(\"Running Sample Test Case 5\")\n",
    "example_verbalized_label = \"none of the options is the correct answer\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e97fd1ec352e54d852beb4306578ca",
     "grade": false,
     "grade_id": "cell-1d4dd26593dba3a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.2: Choose Few-Shot examples\n",
    "\n",
    "Often we can get better performance on a task by providing a few examples of the task as part of the prompt. This is also known as in-context learning, where the model learns to solve a task based on the examples provided in the context (and no updates to the model's weights!). One of the easiest way that works reasonably well in practice is to simply choose `k` examples randomly for each class from the entire training dataset, such that we have n_classes * k few-shot examples where n_classes = 3 for SocialIQA dataset. Implement the `choose_few_shot` function below that does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "449d2a127f553e551a3d33953001fdc6",
     "grade": false,
     "grade_id": "cell-b4e32a28adc71458",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42):\n",
    "    \"\"\"\n",
    "    Randomly chooses k examples from the training set for few-shot in-context learning.\n",
    "    Inputs:\n",
    "        train_prompts: A list of prompts for the training set.\n",
    "        train_verbalized_labels: A list of labels for the training set.\n",
    "        k: The number of examples per class to choose.\n",
    "        n_classes: The number of classes in the dataset.\n",
    "        seed: The random seed to use, to ensure reproducible outputs\n",
    "\n",
    "    Outputs:\n",
    "        - List[Dict[str, str]]: A list of 3k examples from the training set, where each example is represented as a dictionary with \"prompt\" and \"label\" as keys and corresponding values.\n",
    "\n",
    "    Example Output: [\n",
    "        {\n",
    "            \"prompt\": <Example Prompt 1>,\n",
    "            \"label\": <Example Label_1>\n",
    "        },\n",
    "        ...,\n",
    "        {\n",
    "            \"prompt\": <Example Prompt 3k>,\n",
    "            \"label\": <Example Label_3k>\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    fs_examples = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Shuffle the examples to ensure there is no bias in the order of the examples\n",
    "    random.shuffle(fs_examples)\n",
    "\n",
    "    return fs_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d6c8ad26d78b1fd1bc0a5ae885869d0",
     "grade": true,
     "grade_id": "cell-ebd8deef0c41b476",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1. Checking if the output length is correct\")\n",
    "k = 1\n",
    "seed = 42\n",
    "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
    "output_len = len(output)\n",
    "expected_output_len = k * len(set(train_labels))\n",
    "print(f\"k: {k}\")\n",
    "print(f\"Output Length:\\n{output_len}\")\n",
    "print(f\"Expected Output Length:\\n{expected_output_len}\")\n",
    "assert output_len == expected_output_len\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2. Checking if all labels are predicted\")\n",
    "output_labels = sorted(list(set([example[\"label\"] for example in output])))\n",
    "expected_output_labels = [\"The answer is A\", \"The answer is B\", \"The answer is C\"]\n",
    "print(f\"Output Labels:\\n{output_labels}\")\n",
    "print(f\"Expected Output Labels:\\n{expected_output_labels}\")\n",
    "assert output_labels == expected_output_labels\n",
    "\n",
    "# Sample Test Case 3\n",
    "print(\"Running Sample Test Case 3. Checking if count of labels are correct\")\n",
    "k = 3\n",
    "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
    "output_label_counter = Counter(list(([example[\"label\"] for example in output])))\n",
    "expected_output_counter = {\"The answer is A\": k, \"The answer is B\": k, \"The answer is C\": k}\n",
    "print(f\"For k = {k}\")\n",
    "print(f\"Output Label Counter:\\n{output_label_counter}\")\n",
    "print(f\"Expected Output Label Counter:\\n{expected_output_counter}\")\n",
    "assert output_label_counter == expected_output_counter\n",
    "# # Sample Test Case 4\n",
    "# print(\"Running Sample Test Case 3\")\n",
    "# expected_output = [{'prompt': \"Context: Jordan explain another reason why they were late, but their boss wasn't buying it.\\nQuestion: How would Jordan feel afterwards?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: pleased at always being late\\nAnswerB: guilty at always being late\\nAnswerC: sneaky\",\n",
    "#   'label': 'The answer is B'},\n",
    "#  {'prompt': 'Context: Riley played basketball with friends and injured their bicep very badly.\\nQuestion: What will happen to Others?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: try to help\\nAnswerB: quit school\\nAnswerC: go to their room',\n",
    "#   'label': 'The answer is A'},\n",
    "#  {'prompt': 'Context: Alex told Cameron they did not share feelings, but Cameron kissed Alex on the lips anyway.\\nQuestion: How would Alex feel as a result?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: pleased\\nAnswerB: happy\\nAnswerC: upset',\n",
    "#   'label': 'The answer is C'}]\n",
    "\n",
    "# print(f\"Output:\\n{output}\")\n",
    "# print(f\"Expected Output:\\n{expected_output}\")\n",
    "# assert output == expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8e3dd214a5dd0c9112f2ed316ef5143",
     "grade": false,
     "grade_id": "cell-fd7e5e736aa540f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Choose 3 few-shot examples from training data\n",
    "few_shot_examples = choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf466ae38238a86b078ddfea533f90b7",
     "grade": false,
     "grade_id": "cell-501bb0838cde33a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Few-shot examples with explanations\n",
    "\n",
    "So far above we have been constructing label verbalizer to provide the answer directly. Often it can be useful to prompt the model to first generate an explanation before the answer. For eg.\n",
    "```\n",
    "\"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\n",
    "            Question: What does Tracy need to do before this?\n",
    "            Options: \n",
    "            (A) make a new plan \n",
    "            (B) Go home and see Riley \n",
    "            (C) Find somewhere to go\"\n",
    "\"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the answer is C\"\n",
    "```\n",
    "One way to prompt the model to generate such explanations is to provide the explanations for the few-shot examples, which will ground the model to first generate an explanation and then the answer. This helps both improve the performance of the model as well as have more interpretable outputs from LLM.\n",
    "\n",
    "Below we provide a few examples with explanations for SocialIQA task obtained from [Super-NaturalInstructions](https://aclanthology.org/2022.emnlp-main.340/), an amazing resource for prompts, instructions and explanations for around 1600 NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ff8726c5fa46578a5c6ba93a16dcfa1",
     "grade": false,
     "grade_id": "cell-143d385a20fb4e9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fs_examples_w_explanations = [\n",
    "    {\n",
    "        \"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\\nQuestion: What does Tracy need to do before this?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: make a new plan\\nAnswerB: Go home and see Riley\\AnswerC: Find somewhere to go\",\n",
    "        \"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.\\nQuestion: How would you describe Sydney?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: sympathetic\\nAnswerB: like a person who was unable to help\\nAnswerC: incredulous\",\n",
    "        \"label\": \"Sydney is a sympathetic person because she felt bad for someone who needed help, and she couldn't help her. Hence, the correct answer is A.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Context: Taylor gave help to a friend who was having trouble keeping up with their bills.\\nQuestion: What will their friend want to do next?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: help the friend find a higher paying job\\nAnswerB: thank Taylor for the generosity\\nAnswerC: pay some of their late employees\",\n",
    "        \"label\": \"The friend should thank Taylor for the generosity she showed by helping him pay bills. Hence, the correct answer is B.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73fbf6824c493f84e729a8ad6320484e",
     "grade": false,
     "grade_id": "cell-c6f0dc0f3a504eda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fs_examples_w_explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3b09b4b18c6b57f147fdf74545076a1",
     "grade": false,
     "grade_id": "cell-ccc12ad2ac3845b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Evaluating ChatGPT (GPT-3.5-Turbo) on SocialIQA (45 minutes)\n",
    "\n",
    "Today we will be working with OpenAI's GPT family of models. ChatGPT (or GPT-3.5) was built on top of GPT-3, which is a pre-trained Large Language Model (LLM) with 175 Billion parameters, trained on a huge amount of unlabelled data using the language modelling objective (i.e. given k tokens, generate (k+1)th token). While this forms the basis of all GPT family of models, GPT-3.5 and later models are based on [InstructGPT](https://arxiv.org/abs/2203.02155), which further adds an Instruction Tuning step that learns from human feedback to follow provided instructions.\n",
    "\n",
    "![Instruction Tuning](images/instructgpt.png)\n",
    "*From the [Ouyang et al. 2022](https://arxiv.org/abs/2203.02155)*\n",
    "\n",
    "As a consequence of the pre-training with language modeling objective and instruction tuning, we can use GPT-3.5 to complete a given piece of text and provide specific instructions about how to go about completing the text. We achieve this by defining a text prompt which is to be given as the input to the LLM which then generates a completion of the provided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d98c8d281643bacb1d8ee5437098979",
     "grade": false,
     "grade_id": "cell-33802f1e12f144fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we demonstrate how can we hit the OpenAI API to get responses for our prompts. Starting from GPT-3.5 models, the API comes with [ChatCompletions](https://platform.openai.com/docs/guides/gpt/chat-completions-api) support, which take a list of messages (conversation between user and assistant) as input and return a model-generated message as output. An example API call looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e5f18b893843394507c1aee3e3ec60d",
     "grade": false,
     "grade_id": "cell-5aa39c8d3414f5ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ],\n",
    "  max_tokens=20,\n",
    "  temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5860efdcb65b70e65c7aaaabb3503b8",
     "grade": false,
     "grade_id": "cell-6365ffeea93126f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's try to wrap our head around different parameters to this function call.\n",
    "\n",
    "First we have `model`, where we specify which OpenAI model to use. We have used `\"gpt-3.5-turbo\"` here, which is similar to ChatGPT like you would have used online. You can find the list of other models [here](https://platform.openai.com/docs/models).\n",
    "\n",
    "Next, we have `messages`, which contains the conversation between the user and assistant that is to be completed. Notice that the first message is what we call a \"system prompt\", which is used to set the behavior of the assistant. \n",
    "\n",
    "`max_tokens` is used to specify the maximum number of response tokens that the model should generate. This can be useful when you know how long the response is typically going to be, and can help reduce cost.\n",
    "\n",
    "`temperature`, helps in controlling the variability in the output. Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc38e62ea4908ecd6e4cc5c940a703df",
     "grade": false,
     "grade_id": "cell-2e8d7e2747bada3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ea951a2621bb888555c57624c2e1034",
     "grade": false,
     "grade_id": "cell-8f04cc28c3672862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's look at the response. The assistant's reply can be  extracted with `response['choices'][0]['message']['content']`. Every response will include a finish_reason. The possible values for finish_reason are:\n",
    "\n",
    "- stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
    "- length: Incomplete model output due to max_tokens parameter or token limit\n",
    "- function_call: The model decided to call a function\n",
    "- content_filter: Omitted content due to a flag from our content filters\n",
    "- null: API response still in progress or incomplete\n",
    "\n",
    "Depending on input parameters (like providing functions as shown below), the model response may include different information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4106d049f628c8d1b7f6c399e4122fb",
     "grade": false,
     "grade_id": "cell-75330e7dabc91083",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_output = response['choices'][0]['message']['content']\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b405ea22f97a6a1bd9d7fe83a4b8723b",
     "grade": false,
     "grade_id": "cell-6eb42ca1ddf7ed5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2.1: Using ChatGPT to solve SocialIQA problems\n",
    "\n",
    "Now we have an understanding of how to work with OpenAI API, we can go ahead and call the api with the prompts that we just created and check how well does the model perform the task. We promt the model with the test example for which we want the prediction and provide few-shot examples as part of the context. This can be done by simply providing the example prompt and labels as user-assistant conversation history and test example as the most recent query of the user. Implement the function `get_social_iqa_pred_gpt` that receives a test prompt to be answered, few-shot examples, and some api specific hyperparameters to predict the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "800f839aa5acf81c9dba9f082a79da75",
     "grade": true,
     "grade_id": "cell-043cb95bbdfc1a4c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_social_iqa_pred_gpt(\n",
    "        test_prompt,\n",
    "        few_shot_examples,\n",
    "        model_name = \"gpt-3.5-turbo\",\n",
    "        max_tokens = 20,\n",
    "        temperature = 0.0,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calls the OpenAI API with test_prompt and few-shot examples to generate the answer.\n",
    "    Inputs:\n",
    "        test_prompt: The prompt for the test example\n",
    "        few_shot_examples: A list of few-shot examples\n",
    "        model_name: The name of the model to use\n",
    "        max_tokens: The maximum number of tokens to generate\n",
    "        temperature: The temperature to use for the model\n",
    "\n",
    "    Outputs:\n",
    "        model_output: The model's output\n",
    "\n",
    "    Hint: Your messages to be sent should be in the following format:\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": <fs-example-1-promot>},\n",
    "            {\"role\": \"assistant\", \"content\": <fs-example-1-label>},\n",
    "            ...,\n",
    "            {\"role\": \"user\", \"content\": <fs-example-3k-promot>},\n",
    "            {\"role\": \"assistant\", \"content\": <fs-example-3k-label>},\n",
    "            {\"role\": \"user\", \"content\": <test-prompt>},\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    messages_prompt = [{\n",
    "        \"role\": \"user\", \"content\": \"You are an expert of Human Social Common Sense. You need to solve the SocialIQA task. In this task, you're given a context, a question, and three options. Your task is to find the correct answer to the question using the given context and options. Also, you may need to use commonsense reasoning about social situations to answer the questions. Classify your answers into 'A', 'B', and 'C'. You must choose the most likely option.\"\n",
    "    }]\n",
    "    model_output = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            time.sleep(20) # to prevent rate limit error\n",
    "            break\n",
    "        except (openai.error.APIConnectionError, openai.error.RateLimitError, openai.error.Timeout, openai.error.ServiceUnavailableError) as e:\n",
    "            #Sleep and try again\n",
    "            print(f\"Couldn't get response due to {e}. Trying again!\")\n",
    "            time.sleep(20)\n",
    "            continue\n",
    "\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c7a7229bdea5d321955a09c95b282b",
     "grade": false,
     "grade_id": "cell-bfcb1f479a5ba433",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_example = val_prompts[0]\n",
    "test_example_label = val_verbalized_labels[0]\n",
    "model_output = get_social_iqa_pred_gpt(test_example, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)\n",
    "print(test_example)\n",
    "print(f\"Model's response: \", model_output)\n",
    "print(f\"Correct answer: \", test_example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9a637366fc518edc2dc2a8357a33aab",
     "grade": false,
     "grade_id": "cell-a872a9180743cb5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see the model didn't quite get the answer right. Let's try providing examples with explanations i.e. `fs_examples_w_explanations` and see the output. Note that we will need to give a higher value of `max_tokens`, since the model is also expected to generate explanation now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9b31fb5b7e7c5e5c29a2152852614af",
     "grade": false,
     "grade_id": "cell-da9adc4cc4b56841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_example = val_prompts[0]\n",
    "test_example_label = val_verbalized_labels[0]\n",
    "model_output = get_social_iqa_pred_gpt(test_example, fs_examples_w_explanations,\n",
    "                                        model_name = \"gpt-3.5-turbo\", max_tokens = 50, temperature = 0.0)\n",
    "print(test_example)\n",
    "print(f\"Model's response: \", model_output)\n",
    "print(f\"Correct answer: \", test_example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f817f822aa31563f8b6403bd8db13422",
     "grade": false,
     "grade_id": "cell-6b90b2ee95dc573b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see the output is correct and the explanation also makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db4e86c8792e0cf67e96dacda7dc95fb",
     "grade": false,
     "grade_id": "cell-41ee96a7633832f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's do a full fledged evaluation now. Due to cost limits, we will only be evaluating first 32 examples of the validation set and not the whole but that should give us some idea of how good ChatGPT is at solving social common-sense reasoning problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7723b837de07eabfc200cf4f51388725",
     "grade": true,
     "grade_id": "cell-3316a06793521ba9",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(\n",
    "        test_prompts,\n",
    "        few_shot_examples,\n",
    "        model_name = \"gpt-3.5-turbo\",\n",
    "        max_tokens = 20,\n",
    "        temperature = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get predictions for all test prompts using the `get_social_iqa_pred_gpt` function\n",
    "\n",
    "    Inputs:\n",
    "        test_prompts: A list of test prompts\n",
    "        few_shot_examples: A list of few-shot examples\n",
    "        model_name: The name of the model to use\n",
    "        max_tokens: The maximum number of tokens to generate\n",
    "        temperature: The temperature to use for the model\n",
    "    \n",
    "    Outputs:\n",
    "        model_preds: A list of model predictions for each test prompt\n",
    "    \"\"\"\n",
    "\n",
    "    model_preds = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return model_preds\n",
    "\n",
    "def evaluate_model_preds(\n",
    "        model_preds,\n",
    "        test_labels\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the prediction of the model by performing string match between the predictions and labels.\n",
    "\n",
    "    Inputs:\n",
    "        model_preds: A list of model predictions for each test prompt\n",
    "        test_labels: A list of test labels. Note that these are not verbalized\n",
    "\n",
    "    Outputs:\n",
    "        accuracy: The accuracy of the model i.e. #correct_predictions / #total_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b500a1e92858d3e436a5ed12086cbf8",
     "grade": false,
     "grade_id": "cell-5c5bdf033f45a9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To test if things are working fine\n",
    "k = 5\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_preds = get_model_predictions(test_prompts, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6b2249475f4a38d6005b563edce91d1",
     "grade": false,
     "grade_id": "cell-d91502f785d3ae50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 32 validation examples\n",
    "k = 32\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_preds = get_model_predictions(test_prompts, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 32 validation examples with explanations\n",
    "k = 32\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_preds = get_model_predictions(test_prompts, \n",
    "                                    fs_examples_w_explanations, \n",
    "                                    model_name = \"gpt-3.5-turbo\", max_tokens = 50, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get better performance on prompting the model with explanations than without 56% vs 62.5%. We can do more prompt-engineering and better type of explanations to improve the performance further. But we hope with this you would have gotten some idea on how to use these models to solve NLP tasks like this. Also, notice that common sense reasoning remains an open problem for the models we have today, as even with ChatGPT, which is a fairly strong LLM, the accuracy remains comparitively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
