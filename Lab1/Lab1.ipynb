{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Cbgl-nGTO-aF",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "deb2b0ec75586e4268545b8f5df07842",
     "grade": false,
     "grade_id": "cell-e32f6782587492d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 1 : Setting up NLP Pipeline and Text Classification\n",
    "\n",
    "## June 13, 2023\n",
    "\n",
    "Welcome to Lab 1 of our course on Natural Language Processing! In this lab you will learn to implement different text-preprocessing techniques commonly used for NLP tasks as well as implement a standard text-classification algorithm for categorizing different news articles.\n",
    "\n",
    "We assume that you are familiar with `python` programming language, and its libraries like `numpy` and `pandas`. We will also make use of other libraries like `nltk` and `pytorch` in the assignment. Familiarity with these libraries is not assumed so we will provide short tutorials on their usage within the assignment.\n",
    "\n",
    "Expected learning outcomes from this Lab include:\n",
    "- Understand and implement text pre-processing techniques for NLP tasks\n",
    "- Gain understanding of count / frequency based bag of word models for performing text classification\n",
    "- Learn how to use Pytorch to define and train models for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFWd9il4Lu7T",
    "outputId": "1cf7f6e7-0a0e-4d32-d590-9a0ea8a97a17"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Lab1/data/ag_news_csv\"\n",
    "except:\n",
    "    data_dir = \"/home/t-kabirahuja/work/repos/PlakshaNLP/TLPNLP2023/source/Lab1/data/ag_news_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "2fW_rzFZUver",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "633af3fd82c319143e02003877d9d6b1",
     "grade": false,
     "grade_id": "cell-86e7d0dd7f309c53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "79b454a5-a1b6-424e-a7a2-89e17ddcf35d"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "xYa1X_IdIeBC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fb9b3f17dfee7fcb4aa1528b5837ed4",
     "grade": false,
     "grade_id": "cell-5d69275c6c01ef58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "44bbbc5f-1281-4c49-ad46-b53616996c8b"
   },
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the lab.\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TN81bbKeS_7M",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad705ff697b9d6af531ba29a122ba4fe",
     "grade": false,
     "grade_id": "cell-b0972535dc2a8261",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## AG News Dataset\n",
    "\n",
    "For the purposes of this lab we will be working with the [AG News](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) topic classification dataset introduced in [Zhang et al. 2015](https://papers.nips.cc/paper_files/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html). The dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600. The four classes correspond to the following:\n",
    "\n",
    "- 1: World\n",
    "- 2: Sports\n",
    "- 3: Business\n",
    "- 4: Sci/Tech\n",
    "\n",
    "\n",
    "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
    "\n",
    "We start by loading the datasets into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "gHaeXFWkN-Ow",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd87a811712ba7cc184b602861ef0012",
     "grade": false,
     "grade_id": "cell-95f4151f3b684704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "337b16d9-173c-488c-eea7-e22ce8bf1d94"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = 4\n",
    "LABELS_MAP = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "def load_dataset(split):\n",
    "    ## Load the datasets and specify the column names\n",
    "    df = pd.read_csv(f\"{data_dir}/{split}.csv\", names=[\"label\", \"title\", \"description\"])\n",
    "\n",
    "    ## Merge the title and description columns\n",
    "    df[\"news\"] = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n",
    "    ## Remove the title and description columns\n",
    "    df = df.drop([\"title\", \"description\"], axis=1)\n",
    "\n",
    "    ## Have the labels start from 0\n",
    "    df[\"label\"] = df[\"label\"] - 1\n",
    "\n",
    "    ## Map the label to the corresponding class\n",
    "    df[\"label_readable\"] = df[\"label\"].apply(lambda x: LABELS_MAP[int(x)])\n",
    "\n",
    "    df = df[[\"news\", \"label\", \"label_readable\"]]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "## Load the datasets and specify the column names\n",
    "train_df = load_dataset(\"train\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Number of Training Examples: {len(train_df)}\")\n",
    "print(f\"Number of Test Examples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "editable": false,
    "id": "I95nT2iAOEDB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "636369e71f75ac9783fed9352efd9445",
     "grade": false,
     "grade_id": "cell-d2b8cefe8e3cdcdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "13079461-7ca3-4b12-b1fe-f32dbe790d2e"
   },
   "outputs": [],
   "source": [
    "# View a sample of the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4iM2Ji1xtt6e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff968f9498c6ca42bed1fb4f735ea69b",
     "grade": false,
     "grade_id": "cell-13023d00322e4467",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen from the sample of training dataset using `train_df.head()`, the dataframe contains columns `news`, `label` and `label_readable` containing the news and the category label respectively.\n",
    "\n",
    "As part of some preliminary data analysis below we visualize the distribution of the labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f58369d77a9b8f9ac382a84dd19048",
     "grade": false,
     "grade_id": "cell-64209df7515a8bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Visualize the label distribution\n",
    "sns.countplot(train_df, x = \"label_readable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JoyacaXdvSVC",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb61adcc643ed2d363bcc0acf3eef61d",
     "grade": false,
     "grade_id": "cell-00825dbebf40ad34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen from the plot we have fully balaced training dataset with 30k points in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SNSOIRQgvjPB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fec23eb33f257cd73b6b97ada6e11398",
     "grade": false,
     "grade_id": "cell-9311b0cbf6236a1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Preprocessing Pipeline for NLP (30 minutes)\n",
    "\n",
    "You will start by implementing different text-preprocessing functions below. We have provided the definitions for the functions you are supposed to implement. After filling the code for the function, you can run the cell that follows to run test cases on your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yO16JJysBBrL",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cf9da8b7af01c4d33d8d94d5389e9e4",
     "grade": false,
     "grade_id": "cell-51443a1b14453358",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.1: Word Tokenization\n",
    "\n",
    "Before we start preprocessing the text data and eventually training classification models, it is crucial to break the text into a set of constituents called tokens which can either be sentences, words, sub-words or characters. For the purposes of this assignment we will focus on Word Tokenization i.e. breaking a piece of text into a sequence of words.\n",
    "\n",
    "There are different ways splitting a piece of text into a list of words. The simplest solution can be to split whenever a white-space character (i.e. `\" \"`) is encountered in the text i.e. if you have a string `\"this is an example of tokenization\"`, you iterate through it and whenever a white space is encountered you split the word to get: `[\"this\", \"is\", \"an\", \"example\", \"of\", \"tokenization\"]`. \n",
    "\n",
    "Implement the `whitespace_word_tokenize` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "fLL-9xkpAvXv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9ae92d4d8f4d26e1c07178726984ba7",
     "grade": false,
     "grade_id": "cell-26107c9fb9cba954",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def whitespace_word_tokenize(text):\n",
    "  \"\"\"\n",
    "  Splits a python string containing some text to a sequence of words\n",
    "  by splitting on whitespace.\n",
    "  \n",
    "  Parameters:\n",
    "    - text (str): A Python string containing the text to be tokenized\n",
    "\n",
    "  Returns:\n",
    "    - words (list): A list contaning the words present in the text (in the same order)\n",
    "  \n",
    "  \"\"\"\n",
    "  words = None\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "v6AmWOL7ElWR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da1cdf029c7c321dc85250720782cecc",
     "grade": true,
     "grade_id": "cell-b79f7c7e45b2dbe2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "198cd3ca-922d-472b-8a77-eafe1aef010e"
   },
   "outputs": [],
   "source": [
    "def evaluate_list_test_cases(test_case_input,test_case_func_output,test_case_exp_output):\n",
    "\n",
    "    print(f\"Input: {test_case_input}\")\n",
    "    print(f\"Function Output: {test_case_func_output}\")\n",
    "    print(f\"Expected Output: {test_case_exp_output}\")\n",
    "\n",
    "    if test_case_func_output == test_case_exp_output:\n",
    "        print(\"Test Case Passed :)\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Test Case Failed :(\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"We all live in a Yellow Submarine\"\n",
    "test_case_answer = ['We', 'all', 'live', 'in', 'a', 'Yellow', 'Submarine']\n",
    "test_case_student_answer = whitespace_word_tokenize(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"We all live, in a Yellow Submarine.\"\n",
    "test_case_answer = ['We', 'all', 'live,', 'in', 'a', 'Yellow', 'Submarine.']\n",
    "test_case_student_answer = whitespace_word_tokenize(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d8uwLxsWGKns",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e137413a29776ecdbcf2cfae8ee396e6",
     "grade": false,
     "grade_id": "cell-246a5394ffb7af9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see from the outputs above the white space tokenizer does reasonably well in splitting a sentence into words. However, it is not perfect, as can be seen in the output of test case 2 this method fails to split words when punctuations are encountered which are retained as parts of the words like `\"live,\"` and `\"Submarine.\"`. \n",
    "\n",
    "One possible solution is to instead of splitting on the white-space also split when punctuations are encountered. This will partially solve the problem but there are certain other cases that still won't be handled properly by this, like we would want something like `\"don't\"` to be split into `[\"do\", \"n't\"]` instead of `[\"don\", \"'\", \"t\"]`. \n",
    "\n",
    "Thankfully, nltk package provides the `word_tokenize` function that handles most of such cases inbuilt. Implement the `nltk_word_tokenize` function below which uses `word_tokenize` function from the nltk library to tokenize the text. Refer to the documentation [here](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank) to understand the usage of `word_tokenize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "EghhvTl0HXl5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0093485dc3886db6a3ab24bfdee09da4",
     "grade": false,
     "grade_id": "cell-597ee01aabc9acf4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nltk_word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits a python string containing some text to a sequence of words\n",
    "    by using `word_tokenize` function from nltk.\n",
    "    Refer to https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): A Python string containing the text to be tokenized\n",
    "\n",
    "    Returns:\n",
    "    - words (list): A list contaning the words present in the text (in the same order)\n",
    "    \"\"\"\n",
    "\n",
    "    words = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "443uQMVLGJpt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a951f76d6d24328a3245678c814dce13",
     "grade": true,
     "grade_id": "cell-e761dcef61c6e6cb",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a0fa753e-edac-48ac-d78b-7bea654d2510"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"We all live in a Yellow Submarine\"\n",
    "test_case_answer = ['We', 'all', 'live', 'in', 'a', 'Yellow', 'Submarine']\n",
    "test_case_student_answer = nltk_word_tokenize(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"We all live, in a Yellow Submarine.\"\n",
    "test_case_answer = ['We', 'all', 'live', ',', 'in', 'a', 'Yellow', 'Submarine', '.']\n",
    "test_case_student_answer = nltk_word_tokenize(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 3:\")\n",
    "test_case = \"pi isn't a rational number and its approximate value is 3.14\"\n",
    "test_case_answer = ['pi', 'is', \"n't\", 'a', 'rational', 'number', 'and', 'its', 'approximate', 'value', 'is', '3.14']\n",
    "test_case_student_answer = nltk_word_tokenize(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "am5naMCCLHr5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "206d9b064e93231bc477058e88519381",
     "grade": false,
     "grade_id": "cell-e7cd8bd9cdc553ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see (if your test cases passed), nltk does a much better job at splitting the text into constituent words, by splitting the punctuations away from the words as well as also taking care of subtleties like splitting `\"isn't\"` into `\"is\"` and `\"n't\"` and retaining the full decimal `\"3.14\"` which would have been split into `\"3\"` and `\"14\"` if we would have naively split on punctuations along with the whitespace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baBb2faBwO1j",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3d657e096e3a1a0a5d390871dc16431",
     "grade": false,
     "grade_id": "cell-875623b28d021978",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.2: Convert the text to lower case\n",
    "\n",
    "We start the with the most basic of all text preprocessing techniques i.e. converting all the words in the text into lower case. As you will see soon, NLP models often treat different words as different entities and by default do not assume any relation between them. For eg. A word `Bat` will be treated differently from the word `bat` if we use them seperately to define the features. Hence it can be useful to remove such artifacts from the datasets so that we can have common representations for the same words.\n",
    "\n",
    "Complete the function definiton below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "H6vUkukMwLDI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7bab6ac937765e2e752cd5701f68057",
     "grade": false,
     "grade_id": "cell-3e47327b34745799",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    \"\"\" Converts a piece of text to only contain words in lower case\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): A Python string containing the text to be lower-cased\n",
    "\n",
    "    Returns:\n",
    "    - text_lower_case (str): A string containing the input text in lower case\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text_lower_case = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return text_lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "rumqsZp2vPwL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f91c3179392a9b0d054543c62bd4891a",
     "grade": true,
     "grade_id": "cell-c3f69a62a997c91e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c7e4717d-0161-459e-fb5c-20f3bdf0dff2"
   },
   "outputs": [],
   "source": [
    "\"\"\"Don't change code in this cell\"\"\"\n",
    "#SAMPLE TEST CASE\n",
    "\n",
    "def evaluate_string_test_cases(test_case_input,\n",
    "                    test_case_func_output,\n",
    "                    test_case_exp_output):\n",
    "\n",
    "    print(f\"Input: {test_case_input}\")\n",
    "    print(f\"Function Output: {test_case_func_output}\")\n",
    "    print(f\"Expected Output: {test_case_exp_output}\")\n",
    "\n",
    "    if test_case_func_output == test_case_exp_output:\n",
    "        print(\"Test Case Passed :)\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Test Case Failed :(\")\n",
    "        print(\"**********************************\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"We all live in a Yellow Submarine\"\n",
    "test_case_answer = \"we all live in a yellow submarine\"\n",
    "test_case_student_answer = to_lower_case(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"SuRRender To The Void, iT is SHINNING\"\n",
    "test_case_answer = \"surrender to the void, it is shinning\"\n",
    "test_case_student_answer = to_lower_case(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MfgtHs-sVjJs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c2dd6f1a8a2a91d8bfecc705701b25e",
     "grade": false,
     "grade_id": "cell-8f84dbf3659373ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.3: Remove Punctuations\n",
    "\n",
    "Another common way to reduce the number of word variations in the text like `hello` vs `hello,` is to remove punctuations. While for some NLP tasks like POS tagging punctuations might be helpful, for classification tasks punctuations can be assumed to have negligible effect on the actual label.\n",
    "\n",
    "Complete the function `remove_punctuations` below. Examples for the function's working are:\n",
    "\n",
    "| Input                                                                                                  | Expected Output                                                                                  |\n",
    "|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal! | Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal |\n",
    "| \"Little tyke,\" chortled Mr. Dursley as he left the house.                                              | Little tyke chortled Mr Dursley as he left the house                                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "PdBqyCoWan9S",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70da013f663b15f199354f28c16896ec",
     "grade": false,
     "grade_id": "cell-d189c80b31a5ca50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    \"\"\" \n",
    "    Removes punctuations from a piece of text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str) : A Python string containing text from which punctuation is to be removed\n",
    "\n",
    "    Returns:\n",
    "    - text_no_punct (str): Resulting string after removing punctuation.\n",
    "\n",
    "    Hint: You can use `string.punctuation` to get a string containing all punctuation symbols.\n",
    "    >>> print(string.punctuation)\n",
    "    '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text_no_punct = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "XnjHFPs3b8fE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d79f662f18c2736569f32aea54469179",
     "grade": true,
     "grade_id": "cell-bc45beb88678c7c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "be2fd560-4df1-47fc-cbea-de797554a3e4"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\"\n",
    "test_case_answer = \"Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal\"\n",
    "test_case_student_answer = remove_punctuations(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"\\\"Little tyke,\\\" chortled Mr. Dursley as he left the house.\"\n",
    "test_case_answer = \"Little tyke chortled Mr Dursley as he left the house\"\n",
    "test_case_student_answer = remove_punctuations(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2iSawxClehH3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ad368a5e02b946853a262d0217bd4f9",
     "grade": false,
     "grade_id": "cell-e2e22fe1d0b1b38d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.4: Remove Stop Words\n",
    "\n",
    "There are some commonly used words in a language like in case of english 'the', 'a', 'I', 'he' which might not provide much valuable information for the current task in hand, and hence can be removed from the text. Again for a task like POS Tagging (which we will see in the future assignments), this shouldn't be done but for text classification the labels can be assumed to be largely independent of the presence of such words.\n",
    "\n",
    "The choice of the stop words to use can be subjective and in many cases might depend upon the problem in hand. For the purposes of this assignment we will consider the stop words for English language present in the `nltk` package. The code for obtaining these stop words is given in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "deletable": false,
    "editable": false,
    "id": "EF2RUF-VhwgI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ad13de4f03998d74885f8364e81301",
     "grade": false,
     "grade_id": "cell-def1061b193796df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0f030332-ba48-4f92-fe82-087f4bfe2a6a"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\",\".join(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WBFQaRCJiOR8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "266728e353be799a87caf12ecf7bd52d",
     "grade": false,
     "grade_id": "cell-c87968a6b3949535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen from the output above the stop words contain commonly used words that may not provide much information for the downstream task. Implement the function `remove_stop_words` below using the list of stop words given by `STOPWORDS` in the above cell.\n",
    "\n",
    "Note that the stop words list contains the words in lower case, so you might want to convert a word in the text to lower case using `to_lower_case` function that you implemented above, before checking if it is present in the stop words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0Q3Px3ctiAhi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a55bf56bf9d9487ffdf050248ccb7061",
     "grade": false,
     "grade_id": "cell-8bdbcd7ade04ace4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    \"\"\" \n",
    "    Removes stop words given in `nltk.corpus.stopwords.words(\"english)` from a piece of text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str) : A Python string containing text from which stop words are to be removed\n",
    "\n",
    "    Returns:\n",
    "    - text_no_sw (str): Resulting string after removing stop words.\n",
    "\n",
    "    Hint: You should use `nltk_word_tokenize` for splitting the text\n",
    "        into words\n",
    "\n",
    "    \"\"\"\n",
    "    STOPWORDS = stopwords.words(\"english\")\n",
    "    text_no_sw = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return text_no_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "BMJ5LXnBlTf6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc51fe0889368be680a78e04e814fcea",
     "grade": true,
     "grade_id": "cell-cb3de644e9fed6ff",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "49b72947-13ff-4de3-9596-ed0e4639fa89"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\"\n",
    "test_case_answer = \"Mr. Mrs. Dursley , number four , Privet Drive , proud say perfectly normal !\"\n",
    "test_case_student_answer = remove_stop_words(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Y7Gn_swUvdaM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88f1eea19a02b6baf4bfc5f3c91f812e",
     "grade": false,
     "grade_id": "cell-d4102ca807db93c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.5: Stemming\n",
    "\n",
    "It can be often benificial to group together different inflections of a word into a single term. For eg. *organizes* and *organizing* are the morphological inflections of the word *organize*, and replacing the instances of *organizes* and *organizing* with *organize* in our text data can help us reduce the number of unique words in our vocabulary. Stemming is one such approach to reduce the inflectional forms into a common base form.\n",
    "\n",
    "One common way of performing steming over the words are the *Suffix Removal* algorithms, which involves removing certain suffixes from the word based on a pre-defined set of rules like:\n",
    "- If the word ends with 's' remove 's' (denoted as S-> ϵ.eg. plays -> play)\n",
    "- If the word ends with 'es' remove 'es' (denoted as ES-> ϵ. eg. mangoes -> mango).\n",
    "- If the word ends with 'ing' remove 'ing'. (denoted as ING -> ϵ. eg. enjoying  -> enjoying)\n",
    "- If the word ends with 'ly' remove 'ly'. (denoted as LY -> ϵ. eg. badly -> bad)\n",
    "\n",
    "There can be much more similar rules defined to design a sophisticated stemmer. For now we would want you to implement a simple stemmer that uses only these 4 rules to stem words in a sentence. Complete the `stem_word` and `stem_text` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "cXhjp85SLudH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b10fe8e1bcb0726eb93d521b2290da75",
     "grade": false,
     "grade_id": "cell-1bc0ffe3cd1b08db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def stem_word(word):\n",
    "    \"\"\"\n",
    "    Give a word performs suffix removal stemming on it using the following 4 rules:\n",
    "    - S -> ϵ\n",
    "    - ES -> ϵ\n",
    "    - ING -> ϵ\n",
    "    - LY -> ϵ\n",
    "    If none of the four suffixes is to be found in the word, the it must be returned\n",
    "    as it is.\n",
    "    Parameters:\n",
    "    - word (str) : A python string representing the word to stemmed\n",
    "\n",
    "    Returns:\n",
    "    - stemmed_word (str): `word` after stemming\n",
    "\n",
    "    HINT: It is possible that two rules can be satisfied like for mangoes\n",
    "    both the first two rules are satisfied i.e. it ends with 'es' as well as 's'\n",
    "    In such cases consider the suffix with the larger length which is 'es'\n",
    "    in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    stemmed_word = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Stems all the words in a piece of text, by calling `stem_word` for each word.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): A Python string containing text whose words are to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "    - stemmed_text (str) : Resulting string after stemming.\n",
    "\n",
    "    HINT: You should use `nltk_word_tokenize` for splitting the text\n",
    "        into words\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stemmed_text = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return stemmed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "bMfD5uUOO9NH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb3176e2d18a9427636a04ebb7b36e9b",
     "grade": true,
     "grade_id": "cell-64ed84c6f02326a4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f922d312-fed0-4926-f21d-b45d42f67d55"
   },
   "outputs": [],
   "source": [
    "# Sample test cases for `stem_word`\n",
    "\n",
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"mangoes\"\n",
    "test_case_answer = \"mango\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"plays\"\n",
    "test_case_answer = \"play\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 3:\")\n",
    "test_case = \"enjoying\"\n",
    "test_case_answer = \"enjoy\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 4:\")\n",
    "test_case = \"badly\"\n",
    "test_case_answer = \"bad\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 5:\")\n",
    "test_case = \"fly\"\n",
    "test_case_answer = \"f\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 6:\")\n",
    "test_case = \"connection\"\n",
    "test_case_answer = \"connection\"\n",
    "test_case_student_answer = stem_word(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "Fi5K6d_rS215",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a808e9db159f82789a91920c862857bb",
     "grade": true,
     "grade_id": "cell-3cdab854aefe5efc",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f4ebf00d-ccce-419d-c39c-b1a92cc1f588"
   },
   "outputs": [],
   "source": [
    "# Sample test cases for `stem_text`\n",
    "\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"he sits and eats mangoes\"\n",
    "test_case_answer = \"he sit and eat mango\"\n",
    "test_case_student_answer = stem_text(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"he was badly hurt after playing\"\n",
    "test_case_answer = \"he wa bad hurt after play\"\n",
    "test_case_student_answer = stem_text(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 3:\")\n",
    "test_case = \"fly my little birds\"\n",
    "test_case_answer = \"f my little bird\"\n",
    "test_case_student_answer = stem_text(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 4:\")\n",
    "test_case = \"i am facing a poor network connection\"\n",
    "test_case_answer = \"i am fac a poor network connection\"\n",
    "test_case_student_answer = stem_text(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CPjMhPhHWvvV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de06287eb0d63dbb69bdc7d48bebe1c3",
     "grade": false,
     "grade_id": "cell-5ffdb99792619e58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen above the toy stemmer that we just implemented is not perfect. It is limited to the 4 rules so misses some obvious cases like reducing *connection* to *connect* or sometimes can over stem the word like for the word *fly* above is stemmed to `f` which loses the meaning of the original word. The more sophisticated algorithms use a bunch of rules and heuristics to avoid such situations. One of the most commonly used stemming algorithm is `Porter Stemmer`, which uses a 5 step procedure involving different suffix removal as well as modification rules to perform stemming. Interested students can read more about how Porter Stemmer works from [here](https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/). \n",
    "\n",
    "While implementing a sophisticated stemmer like Porter can be very complex, there are many python packages like *NLTK* (https://www.nltk.org/) that provide pre-implementations of a variety of stemming algorithms. Below we demonstrate how to implement `stem_word` function using different stemmers provided in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VD0PLF-WW8gQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ff1c0fd9e8d7f72af9dbe060a3054ee",
     "grade": false,
     "grade_id": "cell-f17876e41b8b7d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing the stemmers from NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "def stem_word_with_nltk(word, stemmer_type = \"porter\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Stems a word using Porter Stemmer from NLTK\n",
    "    - word (str) : A python string representing the word to be stemmed\n",
    "    - stemmer_type (str) : The type of stemmer to be used - porter, lancaster or snowball\n",
    "\n",
    "    Returns:\n",
    "    - stemmed_word (str): `word` after stemming\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an object of the stemer class\n",
    "    if stemmer_type == \"porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer_type == \"lancaster\":\n",
    "        stemmer = LancasterStemmer()\n",
    "    elif stemmer_type == \"snowball\":\n",
    "        #Snowball stemmer works for multiple languages hence one should be specified during initialization\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "    # Call the `stem` method\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "\n",
    "    return stemmed_word\n",
    "\n",
    "def stem_text_with_nltk(text, stemmer_type = \"porter\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Stems all the words in a piece of text using NLTK's stemers, by calling `stem_word_with_nltk` for each word.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): A Python string containing text whose words are to be stemmed.\n",
    "    - stemmer_type (str) : The type of stemmer to be used - porter, lacaster or snowball \n",
    "\n",
    "    Returns:\n",
    "    - stemmed_text (str) : Resulting string after stemming.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stemmed_text = None\n",
    "\n",
    "    stemmed_text = \" \".join([stem_word_with_nltk(word,stemmer_type) for word in nltk_word_tokenize(text)])\n",
    "\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "vayPsP74Wm-R",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29138a51033c7845825af0bc71ef1a4b",
     "grade": false,
     "grade_id": "cell-1085edc224703e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "85b9a8eb-0535-475d-a91c-23a7cca6fc26"
   },
   "outputs": [],
   "source": [
    "input = \"connection\"\n",
    "porter_output = stem_word_with_nltk(input, \"porter\")\n",
    "lancester_output = stem_word_with_nltk(input, \"lancaster\")\n",
    "snowball_output = stem_word_with_nltk(input, \"snowball\")\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Porter Stemmer Output: {porter_output}\")\n",
    "print(f\"Lancaster Stemmer Output: {lancester_output}\")\n",
    "print(f\"Snowball Stemmer Output: {snowball_output}\")\n",
    "\n",
    "print(\"*******************************************\\n\")\n",
    "\n",
    "input = \"fly\"\n",
    "porter_output = stem_word_with_nltk(input, \"porter\")\n",
    "lancester_output = stem_word_with_nltk(input, \"lancaster\")\n",
    "snowball_output = stem_word_with_nltk(input, \"snowball\")\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Porter Stemmer Output: {porter_output}\")\n",
    "print(f\"Lancaster Stemmer Output: {lancester_output}\")\n",
    "print(f\"Snowball Stemmer Output: {snowball_output}\")\n",
    "\n",
    "print(\"*******************************************\\n\")\n",
    "\n",
    "\n",
    "input = \"happiness\"\n",
    "porter_output = stem_word_with_nltk(input, \"porter\")\n",
    "lancester_output = stem_word_with_nltk(input, \"lancaster\")\n",
    "snowball_output = stem_word_with_nltk(input, \"snowball\")\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Porter Stemmer Output: {porter_output}\")\n",
    "print(f\"Lancaster Stemmer Output: {lancester_output}\")\n",
    "print(f\"Snowball Stemmer Output: {snowball_output}\")\n",
    "\n",
    "print(\"*******************************************\\n\")\n",
    "\n",
    "\n",
    "input = \"operational\"\n",
    "porter_output = stem_word_with_nltk(input, \"porter\")\n",
    "lancester_output = stem_word_with_nltk(input, \"lancaster\")\n",
    "snowball_output = stem_word_with_nltk(input, \"snowball\")\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Porter Stemmer Output: {porter_output}\")\n",
    "print(f\"Lancaster Stemmer Output: {lancester_output}\")\n",
    "print(f\"Snowball Stemmer Output: {snowball_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3xj_15e-fuIA",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e73c861777bc5659cec8abf95729151d",
     "grade": false,
     "grade_id": "cell-5155088a5f3f433d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen these stemmers are able to recognize various types of cases for stemming and can do a better job than our toy implementation. However, even these stemmers tend to make errors, like *operational* is reduced to *op* by Lancaster stemmer. This is because in the end stemming algorithms just use a bunch of heuristics to reduce the inflectional forms without any proper morphological analysis. \n",
    "\n",
    "**Lemmatization** is another common technique used to reduce different forms of a word to a common term called *lemma*, which makes use of a pre-defined vocabulary and morphological analysis of the words. Lemmatizers often work better when supplied with *Part of Speech tags* which we will be covering in the future assignments, so we will revisit Lemmatization later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "giHvaqX-xbEA",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8136719689922daaad9e7d2a99b8880",
     "grade": false,
     "grade_id": "cell-66814cfdcf4a8510",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.6: Combine all preprocessing functions to a single pipeline\n",
    "\n",
    "Now that we are done implementing the main preprocessing functions that might be useful for a text classification task, we can combine them into a single function that applies these 4 preprocessing techniques over a piece of text. We will then use this function to preprocess the new articles in our training and dev datasets.\n",
    "\n",
    "Implement the `preprocess_pipeline` and `preprocess_agnews_data` functions below, where the former function combines the 4 preprocessing functions implemented above and the latter applies that to all the news articles in the dataset.\n",
    "\n",
    "Note: The order in which different preprocessing functions are to be applied can lead to different results. For eg. all of our stop words are in lower case hence before we remove them from the text we must make sure we have converted the text to lower case before hand. Please follow the following order for applying the preprocessing functions in your code:\n",
    "\n",
    "  1. to_lower_case\n",
    "  2. remove_punctuations\n",
    "  3. remove_stop_words\n",
    "  4. stem_text_with_nltk (call this with `stemmer_type = \"porter\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "dpZtpNXqdQJ1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f9f76c0494c8f6d9809cbeb37d23a46",
     "grade": false,
     "grade_id": "cell-e862d86747d6aba4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a piece of text applies preprocessing techniques\n",
    "    like converting to lower case, removing stop words and punctuations,\n",
    "    and stemming.\n",
    "\n",
    "    Apply the functions in the following order:\n",
    "    1. to_lower_case\n",
    "    2. remove_punctuations\n",
    "    3. remove_stop_words\n",
    "    4. stem_text_with_nltk (call this with `stemmer_type = \"porter\"`)\n",
    "\n",
    "    Inputs:\n",
    "    - text (str) : A python string containing text to be pre-processed\n",
    "\n",
    "    Returns:\n",
    "    - text_preprocessed (str) : Resulting string after applying preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    text_preprocessed = None\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "mtrD4GXK3ZcB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1094f30d390f9701896b9fb39c009860",
     "grade": true,
     "grade_id": "cell-4b74127c07a2894d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "538440cc-a069-40b5-e527-cca85ffb93e4"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\"\n",
    "test_case_answer = \"mr mr dursley number four privet drive proud say perfectli normal\"\n",
    "test_case_student_answer = preprocess_pipeline(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = \"\\\"Little tyke,\\\" chortled Mr. Dursley as He left the house.\"\n",
    "test_case_answer = \"littl tyke chortl mr dursley left hous\"\n",
    "test_case_student_answer = preprocess_pipeline(test_case)\n",
    "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "vHxSv9-FiRhA",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9a0930fc65cdaf362a0efdd6b93bd50",
     "grade": false,
     "grade_id": "cell-910d9f1b54a3cc70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_agnews_data(df):\n",
    "    \"\"\"\n",
    "    Takes the pandas dataframe containing ag_news data as input and applies\n",
    "    the `preprocess_pipeline` function to the its sentence column.\n",
    "\n",
    "    Inputs:\n",
    "    - df (pd.DataFrame): A pandas dataframe containing the SST-2 data with format:\n",
    "        | news      | label  | label_readable |\n",
    "        ---------------------------------------\n",
    "        | news_1    | label_1| label_1_readable|\n",
    "        | news_2    | label_2| label_2_readable|\n",
    "        | ..........| .......|. ...............|\n",
    "        | ..........| .......|. ...............|\n",
    "\n",
    "    Returns (pd.DataFrame):\n",
    "    - df_preprocessed: Resulting dataframe after applying `preprocessing_pipeline`\n",
    "    to the `sentence` column. Note that the column names of df_preprocessed\n",
    "    should be same as df and its should have the same number of rows as `df`.\n",
    "\n",
    "\n",
    "    Hint: Look up how to use `pd.DataFrame.apply` method in pandas\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_preprocessed = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return df_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "zjlOsHBpx7Oe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d061823f520357c24e18c1fb0f6e6e92",
     "grade": false,
     "grade_id": "cell-8c9d5693a8e93a17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0ad8f6f7-2d65-4826-8ad1-639fedd984a4"
   },
   "outputs": [],
   "source": [
    "# Preprocess the train and test sets. This might take a few minutes\n",
    "train_df_preprocessed = preprocess_agnews_data(train_df)\n",
    "test_df_preprocessed = preprocess_agnews_data(test_df)\n",
    "print(train_df_preprocessed.head())\n",
    "print(\"***************************\")\n",
    "print(test_df_preprocessed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "8hVfSsBy6zX8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bab326a2c38824960b930829ba9ba3c",
     "grade": true,
     "grade_id": "cell-6e496720d48f93dd",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d52d66dd-03e2-4f7e-95af-a72d5cdb4f55"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1: Checking if the object returned is a pandas Dataframe\")\n",
    "assert isinstance(train_df_preprocessed, pd.DataFrame)\n",
    "print(\"Test Case Passed :)\")\n",
    "print(\"***********************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 2: Checking if the returned dataframe has correct columns\")\n",
    "student_column_names = sorted(list(train_df_preprocessed.columns))\n",
    "expected_column_names = [\"label\", \"label_readable\", \"news\"]\n",
    "\n",
    "assert student_column_names == expected_column_names\n",
    "print(\"Test Case Passed :)\")\n",
    "print(\"***********************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 3: Checking if the number of rows of the returned dataframe is same as the original dataframes\")\n",
    "assert len(train_df) == len(train_df_preprocessed) and len(test_df) == len(test_df_preprocessed)\n",
    "print(\"Test Case Passed :)\")\n",
    "print(\"***********************************\\n\")\n",
    "print(\"Sample Test Case 4: Checking if the returned dataframe has sentences preprocessed\")\n",
    "\n",
    "student_output = train_df_preprocessed[\"news\"].values[1]\n",
    "expected_output = \"marsh avert cash crunch embattl insur broker 39 bank agre waiv claus may prevent access credit new york reuter marsh amp mclennan co\"\n",
    "assert evaluate_string_test_cases(train_df[\"news\"].values[1], student_output, expected_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XzbDxFiUgut9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9de95237ad42f41f3e0d54f6674c514c",
     "grade": false,
     "grade_id": "cell-2d3b021cb109b69d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Bag Of Word Models for Text Classification (45 minutes)\n",
    "\n",
    "Now that we are done with preprocessing the news articles in our datasets, we can begin building a classifier to classify the articles into topics.\n",
    "\n",
    "As you might have studied in your Machine Learning courses, typical ML models work on the data described using mathematical objects like vectors and matrices, which are often referred to as features. These features can be of different types depending upon the downstream application, like for building a classifier to predict whether to give credit to a customer we might consider features like their age, income, employement status etc. In the same way to build a classifier for textual data, we need a way to describe each text example in terms of numeric features which can then be fed to the classification algorithm of our choice.\n",
    "\n",
    "Bag of Words model is one of the simplest but surprisingly effective way to represent text data for building Machine Learning models. In bag of words, occurence (or frequency) of each word in a given text example is defined as a feature for training the classifier. The order in which these words occur in the text is not relevant and we are just concerned with which words are present in the text. Consider the following example to understand how bag of words are used to represent text.\n",
    "\n",
    "As an example consider we have 2 examples present in our dataset:\n",
    "\n",
    "x1: john likes to watch movies mary likes movies too\n",
    "\n",
    "x2: mary also likes to watch football games\n",
    "\n",
    "Based on these two documents we can get the list of all words that occur in this dataset which will be:\n",
    "\n",
    "| index | word     |\n",
    "|-------|----------|\n",
    "| 0     | also     |\n",
    "| 1     | football |\n",
    "| 2     | games    |\n",
    "| 3     | john     |\n",
    "| 4     | likes    |\n",
    "| 5     | mary     |\n",
    "| 6     | movies   |\n",
    "| 7     | to       |\n",
    "| 8     | too      |\n",
    "| 9     | watch    |\n",
    "\n",
    "We can then define features for the two x1 and x2 as follows:\n",
    "\n",
    "\n",
    "|    | also | football | games | john | likes | mary | movies | to | too | watch |\n",
    "|----|------|----------|-------|------|-------|------|--------|----|-----|-------|\n",
    "| x1 | 0    | 0        | 0     | 1    | 2     | 1    | 2      | 1  | 2   | 1     |\n",
    "| x2 | 1    | 1        | 1     | 0    | 1     | 1    | 0      | 0  | 0   | 0     |\n",
    "\n",
    "These features can then be used to train an ML model. To summarize the following two steps must be followed to create bag of word representations of the text examples in a dataset.\n",
    "\n",
    "- Step 1: Create a word vocabulary by iterating through all the documents in the **training** dataset, storing all the unique words that are present in each document. Also maintain mappings to map each word to an index and vice-versa, which we will need to define values for each feature dimension.\n",
    "\n",
    "- Step 2: For each document in the training and test sets, get the frequency of each word in our vocabulary and use it to define feature for that example. \n",
    "\n",
    "Below you will implement functions to create bag of words representations of the dataset examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UWPxJNLMwMyu",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c80b4d16c1796e1b71e2b09977e7bcf",
     "grade": false,
     "grade_id": "cell-761f58e096b7331e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1: Create vocabularies\n",
    "\n",
    "As described above our first step will be to create word vocabulary for the documents in our dataset. Implement `create_vocab` function below which takes as input a list of documents and creates a list of unique words that occur in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "CtE_uIoVqIj0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ba98f2dd085b8d975e95d721236dc58",
     "grade": false,
     "grade_id": "cell-1d86cd6bcfca9df9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_vocab(documents):\n",
    "    \"\"\"\n",
    "    Given a list of documents each represented as a string,\n",
    "    create a word vocabulary containing all the words that occur\n",
    "    in these documents.\n",
    "\n",
    "    Inputs:\n",
    "    - documents (list) : A list with each element as a string representing a\n",
    "    document.\n",
    "\n",
    "    Returns:\n",
    "    - vocab (list) : A **sorted** list containing all unique words in the\n",
    "    documents\n",
    "\n",
    "    Example Input: ['john likes to watch movies mary likes movies too',\n",
    "                  'mary also likes to watch football games']\n",
    "\n",
    "    Expected Output: ['also',\n",
    "                    'football',\n",
    "                    'games',\n",
    "                    'john',\n",
    "                    'likes',\n",
    "                    'mary',\n",
    "                    'movies',\n",
    "                    'to',\n",
    "                    'too',\n",
    "                    'watch']\n",
    "\n",
    "\n",
    "    Hint: `nltk_word_tokenize` function may come in handy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return sorted(vocab) # Don't change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "VV9gzaGM80ad",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b743ca10fb0bb9a5089d2784801b5445",
     "grade": true,
     "grade_id": "cell-93811b38a1766d53",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e5a653e6-72f4-458a-d4ce-1e5c8e8b43cf"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "\n",
    "test_case = [\"john likes to watch movies mary likes movies too\",\n",
    "              \"mary also likes to watch football games\"]\n",
    "test_case_answer = ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
    "test_case_student_answer = create_vocab(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "\n",
    "test_case = [\"We all live in a yellow submarine.\",\n",
    "             \"Yellow submarine, yellow submarine!!\"\n",
    "             ]\n",
    "test_case_answer = ['!', ',', '.', 'We', 'Yellow', 'a', 'all', 'in', 'live', 'submarine', 'yellow']\n",
    "test_case_student_answer = create_vocab(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "N4d5FU_11Xc9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0b2a56933497d8a65a07eec7d76d10a",
     "grade": false,
     "grade_id": "cell-d6ece6cd0f7791ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note the output of sample test case 2 contains punctuations as well as different upper-case and lower-case variants of a word detected as seperate words. This illustrates the importance of performing the preprocessing over the documents as it reduces the unnecessary words like punctuations, stop words in the vocablary as well as help provide a common term to different variants of a word. To illustrate this further, we create vocabulary without performing preprocessing and with pre-procesing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fee3c5369ab2e5b5f256e4d1afa40c55",
     "grade": false,
     "grade_id": "cell-b07ae93b336562b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create vocabulary for the ag_news dataset without pre-processing\n",
    "\n",
    "train_documents = train_df[\"news\"].values.tolist()\n",
    "train_vocab = create_vocab(train_documents)\n",
    "\n",
    "print(f\"Training Vocabulary Created. Number of words: {len(train_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "124k words! This means we will be having 124k features for each document. Note that our training data has 120k documents, hence we can easily run into overfitting (Remember your ML class!) if we consider this vocabulary. Let's see how much applying the pre-processing helps in reducing the number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "gghG6ybs0sSn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "103d7c0f40e87b9e59a2e70a4dc90cfb",
     "grade": false,
     "grade_id": "cell-16a71f3a50681f1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d83bc255-e1d5-4735-fb2a-9ee79f9494c2"
   },
   "outputs": [],
   "source": [
    "# Let's create vocabulary for the ag_news dataset with pre-processing\n",
    "\n",
    "train_documents = train_df_preprocessed[\"news\"].values.tolist() # Note that we are selecting preprocessed documents\n",
    "train_vocab = create_vocab(train_documents)\n",
    "\n",
    "print(f\"Training Vocabulary Created. Number of words: {len(train_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f26ba5a0dd4641786901ea69785565c9",
     "grade": false,
     "grade_id": "cell-df0a5af77f24fb7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "81k is definitely much better than 124k. However, this can result in large computational cost for training these models and also a possibility of overfitting! One other simple way to reduce the vacabulary is to remove tokens that appear very rarely in the corpus. Implement the function `create_vocab_w_rare_word_filter` below that does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b21d6b671d2b07f504f1655cb704b25b",
     "grade": false,
     "grade_id": "cell-d2f25d47e58c9120",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_vocab_w_rare_word_filter(documents, threshold):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a list of documents each represented as a string,\n",
    "    create a word vocabulary containing all the words that occur\n",
    "    more than `threshold` times in these documents.\n",
    "\n",
    "    Inputs:\n",
    "    - documents (list) : A list with each element as a string representing a\n",
    "    document.\n",
    "    - threshold (int) : A threshold value to filter out rare words\n",
    "\n",
    "    Returns:\n",
    "    - vocab (list) : A **sorted** list containing all unique words in the\n",
    "    documents\n",
    "\n",
    "    Example Input: corpus = ['john likes to watch movies mary likes movies too',\n",
    "                  'mary also likes to watch football games']\n",
    "                  threshold = 1\n",
    "\n",
    "    Expected Output: ['likes', 'mary', 'movies', 'to', 'watch']\n",
    "\n",
    "\n",
    "    Hint:  collections.Counter might come handy\n",
    "\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return sorted(vocab) # Don't change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbab233f469bd45d252afa14e6bc432e",
     "grade": true,
     "grade_id": "cell-c413e7738036bbb7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "\n",
    "test_case = [\"john likes to watch movies mary likes movies too\",\n",
    "              \"mary also likes to watch football games\"]\n",
    "test_case_answer = ['likes', 'mary', 'movies', 'to', 'watch']\n",
    "test_case_student_answer = create_vocab_w_rare_word_filter(test_case, threshold=1)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "\n",
    "test_case = [\"john likes to watch movies mary likes movies too\",\n",
    "              \"mary also likes to watch football games\"]\n",
    "test_case_answer = ['likes']\n",
    "test_case_student_answer = create_vocab_w_rare_word_filter(test_case, threshold=2)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51ecc21e3ab084f6f25e9f8e7c7c804e",
     "grade": false,
     "grade_id": "cell-61d959bfe5288c52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a vocabulary with words that appear at least 5 times in the training set\n",
    "\n",
    "threshold = 5\n",
    "train_documents = train_df_preprocessed[\"news\"].values.tolist() # Note that we are selecting preprocessed documents\n",
    "train_vocab = create_vocab_w_rare_word_filter(train_documents, threshold=threshold)\n",
    "\n",
    "print(f\"Training Vocabulary Created. Number of words: {len(train_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed78bb27edc6499cf9e5833e48eea2c7",
     "grade": false,
     "grade_id": "cell-670b1c867afb77ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we have ~20k words in our vocabulary which is much more manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "duY4XnCV5-Rv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb3add6c95f4f98579d6838b3ee23489",
     "grade": false,
     "grade_id": "cell-87821ca15761432a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.2: Create word to index mapping\n",
    "Now that we have a list of words in our dataset, we can map each word to an index which will be useful to represent what word each feature dimension refers to. Implement the `get_word_idx_mapping` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "caT93EMH5Tfx",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0c747e0b688b882faa3958cc847e3da",
     "grade": false,
     "grade_id": "cell-832da6a1aea10989",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_word_idx_mapping(vocab):\n",
    "\n",
    "    \"\"\"\n",
    "    Give a list of strings each representing a word in the vocabulary\n",
    "    creates a dictionary that maps each word in the list to its\n",
    "    corresponding index.\n",
    "\n",
    "    Inputs:\n",
    "    - vocab (list): A list of strings each representing a word in the vocabulary\n",
    "\n",
    "    Outputs:\n",
    "    - word2idx (dict): A Python dictionary mapping each word to its index in vocabulary\n",
    "\n",
    "    Example Input: ['also',\n",
    "                    'football',\n",
    "                    'games',\n",
    "                    'john',\n",
    "                    'likes',\n",
    "                    'mary',\n",
    "                    'movies',\n",
    "                    'to',\n",
    "                    'too',\n",
    "                    'watch']\n",
    "\n",
    "    Expected Output: {'also': 0,\n",
    "                  'football': 1,\n",
    "                  'games': 2,\n",
    "                  'john': 3,\n",
    "                  'likes': 4,\n",
    "                  'mary': 5,\n",
    "                  'movies': 6,\n",
    "                  'to': 7,\n",
    "                  'too': 8,\n",
    "                  'watch': 9}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word2idx = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "80IR-BKZ8u4d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1af4dd53a33d04409b85a77a6e99b53",
     "grade": true,
     "grade_id": "cell-aac76832a8b468b7",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "96d261fd-1d25-4b02-b9fb-181318ef0d4b"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
    "test_case_answer = {'also': 0, 'football': 1, 'games': 2, 'john': 3, 'likes': 4, 'mary': 5, 'movies': 6, 'to': 7, 'too': 8, 'watch': 9}\n",
    "test_case_student_answer = get_word_idx_mapping(test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "9bEPYSTg-Mf7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37ea894b6f3567bdf3f7984771968891",
     "grade": false,
     "grade_id": "cell-4f89de4d46c99ff5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c3769f69-c100-40aa-dc74-24ec36a9c62c"
   },
   "outputs": [],
   "source": [
    "train_vocab2idx = get_word_idx_mapping(train_vocab)\n",
    "train_vocab2idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "55-NnrC39qgd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4680e268db8929fe98bcf6dbb68ff6a",
     "grade": false,
     "grade_id": "cell-9e6028db09fce23c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2.3: Create Bag of word features for the documents (1 Mark)\n",
    "\n",
    "Now that we have the list of words and a word to index mapping we can create a bag of word feature vector for each of the document present in training and test data. Implement the function `get_document_bow_feature` that takes as an input a document, a vocabulary, and a vocabulary to index mapping, and returns the feature vector for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "LoauVlK29n1c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "892d6428adad37dff128d47eb5272c34",
     "grade": false,
     "grade_id": "cell-418f7d59edb65074",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_document_bow_feature(document, vocab, word2idx):\n",
    "    \"\"\"\n",
    "    Given a string representing the document and the vocabulary, create a bag of\n",
    "    words representation of the document i.e. a feature vector where each feature\n",
    "    is defined as the frequency of each word in the vocabulary.\n",
    "\n",
    "    Inputs:\n",
    "    - document (str): A python string representing the document for which features are to be obtained\n",
    "    - vocab (list): A list of words present in the vocabulary\n",
    "    - word2idx (dict): A dictionary that maps each word to an index.\n",
    "\n",
    "    Returns:\n",
    "    - bow_feature (numpy.ndarray): A numpy array of size `len(vocab)` whose each element contains the count of each word in the vocabulary.\n",
    "\n",
    "    Example Input:\n",
    "    document = \"john likes to watch movies mary likes movies too\"\n",
    "    vocab = ['also','football','games','john','likes','mary', 'movies','to','too','watch']\n",
    "    word2idx = {'also': 0, 'football': 1, 'games': 2, 'john': 3, 'likes': 4, 'mary': 5, 'movies': 6, 'to': 7, 'too': 8, 'watch': 9}\n",
    "\n",
    "    Expected Output: array([0, 0, 0, 1, 2, 1, 2, 1, 1, 1])\n",
    "\n",
    "    \"\"\"\n",
    "    bow_feature = np.zeros(len(vocab))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return bow_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "a1JpF6sdB__c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67be1e9a6523f4c2422d7108f65361c4",
     "grade": true,
     "grade_id": "cell-d45f4eb621fbd845",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0b9696bd-a822-4407-9451-12155f6db7ab"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "print(\"Sample Test Case 1:\")\n",
    "test_case = {\"document\": \"john likes to watch movies mary likes movies too\",\n",
    "             \"vocab\": ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch'],\n",
    "             \"word2idx\": {'also': 0, 'football': 1, 'games': 2, 'john': 3, 'likes': 4, 'mary': 5, 'movies': 6, 'to': 7, 'too': 8, 'watch': 9}\n",
    "             }\n",
    "test_case_answer = np.array([0, 0, 0, 1, 2, 1, 2, 1, 1, 1])\n",
    "test_case_student_answer = get_document_bow_feature(**test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer.tolist(), test_case_answer.tolist())\n",
    "\n",
    "print(\"Sample Test Case 2:\")\n",
    "test_case = {\"document\": \"mary also likes to watch football games\",\n",
    "             \"vocab\": ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch'],\n",
    "             \"word2idx\": {'also': 0, 'football': 1, 'games': 2, 'john': 3, 'likes': 4, 'mary': 5, 'movies': 6, 'to': 7, 'too': 8, 'watch': 9}\n",
    "             }\n",
    "test_case_answer = np.array([1, 1, 1, 0, 1, 1, 0, 1, 0, 1])\n",
    "test_case_student_answer = get_document_bow_feature(**test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer.tolist(), test_case_answer.tolist())\n",
    "\n",
    "print(\"Sample Test Case 3:\")\n",
    "test_case = {\"document\": \"mary and jane also like to watch football games\",\n",
    "             \"vocab\": ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch'],\n",
    "             \"word2idx\": {'also': 0, 'football': 1, 'games': 2, 'john': 3, 'likes': 4, 'mary': 5, 'movies': 6, 'to': 7, 'too': 8, 'watch': 9}\n",
    "             }\n",
    "test_case_answer = np.array([1, 1, 1, 0, 0, 1, 0, 1, 0, 1])\n",
    "test_case_student_answer = get_document_bow_feature(**test_case)\n",
    "assert evaluate_list_test_cases(test_case, test_case_student_answer.tolist(), test_case_answer.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cM4Of6HOIvf5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d12c868923719f499525e50c8abdc1b",
     "grade": false,
     "grade_id": "cell-90d4b81b34e40d26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that our `get_document_bow_feature` function seems to work properly, let's get bag of word features for the examples in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "o5dPduZoIty6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4511d6950bcf7f52e7a048ec24c8212",
     "grade": false,
     "grade_id": "cell-7c94a44d91cfadb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a7e56e67-d591-4838-8cbb-d2f299ea3e6a"
   },
   "outputs": [],
   "source": [
    "# Getting bow feature for one training document first\n",
    "training_example = train_documents[0]\n",
    "bow_feature = get_document_bow_feature(training_example,\n",
    "                                       train_vocab,\n",
    "                                       train_vocab2idx)\n",
    "print(f\"Length of bow feature: {len(bow_feature)}\")\n",
    "print(f\"Number of non-zero entries in the bow feature: {len(bow_feature[bow_feature != 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z4WgKREUJmlC",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4ea5953bf4565a9145d135faa55be51",
     "grade": false,
     "grade_id": "cell-90aeadd30f0658df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, since our vocabulary size is 19939, bag of words vectors for each document will be of size 19939. Since we have about 67k training examples, it won't be practical to store these high dimensional vectors as it is for all the documents. Instead of storing the features for all the documents in memory, we query the features while training the model in a batch wise fashion i.e. at a time we train on N examples, such that N <<< 19939. This will be more clear when we discuss creating datasets and dataloaders in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9sxGKKCTwdzr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "861c677e3e0820c3ae8e831bcb43b9a6",
     "grade": false,
     "grade_id": "cell-845b66f6ad9eba70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Training a Linear Classifier using Bag of Words Features (1 hr 30 minutes)\n",
    "\n",
    "Now that we have defined our numerical features to represent each of the documents, we can start training a classifier on top of it. For the purposes of this assignment we will be implementing a linear classifier namely **Logistic Regression**. We assume that you have studied logistic regression in your Machine Learning course.\n",
    "\n",
    "We will be using Pytorch to implement and train our logistic regression classifier for the topic prediction task. We start by implementing the dataset and dataloaders to iterate over the dataset, and then we move on to defining the logistic regression module, the cross entropy loss function and an optimizer for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JFj2z-3K0L91",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9be6020baa8c26f427a594f1093ec24",
     "grade": false,
     "grade_id": "cell-f297dd9e6d90ed73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.1: Defining Pytorch Dataset and Dataloaders to iterate over the AG News Dataset\n",
    "\n",
    "Often while training neural networks or in our case linear classifiers, it is not practical to train over the entire dataset at once, and instead we use a batch wise training strategy, where we iterate over different batches of the dataset. Hence, it is useful to define iterators for doing the same. Defining pipelines for processing data samples and then writing iterators on top of that can often be very messy. Pytorch provides `torch.utils.data.Dataset` and `torch.utils.data.Dataloader` classes that make it much more convenient to do the same in a modular fashion.\n",
    "\n",
    "`torch.utils.data.Dataset` provides a wrapper to store our dataset, which can then be used by `torch.utils.data.Dataloader` to define an iterable over the dataset. To learn more about Dataset and Dataloader classes, please refer to the tutorial [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "We start by defining a custom Dataset class for our dataset by extending the `torch.utils.data.Dataset` class. A custom Dataset class must implement three functions:\n",
    "-  `__init__`: This is the constructor for our custom class, and is often used to store the (meta)data, which can then be used by the other functions to create samples of the dataset.\n",
    "- `__len__`: This method returns the number of examples present in the dataset.\n",
    "-  `__getitem__`: This method loads and returns a sample stored at the given index `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "f_FthraCDbM8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb50fa1a6ff4f1741994d8db8899e45b",
     "grade": false,
     "grade_id": "cell-924f8c8e69440c91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, documents, labels, vocab, word2idx):\n",
    "        \"\"\"\n",
    "        Store dataset documents and labels here so that they can be used by\n",
    "        __getitem__ to process and return the samples.\n",
    "\n",
    "        Inputs:\n",
    "          - documents (list): A list of strings containing news articles in our dataset.\n",
    "          - labels (list): A list of class labels (0,1,2 and 3) corresponding to each document.\n",
    "          - vocab (list): A list of words present in the vocabulary\n",
    "          - word2idx (dict): A dictionary that maps each word to an index.\n",
    "        \"\"\"\n",
    "\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns the features and label corresponding to the `idx` index\n",
    "        in the documents and labels lists.\n",
    "\n",
    "        Inputs:\n",
    "          - idx (index): Index of the dataset example to be loaded and returned\n",
    "\n",
    "        Returns:\n",
    "          - features (numpy.ndarray): The bag of word features corresponding the document indexed by `idx`\n",
    "          - label (int): The class label for the `idx`th document\n",
    "\n",
    "        Hint: You can get the document and label by doing self.documents[idx],\n",
    "        self.labels[idx]. Features of the document are to be extracted via\n",
    "        `get_document_bow_feature` function\n",
    "        \"\"\"\n",
    "\n",
    "        features, label = None, None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return features, int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "IKpOKip1OqCz",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1866ce4511f146f97c849c5ca7a13cd8",
     "grade": true,
     "grade_id": "cell-25ef123969040228",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3909da20-c7d6-4d09-fc72-f6f53bdb9a6d"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "sample_documents = [\"google launches their latest pixel tablet\",\n",
    "                    \"manchester united wins the fa cup final\"]\n",
    "sample_labels = [3, 1]\n",
    "sample_vocab = create_vocab_w_rare_word_filter(sample_documents, threshold=0)\n",
    "sample_word2idx = get_word_idx_mapping(sample_vocab)\n",
    "sample_dataset = AGNewsDataset(sample_documents, sample_labels,\n",
    "                             sample_vocab, sample_word2idx)\n",
    "\n",
    "test_case_idx = 0\n",
    "features, label = sample_dataset.__getitem__(test_case_idx)\n",
    "\n",
    "print(f\"Sample Test Case 1: Testing Returned Labels for idx = {test_case_idx}\")\n",
    "print(f\"Output Label: {label}\")\n",
    "print(f\"Expected Label: {sample_labels[test_case_idx]}\")\n",
    "assert label == sample_labels[test_case_idx]\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 2: Testing returned Features for idx = {test_case_idx}\")\n",
    "expected_features = [0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "print(f\"Output Features: {features.tolist()}\")\n",
    "print(f\"Expected Features: {expected_features}\")\n",
    "assert expected_features == features.tolist()\n",
    "\n",
    "test_case_idx = 1\n",
    "features, label = sample_dataset.__getitem__(test_case_idx)\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 3: Testing Returned Labels for idx = {test_case_idx}\")\n",
    "print(f\"Output Label: {label}\")\n",
    "print(f\"Expected Label: {sample_labels[test_case_idx]}\")\n",
    "assert label == sample_labels[test_case_idx]\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 4: Testing returned Features for idx = {test_case_idx}\")\n",
    "expected_features = [1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1]\n",
    "print(f\"Output Features: {features.tolist()}\")\n",
    "print(f\"Expected Features: {expected_features}\")\n",
    "assert expected_features == features.tolist()\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Np8tcm0mUpG9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b3d6655cd1f8e8c6f102b584a470a3b",
     "grade": false,
     "grade_id": "cell-7a79bd8f8e1ddc4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that the custom class AGNewsDataset seems to working fine we can create objects for our training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5kFDYDpUCbi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22d953c2ec3dc28aa787564724854db3",
     "grade": false,
     "grade_id": "cell-3d4fb6f323f29364",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get documents and labels from the dataset\n",
    "train_documents = train_df_preprocessed[\"news\"].values.tolist()\n",
    "train_labels = train_df[\"label\"].values.tolist()\n",
    "test_documents = test_df_preprocessed[\"news\"].values.tolist()\n",
    "test_labels = test_df[\"label\"].values.tolist()\n",
    "\n",
    "# Create vocabulary from training data\n",
    "train_vocab = create_vocab_w_rare_word_filter(train_documents, threshold=5)\n",
    "train_word2idx = get_word_idx_mapping(train_vocab)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AGNewsDataset(train_documents,\n",
    "                            train_labels,\n",
    "                            train_vocab,\n",
    "                            train_word2idx)\n",
    "test_dataset = AGNewsDataset(test_documents,\n",
    "                           test_labels,\n",
    "                           train_vocab,\n",
    "                           train_word2idx\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GfSYJmHoV1o8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be4a0ad98f1734f3a8dfa1a7df38ab94",
     "grade": false,
     "grade_id": "cell-9fd000c069aa4c09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how we used training data vocabulary for creating test dataset as well. Can you tell why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oLnu4s83V_Qj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3ca003dc648aba52cad73ee376934c9",
     "grade": false,
     "grade_id": "cell-e2fe967b3fd6a3af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have created our training and test datasets, we can create dataloaders to iterate over them in batches. We will use a batch size of 64 in our experiments. Note that lower the batch size lesser will be your memory requirements but the noisier will be the training. Since in our case features are sufficiently high dimensional, we might not want to use too large of a batch size, hence we are using 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6Y1Dv0inRi_o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f583c8e751bde8e34a1f950f8ffd9cad",
     "grade": false,
     "grade_id": "cell-129a08eb6dde4719",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BoU8QOTSXPlk",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e432170cebdb98ea20c39e0ae65629b1",
     "grade": false,
     "grade_id": "cell-2dfd4c445918497e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Dataloaders work like any iterable (like Lists, dictionaries etc) in python can be iterated over using a for loop like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "mCj8wKC7XPEv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8836b61601d7cd742496ddcf8e864609",
     "grade": false,
     "grade_id": "cell-9b44a960453c0f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "21888519-6c6a-4686-b763-25e5a993b658"
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    # Unpacking the batch\n",
    "    features, labels = batch\n",
    "    print(f\"Features Shape: {features.shape}\")\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "\n",
    "    # We will break for now as this is just for demonstration\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-JOot4W8Yr8y",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cf527f4af9f0204ccbf72a80959718b",
     "grade": false,
     "grade_id": "cell-6d795607c997ddb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how each batch unraps to a features and a labels torch tensor. Features is a 64x19939, where 64 is the batch size used by us and 19939 is the numeber of features we have for each document. Torch tensors behave very similar to numpy arrays, with the benefit that these can be transferred to a GPU and also support auto-differentiation. We will address these points again when we implement the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-YwQLCrJZr8I",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83a86f8cef3dd021744fd987af23a346",
     "grade": false,
     "grade_id": "cell-eb2d1a2cd2ce6882",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.2: Define the model architecture for the Multinomial Logistic Regression Classifier\n",
    "\n",
    "Multinomial Logistic Regression is an extension of logistic regression to work on multi-class classification problem. The major difference between the binary and multinomial logistic regression is that we use a softmax function over the outputs instead of sigmoid. The figure below neatly summarizes the computation for multinomial logistic regression (ignore the hinge loss part for now, we will revisit it soon):\n",
    "\n",
    "![multi_class_linear_classifiers](https://i.ibb.co/rZNRRqB/multiclass-lin-cls.png)\n",
    "\n",
    "You can refer to these [amazing notes](https://cs231n.github.io/linear-classify/) for detailed discussion of multi-class linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbed4fd275ceef6cd7149bbc16ac2c1e",
     "grade": false,
     "grade_id": "cell-850f2a5a70f53f1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Pytorch provides very elegantly designed `torch.nn` module which contains building blocks for creating different neural network architectures. Some of the sub-modules included in `torch.nn` includes:\n",
    "\n",
    "- `torch.nn.Linear` : Perhaps one of the simplest of the nn modules, it is used to apply a linear transformation to the data i.e. y = xA^T +b, where x is the input and y is the output of the layer. A and b are the parameters of the layer, where A is often called the weights matrix and b is the bias vector.\n",
    "\n",
    "- `torch.nn.Conv2d`: Used to create Convolutional Layers.\n",
    "\n",
    "- `torch.nn.Transformer`: Used to create Transformer layers\n",
    "\n",
    "and many more. For the purposes of this assignment we will only be needing `torch.nn.Linear` to define our network.\n",
    "\n",
    "It also supports different activation functions like:\n",
    "- `torch.nn.ReLU` \n",
    "- `torch.nn.Sigmoid`\n",
    "- `torch.nn.Tanh`\n",
    "- `torch.nn.Softmax`\n",
    "\n",
    "\n",
    "\n",
    "Below we demonstrate the usage of some of these modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "f3TXwXWWh7hu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b847c7eb6d9df165ff72d6fdfcaab27",
     "grade": false,
     "grade_id": "cell-f9edd5a25c8c910c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8fe3782b-e942-413f-d804-871d25437a09"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer that maps a 5 dimensional vector to a 3 dimensional vector\n",
    "example_linear_layer = nn.Linear(5, 3) # Initialize a linear layer\n",
    "example_input = torch.rand(5) # Create a random vector for demonstration\n",
    "print(f\"Input: {example_input}\")\n",
    "print(f\"Input Shape: {example_input.shape}\")\n",
    "example_output = example_linear_layer(example_input) # Feed the example input to the linear layer\n",
    "print(f\"Linear layer output: {example_output}\")\n",
    "print(f\"Linear layer output Shape: {example_output.shape}\")\n",
    "\n",
    "print(\"************************************************\\n\")\n",
    "\n",
    "# We can also use linear layers with batched inputs\n",
    "example_batch_input = torch.rand(4,5) # Create an example input containing 4 inputs of 5 dimension each\n",
    "print(f\"Batched Input:\\n {example_batch_input}\")\n",
    "print(f\"Batched Input Shape: {example_batch_input.shape}\")\n",
    "example_batch_output = example_linear_layer(example_batch_input)\n",
    "print(f\"Linear layer batched output:\\n {example_batch_output}\")\n",
    "print(f\"Linear layer batched output Shape: {example_batch_output.shape}\")\n",
    "\n",
    "print(\"************************************************\\n\")\n",
    "\n",
    "# Using activation functions\n",
    "sigmoid_activation = nn.Sigmoid() #Define sigmoid activation\n",
    "relu_activation = nn.ReLU() #Define relu activation\n",
    "softmax_activation = nn.Softmax(dim=-1) #Define softmax activation\n",
    "log_softmax_activation = nn.LogSoftmax(dim=-1) #Define softmax activation\n",
    "\n",
    "\n",
    "sigmoid_output = sigmoid_activation(example_batch_output) # Apply the sigmoid function to the output of the linear layer\n",
    "relu_output = relu_activation(example_batch_output) # Apply the relu function to the output of the linear layer\n",
    "softmax_output = softmax_activation(example_batch_output) # Apply the softmax function to the output of the linear layer\n",
    "log_softmax_output = log_softmax_activation(example_batch_output) # Apply the softmax function to the output of the linear layer\n",
    "\n",
    "print(f\"Before Applying the activation function:\\n {example_batch_output}\")\n",
    "print(f\"After Applying the sigmoid function:\\n {sigmoid_output}\")\n",
    "print(f\"After Applying the relu function:\\n {relu_output}\")\n",
    "print(f\"After Applying the softmax function:\\n {softmax_output}\")\n",
    "print(f\"After Applying the log softmax function:\\n {log_softmax_output}\")\n",
    "print(\"************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "K81l_ca-tPre",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c0ef9e9bfb4945074b9bb6d6ea6defd",
     "grade": false,
     "grade_id": "cell-7d61d873e142415c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how the outputs of the nn layers also contain a `grad_fn`. This is used during backpropagation to compute the gradients which are used by the optimizer to learn the parameters of these layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WKgETCP1h6P9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d64e82043018e14aa55803fc009191e8",
     "grade": false,
     "grade_id": "cell-8a8cb39a7e2f7b13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We define a network in Pytorch by extending the `torch.nn.Module` class and implementing the `__init__` and `forward` method. The `__init__` method is used to define the components of the architecture, while `forward`, takes an input tensor and passes it through the different layers of the network. You can refer to the documentation for `torch.nn` [here](https://pytorch.org/docs/stable/nn.html) and also can refer to [this](https://pytorch.org/tutorials/beginner/nn_tutorial.html) for a detailed tutorial on the same. \n",
    "\n",
    "Below we implement the `MultinomialLogisticRegressionModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "d8asqBPFYK4m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ec30afe32e5099d5d5e6b2e993412d8",
     "grade": false,
     "grade_id": "cell-688fd07073adc6d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultinomialLogisticRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_input, num_labels):\n",
    "        \"\"\"\n",
    "        Define the architecture of a Multinomial Logistic Regression classifier.\n",
    "        You will need to define two components, one will be the linear layer using\n",
    "        nn.Linear, and a log-softmax activation function for the output \n",
    "        (log-softmax is numerically more stable and as we will see later we just need to log of the probabilities to calculate the loss).\n",
    "\n",
    "        Inputs:\n",
    "          - d_input (int): The dimensionality or number of features in each input. \n",
    "                            This will be required to define the linear layer\n",
    "          - num_labels (int): The number of classes in the dataset. This will be\n",
    "\n",
    "        Hint: Recall that in multinomial logistic regression we obtain a `num_labels` probablilities (or log-probabilities in this case)\n",
    "        value for each input that denotes how likely is the input belonging\n",
    "        to each class.\n",
    "        \"\"\"\n",
    "        #Need to call the constructor of the parent class\n",
    "        super(MultinomialLogisticRegressionModel, self).__init__()\n",
    "\n",
    "        self.linear_layer = None\n",
    "        self.log_softmax_layer = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input `x` through the layers in the network and returns the output\n",
    "\n",
    "        Inputs:\n",
    "          - x (torch.tensor): A torch tensor of shape [batch_size, d_input] representing the batch of inputs\n",
    "\n",
    "        Returns:\n",
    "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
    "\n",
    "        \"\"\"\n",
    "        output = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "-1qD-_S3xgn3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8baefa3da6909f88bfdea5d57dc009b4",
     "grade": true,
     "grade_id": "cell-a6bd7ef76c46b9cc",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d2dd146d-f0f9-40b9-ca93-63e270835b39"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "torch.manual_seed(42)\n",
    "d_input = 5\n",
    "num_labels = 3\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = d_input, num_labels=num_labels)\n",
    "print(f\"Sample Test Case 1: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
    "in_features = sample_lr_model.linear_layer.in_features\n",
    "out_features = sample_lr_model.linear_layer.out_features\n",
    "\n",
    "print(f\"Number of Input Features: {in_features}\")\n",
    "print(f\"Number of Output Features: {out_features}\")\n",
    "print(f\"Expected Number of Input Features: {d_input}\")\n",
    "print(f\"Expected Number of Output Features: {num_labels}\")\n",
    "assert in_features == d_input and out_features == num_labels\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "d_input = 24\n",
    "num_labels = 4\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = d_input, num_labels=num_labels)\n",
    "print(f\"Sample Test Case 2: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
    "in_features = sample_lr_model.linear_layer.in_features\n",
    "out_features = sample_lr_model.linear_layer.out_features\n",
    "\n",
    "print(f\"Number of Input Features: {in_features}\")\n",
    "print(f\"Number of Output Features: {out_features}\")\n",
    "print(f\"Expected Number of Input Features: {d_input}\")\n",
    "print(f\"Expected Number of Output Features: {num_labels}\")\n",
    "assert in_features == d_input and out_features == num_labels\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 3: Checking if the model gives correct output\")\n",
    "test_input = torch.rand(d_input)\n",
    "model_output = sample_lr_model(test_input)\n",
    "model_output_np = model_output.detach().numpy()\n",
    "expected_output = np.array([-1.4595408, -1.1341913, -1.339573,  -1.6927251])\n",
    "print(f\"Model Output: {model_output_np}\")\n",
    "print(f\"Expected Output: {expected_output}\")\n",
    "\n",
    "assert np.allclose(model_output_np, expected_output, 1e-5)\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(f\"Sample Test Case 4: Checking if the model gives correct output\")\n",
    "test_input = torch.rand(4, d_input)\n",
    "model_output = sample_lr_model(test_input)\n",
    "model_output_np = model_output.detach().numpy()\n",
    "expected_output = np.array([[-1.4122071, -1.1847966, -1.1376545, -2.0400925],\n",
    "       [-1.4238064, -1.0178945, -1.3768857, -1.9276747],\n",
    "       [-1.5492793, -1.1670357, -1.3287748, -1.553487 ],\n",
    "       [-1.4491211, -1.0896355, -1.295007 , -1.8644052]])\n",
    "print(f\"Model Output: {model_output_np}\")\n",
    "print(f\"Expected Output: {expected_output}\")\n",
    "\n",
    "assert model_output_np.shape == expected_output.shape and np.allclose(model_output_np, expected_output, 1e-5)\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W2tQHcs36Ec5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d858329a110f04cdb32bd0e7b5ea40c1",
     "grade": false,
     "grade_id": "cell-099f26bffc5c1d0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that our logistic regression model seems to be defined correctly, let's initialize the model for our text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "GzInqWf46Dpo",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75e7f162255fd8ec469a6aa02a38819",
     "grade": false,
     "grade_id": "cell-58464221477c485c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a295add5-61bf-48d5-91d2-ab9e1cd4d8af"
   },
   "outputs": [],
   "source": [
    "newstopic_lr_model = MultinomialLogisticRegressionModel(\n",
    "    d_input = len(train_vocab),\n",
    "    num_labels = train_df_preprocessed[\"label\"].nunique()\n",
    ")\n",
    "newstopic_lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e1ZSvNsI6fm4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "215b0ee83b09dcbcd262ce57798935b2",
     "grade": false,
     "grade_id": "cell-2629df60b2e8e2d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Defining a loss function\n",
    "\n",
    "Now that we have implemented the model architecture, to train it we first need to define a loss function which we will minimize using an optimization algorithm. A loss function meausres how well the predictions of the model alligns with the actual training labels. In addition to the architecture blocks and activation functions `torch.nn` also offers a wide variety of loss functions that include:\n",
    "- `torch.nn.MSELoss` : Mean squared error loss function. Typically used for regression problems.\n",
    "- `torch.nn.L1Loss`: Mean absolute error loss function. Like MSE loss it is also used for regression problems. Takes the mean of absolute value of the errors instead of squared values.\n",
    "- `torch.nn.NLLLoss`: The negative log likelihood loss. It is useful to train a classification problem with C classes. Also called cross-entropy loss.\n",
    "- `torch.nn.CrossEntropyLoss`: Same as NLLLoss, but the API is different. It directly takes the output scores (without log-softmax), but computes the same value.\n",
    "\n",
    "You can look at the documentations of these and multiple other loss functions included in `torch.nn` package [here](https://pytorch.org/docs/stable/nn.html#loss-functions). For our purposes since we will be using `torch.nn.NLLLoss`. Below we define the loss function and demonstrate the usage on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN4I98QC8zop",
    "outputId": "f1f7cc2e-dee4-44e7-ab36-d98f09be387e"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# Demonstarting usage on a random example\n",
    "torch.manual_seed(42)\n",
    "example_model = MultinomialLogisticRegressionModel(d_input = 5, num_labels = 3)\n",
    "input = torch.rand(4, 5) # Defining a random input for demonstration\n",
    "preds = example_model(input)\n",
    "labels = torch.LongTensor([1, 0, 2, 0]) # Defining a random labels for demonstration\n",
    "loss = loss_fn(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Un0L_x_L3FxT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20523e9e12c13f2e5c24804b3da3a69f",
     "grade": false,
     "grade_id": "cell-f979921b070274f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see the `loss_fn` takes as the input the log probabilities on a batch of inputs and the labels corresponding to each input. Again note that the loss also contains a `grad_fn`. We can use `loss.backward()` to compute the gradients with respect to all the model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDBPzQ2_3FGR",
    "outputId": "8b2e4778-c5a4-49aa-8c09-4b2ddfe5a6fd"
   },
   "outputs": [],
   "source": [
    "print(example_model.linear_layer.weight.grad)\n",
    "loss.backward()\n",
    "print(example_model.linear_layer.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TXvSg01QCHqj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2593e43068957b31349a5063bbfcf4d9",
     "grade": false,
     "grade_id": "cell-3632ecc34cfdee23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how before calling `loss.backward()` the gradient was None, but after the call it gets populated. The gradients will then be used by the optimizer to update the parameters, which is a nice segue to our next topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kczo95DjC3AB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a0291df17e30e314dd18ff26b2dea01",
     "grade": false,
     "grade_id": "cell-01b8e849ec0cbea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Defining an Optimizer\n",
    "\n",
    "An optimizer is used to find the optimum set of parameters of the model that minimize the loss functions. Most commonly used optimizers in Machine Learning, especially in Deep Learning are the variants of Stochastic Gradient Descent (SGD), which updates the parameters of the model by moving then in the opposite direction of the gradients.\n",
    "\n",
    "![1406-7.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbwAAACdCAAAAADqXJIkAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAACYktHRAD/h4/MvwAAAAlwSFlzAAAOwwAADsMBx2+oZAAAAAd0SU1FB+QGCw0BFBJZ9/oAAA0WSURBVHja7Z3vbxTHGcf5n54XKyvyi6tkZCFbcALkxIqNjLBq4RaIJZQ6SFapwGopJAS51EW6UqA4xK1QjEiTa0ElMtBc0yMCREAIO3XBEdgYsKmxjc57nWfPez9mdvbm9m5mb+nzeeO7s703+3x3Zp55ZuaZdVkisqwLuwBEcEi8CEPiRRgSL8KQeBGGxIswJF6EIfEiDIkXYUi8CEPiRRgSL8KQeBGGxIswJF6EIfEiDInniz3z6a5WiP3kzy/DLokXJJ4/Xx75enZ2fDes/2I17KKIkHgqrH5mWWN22KUQIPGUyAxD892wCyFA4qnxMA4fZsIuBA+Jp0bmEMQfhl0IHhJPkUsAl8IuAw+Jp8i9Jjhaby4LiafI1EbYtxR2IThIPEWeb4Ntz8MuBAeJpwgTr2Mu7EJwkHhlWZ1/vuSI1zIZdlE4SDx/Vq72NQDAjtTDLSRetLDTmyB2+pk9M2h90EjiRYrMaQs6J/DV025W/TZOhV0gDhJPTuaEBfGJ3OvjTDzyNiPERQtiqbXXY0y8vsWwS8RB4kl5uBVgyA1Go3gHXoddJA4ST0aGtZSFWHSCiZcIu0g8JJ6Mh3EozAJlDjHxxsMuEg+JJyMJYKXdNws7of5GCiSejNcHALqeue8mW6D+4tIknoz57cUeCvoro2EXSYDEkzC9BWDEfbOyH+pwIp3Ek4ENZX7q/H4z1OESFhJPBorn+is4aqg/d4XEk/J8W0G8B0zIc/W2BiJL4klZGQBYi40t/RxgoN5CYwiJJ2PMdVjsMQu6Z8Mujhcknoy5HuidZz8zoxb0vwi7NJ6QeFImtsLum3Nf74YNF1bCLos3JJ6cl592WQ3th9L1N0ZYg8SLMF7iPf+kOwabEwt6v3n1xv7NENtzow5dcEO8Op4WPhNsP3XgkfQCongrZxo6v8ncj+dXAOhhYoc1/HSFjX6H67ZV0sydttP8rXvY3r6y6SvZ8y2I96Iftk+zn6MAe/WNbewrMWuUFX2mA6xrYVjOCKc2wxoN7/FjDTvZNMpr5237G62nJc83L95sL7Tdwxf3mvKD1NpjX2yAE8vZ3CRn/yszpgyD5XMWk846I2xpz4y+JUgis32q6YS3epx4c30Qu+LU0rkOgGOabslOxtyh0/n6jBrWDAyRwh5hmMgG/kO8IFLb22Nvee+pLhUPA0FHlp2XGNvT1W6mY9ByK/cSJ8quGzFjOGAl8oiLpmJCzMbH9isHY56NYIl4GAhyu0q8QM+8lhua7S64KShe0ogZw+ESuz9xN/tEvJF/Yn1tP9my1Ws2sUS8u80ApzKFC7zzRMf9ZA4DtLmLj1G8kaouV9fYQ+z+fvqM+5RVsv180MbX9nYCDnpEeYrFW+wvsuqTd3SJd72xUMzsyJstHi5cguN87zZuCS52Gdvfb270aDiLxfubVWRV7Gm7n2q4n/neomJmj7GbGzNhxnDALs/ilwwyCwgdUhnbL+3zckCKxMPNFC0P3HdpXeu7saHMe1qsUHW4HrJ2YMMiLH65ZsEQ58KUtf0INN4Url4kXpL9y2C+ZcWeVsdit/ke1oPny4HNSqO24aTD8vjeVoj97C4zV+ZOKpUyObvjPJu8FXFsyyeWKGv7by2PRTQF8Rb3MjsWfCDcFqNjzQ177KA//1RNbWRS3tdovuWz663hp6tXW+CjjLMI+n09HrQ3uARNGCiwHq3xdulH5W3PriR6IAXxbjI/YvuM+w5XAehYqYiL6OCz/O2wBwo6Zqq5YBnr9UAsiWZgfcoohp0ar+j7MpFxdrNN33Ifpi3YMl36UXnbz2/3iCPmxXOc2oJfhA6PSnuWBhnegRPsi4t+M6KpcV5jIg5WLoL4rAs6JnsMVzzHpoLXNyLs9FOwPTbAwkaXvHjs7gB+k3IZsdSWmVYqHror7Zfdb7nKPE84qW1aAaMBa0EL7Gh+ZUGj0WiOM1A4zN0elmRn6XSbgu1RvAF+qJcX73ajYH6VKlGheLgDgEOfv7JysGhQMuJ8Wb/RRWBObIyPH6EMnGVVbM/GVNv5ViMv3vmSW3OMrGGpIoZci55+LHX7Y122S7GrjxbvjoSYXseWZ8xroICbIDhpVGx/zCNmsq74PwqOBDa7OhJM3i7pmZ3qcFix1Twmq+KyMT66cG35aWhHvJLVl1Br+AI4NhWqEMa+Sj9Usv0xj7bMFQ+b5yK/aFxTE4MjmF/km+7FPlDvhSoWD124wrzLmPmKhyqIVUgUT8n2fuKVhtOwU7V0ZCgsDWVin7BHddBcsXiJ4ifDPsr+8uCyhluSw8YE4kDBo9lUsr2feOjCFyIyOJrVskgYFchHw2xmXn2pm7Fat+X7U3y6m29o+ioJuI9dDA+LDouS7ZnlhNxnrng4Wi60Z6MljTBiJ9dvrX5BEg4/C8/iTEdJB4jMvW+dqNHIAaMbhWAutqGGKx4+PR49ujhUKGd7Bz9vE13+Q+4XYfxxV6nMpRtFC1Q2VMCnrume+w53fSdLi/k5CKGjoGCbnL8jDOyYrnhYobz6HmGQXs72ruWk47ySCyTFMBLefcKjhatCvHmPgAerILXagZou7l6x4pnOgYPOWZvHokvWFZYmwipne+RVv0+EZWJD4QJPC4GJov/+yxderU5l4qHbkBeP+X/uSpbCHzz4Y61WixaL50T3TWWfepn+AZ9yZwAw6DH/LQSmy9o+6/iolrBE1xUPe4hf5i5gn4Pc8kENsD78R9/lXj5ud+OOekCTuOLhVCc6dPbv9c8dPunMjQ/Qh/Qcm6CqJa2piu1ZGywG8F3xcES71q48aIF4jfodAZwQytkvMwTwsU4PAoM5R3JnyLDnJP4ePjWPd+nPCnBprZLjQMF7SyY/Gati+xT4zeeNuo71bDes/0qX/44O5hl8gYulBrTOi2Y+dJdgLQ5Ay3XWSJ+0Tw7q36zFfC7Ys+CMgyQuEuvsS5dBKNg+AbFbwod58bCxvZx1llxv/Zfw/69n/3nqT7W4NWbD3oXccvePhJ0s9sJE8uP/1MqKt2K5bmLlOHTesdmj29C78U6tLi4HM0cwbb6Py3uFMW5Ng7/tEaa3x/KxwmTsjRZon169tcMaFNrc8e5WqNGZEIu/Bji7+jLRsOnvfGEWBtsbhNmSKsA19W3fZ6c/aBhirvf026C3j81/LWtT4NDlTmgYkfUK871cG+hj+xzXLK9Ic9EalrnfroeGHWenvKRnHUhhfFYVq//4sQWt+8c9NyicL/KZa8Cjg61gdZ11nD/7rxve/dLIDlf7AX7tu7+blPc9qRinhZ/ts06v6DXFo7i5kolXuyoh53wdnvmig8yw5yJaKRctT9dHUbx7TTWtErJ7OlSr+l3vLA5UEtSdiHsHmhXFY/7v5/pvaWGnGL97Q5ntblZ2nma7O//t+Qs18eyjRqoEG1cbqN/1weNe1Z3HL/p7JG6MmnisSnQ9U/rLqrj0Ri9953gxoDZNM9cnHQ+riTfZYqJK2EP/L12eQ+bC2wot56OdF6SWVxNv3EiVWOwzUr/rh/8qeJwrPk6AknisStRqls0PVr/DzH5f9kx0MylOKkBJPFYlTBw/xur3+TBt4X8muqEUJ5WgJN7URiNV4riR+l0OyZnoZlKcVIaSeGaqhKH6XRbPM9HNpDipECXxEkaqxPSWOjnwxeNMdDMpTipFRbxX/UaqRCrkLi+PeCa6oRQnlaIiHqsSJgIfiboZ5fFnohtKcVIxKuKlLRPbxpf21c0ZZ/yZ6GZSnFSOgnh2QuvmVRfW09TLmSHcmeiGUpxUjr949jfDaZyC15yX/sXIJ/PZpMbNXhXCnYluJsVJAPzFY73dvqVrVo9mdyUBkF7ca10M2xgupWeiG0pxEgB/8WY64OzcLu0hhVFo+yFp1UPWVK8z0c2kOAlCmT7vu97mzR9qHyYs/6E53nVttfoLVYf3meiGUpwEgRKEu8jORDeT4iQQJN4asjPRA6Y4MQKJl0N6JnrAFCdGIPFySM9ED5jixAgknoP8THQzKU6CQeIh8jPRDaU4CQaJh8jPRDeU4iQYJB4iPxPdUIqTYJB4Wd8z0Q2lOAkGiZf1PRPdTIqTgJB4Wd8z0YOmODECiZf1PRM9aIoTI5B4Wd8z0YOmODECiZf1PRPdUIqTYJB4Wd8z0Q2lOAkGiYf4nIluJsVJMEg8xOdMdEMpTgJB4jn4nIluJsVJIEi8HD5noptJcRIEEq8qDKU4kUDiVYWZFCcySLyqMJPiRAaJVw2GUpzIIPGqwVCKExkkXjWYSXEihcSrBjMpTqSQeFVgKMWJFBKvCsJOgUDiVYGhFCdSSLwqCDvrD4lXBWZSnMgh8YJjKMWJHBIvOIZSnMgh8YJjJsWJDyReYAylOPGBxAuCoRQn5SDxgmAoxUk5SLwgGEpxUg4SLxBmUpyUg8SLMCRehCHxIgyJF2FIvAhD4kUYEi/CkHgRhsSLMCRehCHxIgyJF2FIvAhD4kUYEi/CkHgR5n+/bNB7k6+v2gAAAABJRU5ErkJggg==)\n",
    "\n",
    "\n",
    "Here, Θ denotes the model parameters, J is the loss function and α is what we call learning rate which is used to specify the strength of the step that we wish to take. A smaller learning rate ensures we do not move away from the minima, but it makes the learning slower, while with a larger learning rate we move quickly towards the minima but are susceptible to overshooting it. Variants of SGD like SGD + Momentum, RMSProp, Adam etc., tries to improve it's noisy nature (which arises due to the fact we work on batches, instead of the entire dataset at a time), by introducing minor modifications to the update equation to prevent the optimizer taking a step in an overly wrong direction, and some also introduce heuristics to decay the step size as we reach closer to the minima. Adam is one of the most used optimizers in Deep Learning applications and often works reasonably well in practical applications. We will use the same for our experiments. You can read more about how these different optimizers work [here](https://cs231n.github.io/neural-networks-3/). \n",
    "\n",
    "`torch.optim` provides implementations for all of the optimizers we mentioned above. For official documentation for the same, refer [here](https://pytorch.org/docs/stable/optim.html). Below we provide an example on how to define and use optimizers in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3Bjhr47CCeK"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# We will first need to define the model\n",
    "example_model = MultinomialLogisticRegressionModel(d_input = 5, num_labels=3)\n",
    "\n",
    "# Defining the optimizer\n",
    "example_optim = Adam(example_model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SGgdODqGIoWq",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50855aecff119f81e43b3df0c4a2d4c8",
     "grade": false,
     "grade_id": "cell-3a5a88369ea13542",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice that the optimizer takes as input two arguments, the first is the parameters (Θ) of the model that are to be learned using the optimizer and the second is the learning rate (α). Adam optimizer also has other hyperparameters like β1 , β2, ϵ, details of which are beyond the scope of this assignment. However, you need not worry about setting these hyper-parameters as in most of the cases the default vaues of these work well enough. Next let's see how to update the model's parameters using the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLNlnpgsB82x",
    "outputId": "671ae47b-edd3-4ada-c8b3-2f7b136cec8c"
   },
   "outputs": [],
   "source": [
    "example_optim.zero_grad() # This is done to zero-out any existing gradients stored from some previous steps\n",
    "input = torch.rand(2, 5) # Defining a random input for demonstration\n",
    "preds = example_model(input)\n",
    "labels = torch.LongTensor([1,0]) # Defining a random labels for demonstration\n",
    "loss = loss_fn(preds, labels)\n",
    "loss.backward() # Perform backward pass\n",
    "print(f\"Parameters before the update: {example_model.linear_layer.weight}\")\n",
    "example_optim.step() # Update the parameters using the optimizer\n",
    "print(f\"Parameters after the update: {example_model.linear_layer.weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cc8E918pOsq0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d05588bcefb43ddbc27b0cc64853505d",
     "grade": false,
     "grade_id": "cell-e75eef4c28c7203d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see after calling `example_optim.step()` the parameters get updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7f2Fdb2ZU1ei",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31a191c37d891951b0472a37d08b9151",
     "grade": false,
     "grade_id": "cell-6e77847e377e083a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.3: Training the Model\n",
    "\n",
    "Now we have all the different components ready and can start training our text classifier. Implement the `train` function below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "lEgvxQhBOq4u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9142f1eb240a8956c08f4e1918320329",
     "grade": true,
     "grade_id": "cell-bda0b7948d888555",
     "locked": false,
     "points": 2.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train(model, train_dataloader,\n",
    "          lr = 1e-3, num_epochs = 20,\n",
    "          device = \"cpu\",):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs the training loop\n",
    "\n",
    "    Inputs:\n",
    "    - model (MultinomialLogisticRegressionModel): Multinomial Logistic Regression model to be trained\n",
    "    - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
    "    - lr (float): The learning rate for the optimizer\n",
    "    - num_epochs (int): Number of epochs to train the model for.\n",
    "    - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    - model (MultinomialLogisticRegressionModel): Model after completing the training\n",
    "    - epoch_loss (float) : Loss value corresponding to the final epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Transfer the model to specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Step 1: Define the Binary Cross Entropy loss function\n",
    "    loss_fn = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Step 2: Define Adam Optimizer\n",
    "    optimizer = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Iterate over `num_epochs`\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n",
    "        # Iterate over each batch using the `train_dataloader`\n",
    "        for train_batch in tqdm.tqdm(train_dataloader):\n",
    "            # Zero out any gradients stored in the previous steps\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unwrap the batch to get features and labels\n",
    "            features, labels = train_batch\n",
    "\n",
    "            # Most nn modules and loss functions assume the inputs are of type Float while the labels are expected to be of type Long\n",
    "            features = features.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Transfer the features and labels to device\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            # Step 3: Feed the input features to the model to get predictions\n",
    "            preds = None\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Step 4: Compute the loss and perform backward pass\n",
    "            loss = None\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Step 5: Take optimizer step\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Store loss value for tracking\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch} completed.. Average Loss: {epoch_loss}\")\n",
    "\n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "_H4_v07ViC50",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1a98fea46e6e4c784286d59011010bc",
     "grade": true,
     "grade_id": "cell-62f537900cbf53d1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2eda9b98-5a8a-4b55-abef-4fcabb2799a4"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "print(\"Training on just 100 training examples for sanity check\")\n",
    "torch.manual_seed(42)\n",
    "sample_documents = train_df_preprocessed[\"news\"].values.tolist()[:100]\n",
    "sample_labels = train_df_preprocessed[\"label\"].values.tolist()[:100]\n",
    "sample_vocab = create_vocab_w_rare_word_filter(train_documents, threshold=5)\n",
    "sample_word2idx = get_word_idx_mapping(train_vocab)\n",
    "\n",
    "sample_dataset = AGNewsDataset(sample_documents,\n",
    "                            sample_labels,\n",
    "                            sample_vocab,\n",
    "                            sample_word2idx)\n",
    "\n",
    "sample_dataloader = DataLoader(sample_dataset)\n",
    "\n",
    "sample_lr_model = MultinomialLogisticRegressionModel(d_input = len(sample_vocab), num_labels = NUM_LABELS)\n",
    "\n",
    "sample_lr_model, loss = train(sample_lr_model, sample_dataloader,\n",
    "      lr = 1e-2, num_epochs = 10,\n",
    "      device = \"cpu\")\n",
    "\n",
    "expected_loss = 0.007294731960864737\n",
    "print(f\"Final Loss Value: {loss}\")\n",
    "print(f\"Expected Loss Value: {expected_loss}\")\n",
    "\n",
    "#assert np.allclose(expected_loss, loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c_ssqj6MhwXJ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dcf4a1a2581ff76bd7d3a37bedb153e",
     "grade": false,
     "grade_id": "cell-b142da2242674d80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Don't worry if the exact loss values do not match, as long as your loss is reducing with epochs and the final loss is in the same range, you should be fine. And now lets train on the entire dataset, this may take some time, approximate 8 minutes per epoch. So relax and have yourself a cup of coffee while this runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "FGLFCmFkzyqc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d46e44f8efd0b410dd781067b1e9f7b",
     "grade": false,
     "grade_id": "cell-ca17de4d581b63da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1b923ef0-2292-4308-e38d-88404b1dfdea"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "agnews_lr_model = MultinomialLogisticRegressionModel(\n",
    "    d_input = len(train_vocab), num_labels=NUM_LABELS\n",
    ")\n",
    "agnews_lr_model, final_loss = train(agnews_lr_model, train_dataloader,\n",
    "      lr = 1e-2, num_epochs = 2,\n",
    "      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wZXvrhEmolLq",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23360e81585cc21180391ed23a48a2cc",
     "grade": false,
     "grade_id": "cell-d29b6970919f9dd7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.4: Evaluating the Model\n",
    "\n",
    "Evaluation is one of the most important step in a Machine Learning pipeline, as it help us measure how well the trained is able to predict on unseen data. There are different performance metrics that can be used for evaluating machine learning algorithms. One of the most commonly used metrics for evaluating classification models is accuracy which is defined as the number of test examples predicted correctly by the model divided by the total number of test examples.![1_udGMH6OQF4CMcv42mjW_qg.png](data:image/png;base64,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)\n",
    "\n",
    "To check the number of correct predictions we can check the label predicted by our model and the actual label for a test example and if they both are the same. However, note that our model outputs log-probabilities for each class and not directly predicts the label. To get that, we can simply select the class as the predicted label which has the highest log probability. We start by implementing `convert_outputs_to_labels` function which does exactly that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "nAWTto1bBBu_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e66bea06536bb1cd58fa7d24ce45584",
     "grade": false,
     "grade_id": "cell-7ab99bc2c084b78d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_outputs_to_labels(outputs):\n",
    "    \"\"\"\n",
    "    Convert the model's outputs or log-probabilities to labels by using the specified threshold\n",
    "\n",
    "    Inputs:\n",
    "    - outputs (numpy.ndarray): A numpy 2d array containing the outputs predicted by the classifier model\n",
    "\n",
    "    Returns:\n",
    "    - labels (numpy.ndarray): Labels obtained after thresholding\n",
    "    - confs (numpy.ndarray): Model's confidence (or probability) for each predicted label\n",
    "\n",
    "    Hint: np.argmax or np.max can be useful here\n",
    "    \"\"\"\n",
    "\n",
    "    labels = None\n",
    "    confs = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return labels, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "WUD3S4P1s7Zc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb4950353a0a5fcf7f13ed64dd3b34dc",
     "grade": true,
     "grade_id": "cell-4a01d59eb7100f8e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "9daad8c4-e6ce-4532-9161-dc93614e2cad"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "print(\"Sample Test Case 1\")\n",
    "sample_outs = np.log(np.array([[0.2, 0.7, 0.1], [0.76, 0.2, 0.04], [0.1, 0.2, 0.7], [0.8, 0.1, 0.1]]))\n",
    "print(f\"Input Probabilities: {sample_outs}\")\n",
    "sample_labels, sample_confs = convert_outputs_to_labels(sample_outs)\n",
    "expected_labels = np.array([1, 0, 2, 0])\n",
    "expected_confs = np.array([0.7, 0.76, 0.7, 0.8])\n",
    "print(f\"Predicted Lables: {sample_labels}\")\n",
    "print(f\"Confidence: {sample_confs}\")\n",
    "\n",
    "print(f\"Expected Lables: {expected_labels}\")\n",
    "print(f\"Expected Confidence: {expected_confs}\")\n",
    "\n",
    "assert (sample_labels == expected_labels).all()\n",
    "assert (sample_confs == expected_confs).all()\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xOlABIEHvRnB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3357a47a0d11bb9144749958aab6e0bb",
     "grade": false,
     "grade_id": "cell-f49f4043d76742d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next lets implement the `get_accuracy` function which takes as input predicted labels and actual labels and computes the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Y1VhwC5WvYxr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4ccf8a697bd5c7a8ddf4f3075cf9bed",
     "grade": false,
     "grade_id": "cell-ed9814cc82b4b4dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(pred_labels, act_labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy value by comparing predicted labels with actual labels\n",
    "\n",
    "    Inputs:\n",
    "    - pred_labels (numpy.ndarray) : A numpy 1d array containing predicted labels. \n",
    "    - act_labels (numpy.ndarray): A numpy 1d array containing actual labels (of same size as pred_labels). \n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Number of correct predictions / Total number of predictions\n",
    "\n",
    "    \"\"\"\n",
    "    accuracy = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "ShWo6Kl0wfne",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ce19a6302a52b34c368bfa4d72df19",
     "grade": true,
     "grade_id": "cell-21d679aad70d9602",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "af415af5-5adf-4168-e630-b6ececf1e471"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "print(\"Sample Test Case 1\")\n",
    "sample_pred_labels = np.array([0, 0, 1, 1])\n",
    "sample_act_labels = np.array([0, 0, 0, 1])\n",
    "sample_acc = get_accuracy(sample_pred_labels, sample_act_labels)\n",
    "expected_acc = 0.75\n",
    "print(f\"Input Predicted Labels: {sample_pred_labels}\")\n",
    "print(f\"Input Actual Labels: {sample_act_labels}\")\n",
    "print(f\"Accuracy: {sample_acc}\")\n",
    "print(f\"Expected Accuracy: {expected_acc}\")\n",
    "\n",
    "assert sample_acc == expected_acc\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 2\")\n",
    "sample_pred_labels = np.array([0, 0, 1, 1, 0])\n",
    "sample_act_labels = np.array([1, 1, 0, 0, 1])\n",
    "sample_acc = get_accuracy(sample_pred_labels, sample_act_labels)\n",
    "expected_acc = 0\n",
    "print(f\"Input Predicted Labels: {sample_pred_labels}\")\n",
    "print(f\"Input Actual Labels: {sample_act_labels}\")\n",
    "print(f\"Accuracy: {sample_acc}\")\n",
    "print(f\"Expected Accuracy: {expected_acc}\")\n",
    "\n",
    "assert sample_acc == expected_acc\n",
    "\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "V_iKfN8WxkH8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e00f161db470bca27d0538b7c6124b58",
     "grade": false,
     "grade_id": "cell-f73ba4d7f000e6bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can implement `evaluate` function which takes in a model and a test dataloader, iterates through every batch of the test dataset and calculates the average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Yac4aNBHxKjP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68e89322c825aac6b6b5ea82a2e844bb",
     "grade": true,
     "grade_id": "cell-80252e20034d4918",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluates `model` on test dataset\n",
    "\n",
    "    Inputs:\n",
    "    - model (MultinomialLogisticRegressionModel): Logistic Regression model to be evaluated\n",
    "    - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Average accuracy over the test dataset\n",
    "    - preds (np.ndarray): Predictions of the model on test dataset\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model = model.eval() # Set model to evaluation model \n",
    "    accuracy = 0\n",
    "    preds = []\n",
    "\n",
    "    # by specifying `torch.no_grad`, it ensures no gradients are calcuated while running the model,\n",
    "    # this makes the computation much more faster\n",
    "    with torch.no_grad():\n",
    "        for test_batch in tqdm.tqdm(test_dataloader):\n",
    "            features, labels = test_batch\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            # Step 1: Get probability predictions from the model and store it in `pred_probs`\n",
    "            pred_logprobs = None\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Convert predictions and labels to numpy arrays from torch tensors as they are easier to operate for computing metrics\n",
    "            pred_logprobs = pred_logprobs.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "\n",
    "            # Step 2: Get accuracy of predictions and store it in `batch_accuracy`\n",
    "            batch_accuracy = None\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            accuracy += batch_accuracy\n",
    "            preds += pred_labels.tolist()\n",
    "\n",
    "    # Divide by number of batches to get average accuracy\n",
    "    accuracy = accuracy / len(test_dataloader)\n",
    "\n",
    "    return accuracy, np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "sY8H0Rmg0sbB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "068ab7503b83063534be398ef57d93cf",
     "grade": true,
     "grade_id": "cell-73575615ae8bab92",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "21b8c0f3-0ef3-4770-c9a3-96012b9feed4"
   },
   "outputs": [],
   "source": [
    "print(\"Running Sample Test Cases\")\n",
    "\n",
    "print(\"Testing on just 100 test examples for sanity check\")\n",
    "torch.manual_seed(42)\n",
    "sample_documents = test_df_preprocessed[\"news\"].values.tolist()[:100]\n",
    "sample_labels = test_df[\"label\"].values.tolist()[:100]\n",
    "\n",
    "sample_dataset = AGNewsDataset(sample_documents,\n",
    "                            sample_labels,\n",
    "                            train_vocab,\n",
    "                            train_word2idx)\n",
    "\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size = 64)\n",
    "\n",
    "accuracy, _ = evaluate(agnews_lr_model, sample_dataloader, device =\"cpu\")\n",
    "\n",
    "expected_accuracy = 0.8680555555555556\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Expected Accuracy: {expected_accuracy}\")\n",
    "\n",
    "#assert np.allclose(expected_accuracy, accuracy, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, don't worry if the values do not match exactly. As long as the value you obtained is close to 0.86 it should be fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DD4YSQ_pFNo9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa6271635f57179d5c792ec32733eec0",
     "grade": false,
     "grade_id": "cell-6fef0bb78fcce993",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's obtain the accuracy on the entire test set now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "PvkP6uV9FM0f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bf54968af8f23b454ee2f5e8e27e7ce",
     "grade": false,
     "grade_id": "cell-4b07a4a33df81dbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a6f4c697-1a8f-46f3-df86-df5f9c8ef86b"
   },
   "outputs": [],
   "source": [
    "test_acc, test_preds = evaluate(agnews_lr_model, test_dataloader, device = device)\n",
    "print(f\"Test Accuracy: {(100* test_acc).round(2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "N1nsCdSUFrSk",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff89eb95cace16d08b5e7eeeee1bed67",
     "grade": false,
     "grade_id": "cell-fb25a045569c78ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We obtain around 89.47% accuracy form our logistic regression, which is very reasonable considering random guessing will fetch you an accuracy of ~20%. So we are doing ~70% better than random guessing which is pretty good for our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8361ea4a2fb291b6c29edeec934f78be",
     "grade": false,
     "grade_id": "cell-e10c9947cdc19550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.5: Error Analysis\n",
    "\n",
    "While it is nice to have a single number (accuracy in this case) that explains the performance of our model, in practice we often need to have more detailed insights into how the model is performing. One of popular methods for visualizing the performance of multi-class classifiers is Confusion Matrix. Each row of a confusion matrix represents the instances in an actual class while each column represents the instances in a predicted class.\n",
    "\n",
    "![conf_mat](https://miro.medium.com/v2/resize:fit:1400/1*yH2SM0DIUQlEiveK42NnBg.png)\n",
    "\n",
    "Below we implement the function `get_confusion_matrix` that takes the predictions and labels to construct the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6f17d2036209250d4d85ca777521902",
     "grade": false,
     "grade_id": "cell-79b561ad74ab035c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix(preds, labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes confusion matrix from `preds` and `labels`\n",
    "\n",
    "    Inputs:\n",
    "        - preds (np.ndarray): Predictions of the model on test dataset\n",
    "        - labels (np.ndarray): Actual labels of the test dataset\n",
    "\n",
    "    Returns:\n",
    "        - confusion_matrix (np.ndarray): A 2d numpy array containing the confusion matrix\n",
    "    \"\"\"\n",
    "    confusion_matrix = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66c7784c80e7af2eb41d6ae532012ca5",
     "grade": true,
     "grade_id": "cell-2bd8cd81a9ab67e0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sample Test Case 1\n",
    "print(\"Sample Test Case 1\")\n",
    "sample_pred_labels = np.array([0, 0, 1, 1, 0])\n",
    "sample_act_labels = np.array([1, 1, 0, 0, 1])\n",
    "sample_confusion_matrix = get_confusion_matrix(sample_pred_labels, sample_act_labels)\n",
    "expected_confusion_matrix = np.array([[0, 2], [3, 0]])\n",
    "print(f\"Input Predicted Labels: {sample_pred_labels}\")\n",
    "print(f\"Input Actual Labels: {sample_act_labels}\")\n",
    "print(f\"Confusion Matrix: \\n{sample_confusion_matrix}\")\n",
    "print(f\"Expected Confusion Matrix: \\n{expected_confusion_matrix}\")\n",
    "\n",
    "assert np.allclose(sample_confusion_matrix, expected_confusion_matrix)\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "# Sample Test Case 2 (this contains more than 2 classes)\n",
    "print(\"Sample Test Case 2\")\n",
    "sample_pred_labels = np.array([0, 0, 1, 1, 0, 2, 2, 2, 2])\n",
    "sample_act_labels = np.array([1, 1, 0, 0, 1, 2, 2, 2, 2])\n",
    "sample_confusion_matrix = get_confusion_matrix(sample_pred_labels, sample_act_labels)\n",
    "expected_confusion_matrix = np.array([[0, 2, 0], [3, 0, 0], [0, 0, 4]])\n",
    "print(f\"Input Predicted Labels: {sample_pred_labels}\")\n",
    "print(f\"Input Actual Labels: {sample_act_labels}\")\n",
    "print(f\"Confusion Matrix: \\n{sample_confusion_matrix}\")\n",
    "print(f\"Expected Confusion Matrix: \\n{expected_confusion_matrix}\")\n",
    "\n",
    "assert np.allclose(sample_confusion_matrix, expected_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed6102967777de5ba8eb7dea296eaef5",
     "grade": false,
     "grade_id": "cell-0c252326194183c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that our implementation of `get_confusion_matrix` looks good, we obtain the confusion matrix for our model's predictions on the test data and analyse the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a45c346c8194d4092328c79dfdfc1e8b",
     "grade": false,
     "grade_id": "cell-8005d0ecdb9870a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get confusion matrix for the test dataset\n",
    "labels = test_df[\"label\"].values.tolist()\n",
    "confusion_matrix = get_confusion_matrix(test_preds, labels)\n",
    "\n",
    "# Plot the confusion matrix using a heatmap in seaborn with LABELS_MAP as labels instead of integers\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(confusion_matrix, annot = True, fmt = \".0f\", cmap = \"Blues\", xticklabels = LABELS_MAP, yticklabels = LABELS_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e2264ba91dcea4d8ed26122b0813e6f",
     "grade": false,
     "grade_id": "cell-47701c30f53000f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen, the most common point of confusion for the model comes from distinguishing Sci/Tech and Business related news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ejn3fsID1WBf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe96e1ed22add426ce4016b7e8680943",
     "grade": false,
     "grade_id": "cell-62e6eb8240a1076e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.6: Making Predictions from scratch\n",
    "\n",
    "Now that we have trained the model and evaluated it's performance it seems like a nice place to end, right? However, one aspect that is often overlooked in ML or NLP pipelines is designing an interface that can make prediction directly on a piece of text using the trained model, abstracting away all the pre-processing and model run details from the user. Let's implement the `predict_document` function below that does exactly that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Ie6ha5Fw4u5_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fccd66934d22676fbc239bf7ee1d094a",
     "grade": false,
     "grade_id": "cell-c2f8054e4e000f1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_document(document, model, train_vocab, train_word2idx, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Predicts the class label for the `document` using `model`\n",
    "\n",
    "    Inputs:\n",
    "    - document (str): The document whose news category is to be predicted\n",
    "    - model (MultinomialLogisticRegressionModel): A trained logistic regression model\n",
    "    - train_vocab (list): Vocabulary on which the model was trained on\n",
    "    - train_word2idx (dict): A Python dictionary mapping each word to its index in vocabulary\n",
    "\n",
    "    Returns:\n",
    "    - pred_label (str): Predicted topic of the document (shouldn't be an integer but the label's name)\n",
    "    - confidence (float): Confidence of the prediction\n",
    "\n",
    "    Hint: Follow the following steps:\n",
    "    - preprocess the document\n",
    "    - obtain bag of words features from the preprocessed document\n",
    "    - convert the features to a pytorch tensor using torch.FloatTensor(features)\n",
    "    - feed the features tensor to the model to obtain predicted probabilities\n",
    "    - convert predicted probabilities to labels by checking if predicted probability is greater than or less than the threshold\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    pred_label = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return pred_label, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2dd51e52b5edf6c59fa8702f32a7e1",
     "grade": true,
     "grade_id": "cell-3d9049beee667bd7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Write sample test cases to test a news topic classification model\n",
    "print(\"Sample Test Case 1\")\n",
    "sample_document = \"India's population to cross China's in 6 years: UN\"\n",
    "sample_pred_label, sample_confidence = predict_document(sample_document, agnews_lr_model, train_vocab, train_word2idx)\n",
    "expected_pred_label = \"World\"\n",
    "\n",
    "print(f\"Input Document: {sample_document}\")\n",
    "print(f\"Predicted Label: {sample_pred_label}\")\n",
    "print(f\"Cofidence: {sample_confidence}\")\n",
    "print(f\"Expected Predicted Label: {expected_pred_label}\")\n",
    "\n",
    "assert sample_pred_label == expected_pred_label\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 2\")\n",
    "sample_document = \"Apple's new iPhone SE for the budget-conscious\"\n",
    "sample_pred_label, sample_confidence = predict_document(sample_document, agnews_lr_model, train_vocab, train_word2idx)\n",
    "expected_pred_label = \"Sci/Tech\"\n",
    "\n",
    "print(f\"Input Document: {sample_document}\")\n",
    "print(f\"Predicted Label: {sample_pred_label}\")\n",
    "print(f\"Cofidence: {sample_confidence}\")\n",
    "print(f\"Expected Predicted Label: {expected_pred_label}\")\n",
    "\n",
    "assert sample_pred_label == expected_pred_label\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "print(\"Sample Test Case 3\")\n",
    "sample_document = \"India's football team captain Sunil Chhetri says he's missing playing\"\n",
    "sample_pred_label, sample_confidence = predict_document(sample_document, agnews_lr_model, train_vocab, train_word2idx)\n",
    "expected_pred_label = \"Sports\"\n",
    "\n",
    "print(f\"Input Document: {sample_document}\")\n",
    "print(f\"Predicted Label: {sample_pred_label}\")\n",
    "print(f\"Cofidence: {sample_confidence}\")\n",
    "print(f\"Expected Predicted Label: {expected_pred_label}\")\n",
    "\n",
    "assert sample_pred_label == expected_pred_label\n",
    "\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8BcJ0z6l-HC-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a126c389cd4dea42eb22edea13fe711f",
     "grade": false,
     "grade_id": "cell-5d4627dfa383c43d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Bonus Task: Interpreting the trained model\n",
    "\n",
    "One of the biggest advantages of using linear models like Logistic Regression is that they are much easier to interpret compared to more sophisticated neural network models. We can just look at the weights of a trained logistic regression model and based on that can determine certain interesting insights about the model. \n",
    "\n",
    "![linear_model_interpretation](https://i.ibb.co/2ZgKJSf/image-6487327.jpg)\n",
    "\n",
    "As can be seen from the image above we look at the we can just look at the features corresponding the highest values of a row j to find the most important features for the prediction of class j according to our model. Recall that for bag of words each of the feature can be interpreted as a word from the vocabulary. Hence, by just looking at the weights we can figure out the words whose presence is most likely to cause prediction of a particular class. Implement `get_imp_features_for_class` and `interpret_linear_model` below to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1577c5650bab8b4c79615e6871996c10",
     "grade": true,
     "grade_id": "cell-46f8e0b8c1979b56",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_imp_features_for_class(weights, cls, train_vocab, topk = 10):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the top-k important features for a given class\n",
    "\n",
    "    Inputs:\n",
    "        - weights (ndarray): The weights of the model. Shape: [num_classes, num_features]\n",
    "        - cls (int): The class for which the top-k important features are to be returned\n",
    "        - train_vocab (list): Vocabulary on which the model was trained on\n",
    "        - topk (int): Number of top-k important features to be returned\n",
    "\n",
    "    Returns:\n",
    "        - imp_features (list): List of top-k important features for the given class. \n",
    "            Each element of the list should be a string corresponding to a word in the vocabulary.\n",
    "        - imp_values (list): List of top-k important feature values for the given class (given by just the value of the weights).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    imp_features = []\n",
    "    imp_values = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return imp_features, imp_values\n",
    "\n",
    "\n",
    "def intepret_linear_model(model, train_vocab, topk = 10):\n",
    "\n",
    "    \"\"\"\n",
    "    Interpret the linear model by obtaining the top-k important features for each class\n",
    "\n",
    "    Inputs:\n",
    "        - model (MultinomialLogisticRegressionModel): The linear model\n",
    "        - train_vocab (list): Vocabulary on which the model was trained on\n",
    "        - topk (int): Number of top-k important features to be returned\n",
    "\n",
    "    Returns:\n",
    "        - imp_features (dict): Dictionary of top-k important features with their values for each class. \n",
    "            The keys of the dictionary are the class labels and the values are dictionaries of the form {\"features\": imp_features, \"values\": imp_values}\n",
    "\n",
    "\n",
    "    Example output:\n",
    "    {\n",
    "        \"World\": {\"features\": [\"world\", \"said\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"monday\", \"sunday\", \"year\", \"new\"], \"values\": [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01]},\n",
    "        \"Sports\": {\"features\": [\"said\", \"game\", \"year\", \"team\", \"games\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"monday\"], \"values\": [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01]},\n",
    "        \"Business\": {\"features\": [\"said\", \"company\", \"business\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"monday\", \"new\", \"year\"], \"values\": [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01]},\n",
    "        \"Sci/Tech\": {\"features\": [\"said\", \"new\", \"year\", \"people\", \"time\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"monday\"], \"values\": [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01]}\n",
    "    }\n",
    "\n",
    "    These values were just invented randomly, the only thing you should take-away from the example is the format of the output.\n",
    "\n",
    "    Hint: Weights of the model can be obtained by accessing `model.linear_layer.weight.data`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    imp_features = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e5ef4c3210accc85a9c223c76748ad7",
     "grade": true,
     "grade_id": "cell-8a6a1cd8c3e071c6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "imp_features = intepret_linear_model(agnews_lr_model, train_vocab, topk = 10)\n",
    "\n",
    "# Visualize the top-k important features for each class using seaborn bar plots\n",
    "\n",
    "for cls in imp_features:\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.barplot(x = imp_features[cls][\"values\"], y = imp_features[cls][\"features\"])\n",
    "    plt.title(f\"Top-{10} important features for class {cls}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aec4c4353f99033f58d5abd903613a0",
     "grade": false,
     "grade_id": "cell-1c14fd41cab74fa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As can be seen we derive insights into how the model classifies the sentences."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
